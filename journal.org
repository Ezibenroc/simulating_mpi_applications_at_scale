#-*- coding: utf-8 -*-
#+TITLE:  Research journal
#+LANGUAGE: EN
#+CATEGORY: INRIA
#+STARTUP: overview indent inlineimages logdrawer hidestars
#+TAGS: [ SIMGRID : SMPI(s) ]
#+TAGS: [ PROGRAMMING : C(C) CPP(c) PYTHON(p) R(r) SHELL(S) ]
#+TAGS: [ TOOLS : ORGMODE(o) GIT(g) GDB ]
#+TAGS: [ EXPERIMENTS(x) : EXP_SETUP EXP_EXEC EXP_RESULT EXP_ANALYSIS ]
#+TAGS: TRACING(t) PERFORMANCE(X) PROFILING(R) BUG(b) PAPER(P) HPL(h) MEETING(m) G5K(G) BORDEAUX(B) VALIDATION(v) REPORT(V)
#+LOGGING: lognoterepeat
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) CANCELLED(c@) DEFERRED(f@) | DONE(d!)
#+SEQ_TODO: UNREAD | READ
#+SEQ_TODO: GOOD(g!) CRITICISM INTERESTING(w!) INVESTIGATE PROPOSAL
#+SEQ_TODO: QUESTION(q!) | RESOLVED(r!)

* 2017
** 2017-02 February
*** 2017-02-06 Monday
**** DONE Read [[file:5218a011.pdf][An Evaluation of Network Architectures for Next Generation Supercomputers]] :PAPER:
Bibtex: Chen:2016:ENA:3019057.3019059
- The authors roughly do what we want to do: they use a simulator to do performance evaluation of different topologies,
  with different workloads and routing algorithms.
- In a first part, they detail what are these topologies, routing algorithms and workloads.  This could give us some
  ideas of what to test. Maybe we could try to reproduce their results?
- They focus on topologies having:
  + Full uniform bandwidth.
  + Have good partitionability and can be grown modularly.
  + Come at a lower cost than a 3-level fat tree (which is the state of the art in terms of pure performances).
- They test an adversarial traffic (task i sends to task (i+D) mod G,  tuned to “be bad”).
  + Fat tree has great performances, regardless of the routing algorithms.
  + Other topologies (Dragonfly+, Stacked all-to-all, Stacked 2D hyperX) have terrible performances with direct
    routing. For indirect or adaptive routing, performances are much better (but still a factor 2 lower than the fat
    tree).
- Then, they test neighbor traffic (the logical topology is a grid for instance).
  + Again, the fat tree has nearly full performances, regardless of the routing algorithm.
  + Other topologies have lower performances with indirect routing. Their performances are ok with direct or adaptive
    routing.
- Next, they look at AMR.
  + Here, all topologies and routing algorithms have poor performances.
  + The average throughput is high at the beginning, but decreases very quickly to nearly 0.  This long tail with low
    throughput accounts for the major part of the execution time.
  + Thus, AMR seems to be inherently bad for parallelism.
- To sum up, the best routing algorithm is the adaptive routing (except maybe for the fat tree), the best topology is
  the fat tree.
- The authors then had a look at random-mappings of the processes to the nodes (until now, the mapping was ideal). This
  could reflect what would do a scheduler which is not topology-aware.  In general, with adaptive routing, the Fat
  Tree and the Dragonfly+ are very robust to irregular placements, the completion time is not impacted too much. This is
  not the case for stacked topologies (due to a lack of path diversity). Thus, we should use a topology-aware job
  scheduler, especially for stacked topologies.  With non-adaptive routing, all the topologies suffer of performance
  degradations.
**** DONE Talk with Arnaud about the internship.                 :MEETING:
***** Two possible things to have a look at.
- Simulate the impact of network failures on the performances.
  + May need to work on Simgrid implementation, to handle failures.
  + A recent paper has shown that, in their case, removing one of the two root switches of their fat tree did not impact
    significantly the performances.
  + A reason is that jobs rarely occupy the full tree, they are localized in one of its sub-trees. Thus, nearly no
    communication go to the top switches.
- Modelize in Simgrid [[https://www.tacc.utexas.edu/stampede/][Stampede]] super computer.
  + It uses a fat tree topology.
  + We have access to real benchmark results.
  + We have access to its configuration (e.g. OS and compiler used).
***** A first step for this internship would be to run HPL on a fat tree, with Simgrid.
***** Some features of Simgrid to speedup a simulation.
- A macro to only run the first steps of a loop and infer the total time from it.
- An allocator (replacing malloc/free) to share memory between processes.
***** Some technical details about Simgrid
- For every process, we run each piece of code until we reach a MPI operation.  This gives us the execution time of this
  code block.
- We know all the communication flows of the “current step”, thanks to the routing.
  We thus have a list of linear constraints (e.g. the bandwidth of all flows going
  through a same link should not exceed the capacity of this link). We solve this
  by maximizing the minimum bandwidth of any flow (empirically, this is close to the
  reality, where flows have a fair share of the resources).
- Routing is made with an AS hierarchy. There are local routing decisions (within an
  AS) and global routing decisions (between two AS).
***** There exists other simulators.
Mainly codes/ross. Discrete event simulators, so they consider the problem at a lower level.  But being too precises has
some drawbacks:
- The exact version of every piece of code can have noticeable impact → tedious to calibrate.
- The simulation takes much more time, does not scale as much as Simgrid.
*** 2017-02-07 Tuesday
**** DONE Begin writing a journal \o/
**** DONE Read [[file:8815a909.pdf][Characterizing Parallel Scientific Applications on Commodity Clusters: An Empirical Study of a Tapered Fat-Tree]] :PAPER:
Bibtex: Leon:2016:CPS:3014904.3015009
- The authors want to characterize the behavior of applications that run on their clusters, with an emphasis on
  communication requirements.
- This should help to make more informed choices when building new clusters (should we use our budget to get more links
  or more nodes?).
- They measured the utilization of their cluster during one week. It has a fat tree topology.  The measurements show
  that the network is not used very much: the maximal link utilization is approximately 50%, the average link
  utilization is 2.4%.
- They did the same measures with a tapered fat tree (they removed one of the root switches).  Except for some outliers
  having a 90% link utilization at some point, this modification did not had a major impact on the link utilization,
  which was 3.9%.
- The authors recorded which type of jobs were submitted. A great majority of them was really small.  95% of jobs have
  at most 16 nodes, 76% have only one node. Jobs of less than 64 nodes consume 75% of the time.  Thus, if the jobs are
  well placed, the need for distant communications is very low, which explains the good performances of the tapered fat
  tree.  Of course this may change from one cluster to another, so we should reproduce these measurements and make our
  own conclusions.
- Then, the authors removed one of there two top switches.
- A first micro-benchmark shows that it only impacts the aggregate bisection bandwidth, for large messages (> 32kB).
- Then, they evaluated the impact of the tapering on the performances of several “real-life” applications.
- They found that only one of these applications was sensible to the tapering. This application does collective
  communications as well as point-to-point commulnications of large messages.
- However, the impact on the execution time of this application remains small: only 1-2% (it impacts its communication
  time by 6-7.5% which itself accounts for only 9-15%). Furthermore, this only happens for a large number of nodes (>
  512).
- Finally, the authors claim that next generation hardware (faster CPU, memory and network, accelerators...) will lead
  to some rewriting of the application to leverage this new hardware. In some applications, message sizes will be
  larger.  Thus, a tapered fat tree may have more impact with this new hardware, new experimentations will be needed to
  find out.
**** DONE Some thoughts regarding previous paper, to discuss with Arnaud :PAPER:MEETING:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-03-03 Fri 17:51]
:END:
***** Can we have data about the utilization of clusters we will work on (Stampede, Bull)?
- It would help us to find relevant hypothesis (e.g. “pruning the fat-tree will not have any impact”).
- We need this for the simulation. What kind of jobs should we run? Small ones? Large ones?
***** Can we have data about the nature of the jobs submitted on these clusters?
- What are these applications?
- What fraction of the time do they use for communications?
- Small or large messages?
- Again, it will help us to make hypothesis and perform meaningful experimentations.
\to It changes a lot from one cluster to another, or even across time. It is also hard to record (a batch scheduler does
not know the nature of the jobs that it handles).
***** How to simulate “big nodes”?
- Can we simulate MPI+OpenMP programs with Simgrid?
- The paper from Christian explains briefly how Simgrid simulates multi-core machines (with one MPI process per core, no
  threads).  Why don't they talk about it in the other paper? Both papers are from the same year.
\to It would be very hard to support OpenMP in Simgrid, the standard is quite big. Also, in OpenMP, communications are
made with shared memory, so much more difficult to track than MPI communications.
***** Are time-independent traces larger than the “classical” traces?
\to No, same thing.
***** With ScalaTrace, traces have “near-constant size”. How?
\to Compression, lossless or lossy.
***** What is “detached mode” in point-to-poin communication?
- Does the OS of the sender interrupt it, to ask it to send the data?
- If so, why is large mode slower for the sender? In detached mode, the sender has to stop what it is doing,
  whereas in synchronous mode it is waiting.
\to Yes, the kernel interrupts the sender when the receiver is ready. Simgrid does not model the small messages used for
the synchronization.
***** What does the community think of closed source simulators, like xSim? Researchers behind xSim are doing strong claims that
cannot be verified by independent researchers...
***** Why are there never confidence intervals in the plots of Simgrid's papers?
\to They are often not needed, because of too small variation.
***** About the paper Git/Org-mode
- Is there an implementation somewhere?  Creating custom git commands seems [[http://thediscoblog.com/blog/2014/03/29/custom-git-commands-in-3-steps/][really easy]].
  \to Yes, but not packaged yet. To test the “beta version”, ask Vincent.
- Was not convinced by the subsection 4.3.3 (“fixing code”). When terminating an experiment, they revert all the changes
  made to the source code since these may be ad hoc changes. Then the user has to cherry pick the changes (s)he wants to
  keep. Sounds really dirty...  It seems better to have generic scripts that you configure by giving command line
  arguments and/or configuration files.  Then you can simply put these arguments/files in the journal.
***** Routing in Simgrid (according to the doc)
- Routing tables are static (to achieve high performance).  → Does it mean that handling link failures and dynanmic
  re-routing will require a large code refactoring? What about the performance penalty?
- Routing algorithms are either based on short path (e.g. Floyd, Dijkstra) or manually entered. What about “classical”
  algorithms like D-mod-K?  An example is provided on [[https://github.com/simgrid/simgrid/blob/master/examples/platforms/cluster_fat_tree.xml][Github]]. The example implements a two levels fat-tree with
  D-mod-K. However, D-mod-K is not specified in the XML, it seems to be implicit. Does it mean that we are forced to use
  this routing algorithm for fat trees?
\to Read the code. Shortest path routing is a feature introduced by some Belgian researchers. For specific topologies like
fat-trees, the routing algorithm is hard-coded.
**** DONE Read [[file:hal-01415484.pdf][Simulating MPI applications: the SMPI approach]]      :PAPER:
Bibtex: degomme:hal-01415484
- This paper is about simulation of HPC systems.
- The authors claim that some research papers are based on simulation made with one-off programs with poor
  documentation, making simplifying assumptions. Worse, these programs are sometimes not public. This is a big issue for
  reproducibility.
- The whole paper consider several important aspects that a good simulator should take care of.
- Several use cases for simulation.
  + Quantitative performance evaluation (what will be the performances if we take a bigger version of our hardware?).
  + Qualitative performance evaluation (what will be the performances if we take different hardware?).
  + Detection of hardware misconfiguration (leading to unexpected performance behaviors).
  + MPI runtime tuning (e.g. choosing the algorithms of MPI collective operations).
  + Teaching (supercomputers are expensive, we cannot let the students play with them).
***** Capturing the behavior of an application.
- Off-line simulation. A trace of MPI communication events is first obtained and then replayed.
  + We measure the durations of the CPU bursts. Then, when replaying the application, we modify them to account for the
    performance differences between the target platform and the platform used to get the traces.
  + One problem is the size of the traces, which can be very large. To fix this, we may only record aggregated
    statistics. They can be enough to detect some anomalies, but we cannot do more in-depth analysis.
  + Another issue is extrapolation. Being able to extrapolate in the general case require assumptions hardly justifiable.
  + In SMPI, they use “time-independent traces”. Instead of recording time durations, they log the number of
    instructions and the number of bytes transferred by MPI primitives. These are independent of the hardware, so the
    extrapolation issue is fixed.
  + It does not solve anything for applications that adapt their behavior to the platform. But this is hopeless with
    off-line simulation.
  + There is still the issue of very large traces, they grow linearly with the problem size and the number of
    processes. It seems to be fixed by ScalaTrace, but no explanation is given.
- On-line simulation. The actual application code is executed, part of the instruction stream is intercepted and passed
  to a simulator.
  + Several challenges. Intercepting MPI calls. Interractions between the application and the simulation kernel.
    Obtaining full coverage of MPI standard. Over-subscribing resources.
  + Several possibilities to capture MPI calls. Use PMPI interface (provided by every MPI implementation), but limited
    to the high- level calls. Design a specific MPICH or OpenMPI driver, but tie the solution to a specific
    implementation. One can also develop an ad-hoc implementation of the MPI standard.
  + Many tool fold the application into a single process with several threads. This raise an issue for global variables,
    they must be protected. One can duplicate the memory area of the global variables, or use a trick based on the
    Global Offset Table (GOT).
  + SMPI is based on a complete reimplementation of MPI standard. No full-coverage yet (e.g. remote memory access or
    multithreaded MPI applications).
  + Run MPICH internal compliance tests as part of their automatic testing.
  + To protect global variables, duplicate their memory zone using mmap (smart thing, much more efficient thanks to
    COW).
***** Modeling the infrastructure (network and CPU)
- Network modeling.
  + Several solutions exist to modelize the network.
  + Packet-level simulation, here we look at individual packets. It is very precise, but it is hard to know precisely
    what we are modeling. Being precise with a wrong model is useless. Moreover, this model is very costly in terms of
    simulations.
  + Flow model. The finest grain here is the communication. Time to transfer a message of size S from i to j: L_i,j + S*B_i,j. The
    B_i,j are not constant, they need to be evaluated for every moment. This model catch some complex behaviors (e.g. RTT unfairness
    of TCP). Quite complex to implement, more costly than the delay model. Also, until recently, contentions could be neglected.
  + Delay model, we have some equations to describe the communication times (e.g. LogP, LogGPS). It is elegant and cheap
    in terms of simulations, but very unprecise. Does not take into account network topology (and eventual contentions)
    and suppose a processor can only send one message at a time (single-port model).
  + SMPI uses a hybrid network model. Point-to-point communications are divided in three modes: asynchronous, detached
    and synchronous.  Each mode has different values of bandwidth and latency, estimated by doing some benchmarks and
    then a linear regression.
  + To modelize network contentions, SMPI has three logical links for any physical link: a downlink, an uplink, and a
    limiter link.  The bandwidth of uploads (resp. downloads) must be lower than the capacity of uplinks
    (resp. downlinks). The sum of the bandwidths must be lower than the capacity of the limiter link.
- CPU modeling.
  + Like network modeling, several solutions.
  + Microscopic models, very precise, but also very costly.
  + Models with a coarser grain. For instance, we neglect the CPU load induced by communications → focus on Sequential
    Execution Blocks (SEB).
  + Most simplistic model: “CPU A is x times faster than CPU B”. Results ok for similar architectures, but not precise
    at all if too different.  For instance, number of registers, number of hypertreading cores, speed of floating point
    computations, bandwidth to memory, etc.
  + Thus, impossible to predict precisely without a perfect knowledge of the system state (and therefore a microscopic
    model).
  + Approach of SMPI: run SEB on a processor of the target architecture. Predict performances of *similar* architecrues by
    applying a constant factor.
  + Also, not all the code logic is data dependent. We can therefore greatly decrease the simulation time with two
    tricks.
    - Kernel sampling. Annotate some regions with macros. Execute them only a few times to obtain estimations, then skip
      them.
    - Memory folding. Share some data structures across processes.
***** Modeling the collective operations
- Again, several solutions for the modelization.
- More analytical ones: each collective operation has a cost equation (depending for instance on the message size and
  the number of processes). As discussed for the network modelization, such approaches do not catch the eventual network
  contention.
- Another approach is to benchmark each collective operation on the target platform, with various parameters and
  communicators.  Then, the obtained timings are reinjected in the simulation. We cannot do performance extrapolation
  with this approach. Also, the benchmarking phase may be very long.
- Some replace every collective operation by the corresponding sequence of point-to-point communications (at compile
  time).  This does not capture the logic of selecting the right algorithm.
- Others capture this decomposition into point-to-point communication during the execution, then replay it. But this is
  limited to off-line analysis.
- Simgrid implements all the collective algorithms and selection logics of both OpenMPI and MPICH. We are sure to
  capture correctly the behavior of the operations, but this is an important work. Another interesting feature is that
  the user can chose the selector or the algorithm from the command line.
***** Efficient simulation engine
- Rely on a efficient Discrete Event Simulation (DES) kernel.
- Some simulators parallelized this part (using MPI). But this results in a more complex implementations.
- In the way Simgrid works, there is not much potential parallelism. They therefore decided to keep a sequential DES.
- Simulation cost comes from the application itself (which can be greatly reduced, CPU modelization) and from the flow
  level model.
***** Evaluation
Here, the authors show that the use cases mentionned at the beginning of the paper are all realised by Simgrid.
- Simgrid is very scalable, more than xSim which is already one of the most scalable simulators (self proclaimed).
- Kernel sampling and memory folding enable simulations of non-trivial applications with a very large number of cores.
- Then, the ability to make good predictions is demonstrated with a Mont Blanc project example. Here, Simgrid is much
  closer to the reality than LogGPS model. However, no comparison is done with other simulators, so this result is hard
  to evaluate.
- A quantitative performance extrapolation is demonstrated, showing good results.
- Empirically, the largest error made by SMPI in terms of time prediction is 5%. This allow to use SMPI to detect
  hardware misconfiguration.  Indeed, it already happened to the Simgrid team.
- Similarly to the previous point, the good accuracy of SMPI allow to investigate to find which MPI parameters lead to
  the best performances.
- Finally, for obvious reasons, using a simulator is great for teaching MPI (rather than using a real cluster).
***** Conclusion
- The paper focused on MPI applications.
- But Simgrid has other use cases: formal verification of HPC applications, hybrid applications (MPI+CUDA).
**** DONE Read [[file:hal-01446134.pdf][Predicting the Performance and the Power Consumption of MPI Applications With SimGrid]] :PAPER:
Bibtex: heinrich:hal-01446134
- The paper is about using Simgrid to predict energy consumption.
- This is a challenging question, the modelization is tricky.
  + Power consumption of nodes has a static part (consumption of the node when idle) and a dynamic part.
  + The static part is very significant (~50%), so we should really do something when the core is idle.
  + A first solution is to power off the node, but the latency to power it on is large.
  + Another solution is to use Dynamic Voltage Frequency Scaling (DVFS). This is not limited to the case where the core
    is idle, it can also be used when the load is low but non-null. Performance loss is linear in the decrease of the
    frequency, but the power consumption is quadratic.
  + No other HPC simulator than Simgrid embed a power model yet.
***** Modeling multi-core architecture
- If two processes are in the same node (either a same core, or two cores of a same CPU), the simulation becomes tricky.
  + The “naive” approach is to simply give a fair share to each of htese processes. But it does not take into accoutn
    some memory effects.
  + Simgrid can be pessimistic for processes heavily exploiting the L1 cache. In the simulation, the cache will be cold
    after each MPI call, in reality the cache would be hot.
  + Simgrid can be optimistic for processes heavily exploiting the L3 cache and the memory. In the simulation, they will
    have exclusive access, in reality they will interfer between each other.
***** Modeling energy consumption
- The instantaneous power consumption is P_i,f,w(u) = Pstatic_i,f + Pdynamic_i,f,w * u, for a machine i, a frequency f,
  a computational workload w and a usage u.
- In general, we assume that Pstatic_i,f = Pstatic_i (idle state, the frequency does not matter).
- Users can specify arbitrary relation (linear in the usage) for each possible frequency (in general, they should be
  quadratic in the frequency, but it may change with new technologies).
- Each machine can have its own model, accounting for heterogeneity in the platform.
- Power consumption of each host is exposed to the application, allowing it to dynamically decide to change (or not) the
  current frequency.
***** Modeling multicore computation
- A first step is to run the target application with a small workload using all the cores of a single node, on the target platform.
- Then, re-execute the application with the same workload on top of the simulator (hence using a single core).
- From these measures, associate to each code region a speedup factor that should be applied when emulating.
- In some applications, speedups are very close to 1. In other applications, some regions have a speedup of 0.16 while
  other regions have a speedup of 14. Not taking this into accoutn can result to a large inaccuracy (~20-30%).
***** Modeling the network
- See the other paper for the details on the network model of SMPI.
- The authors also speak about local communications, within a node. They are implemented with shared memory. The model
  here is also piecewise linear, but with less variability and higher speed. However, they did not implement this model,
  they kept the classical network model since local communications were rare enough.
***** Validation
- The authors obtain a very good accuracy for performance estimations (as stated in the previous paper).
- For two of the three applications, they also have a very good accuracy for energy consumption estimations.
- With the last application, the accuracy is bad. The reason is that the application (HPL) does busy waiting on
  communications (with MPI_Probe). In the current model, they assume that it does not cost energy.
***** Experimental environment
Minor modifications to the setup can have a major impact on the performances and/or the power consumption.  The authors
therefore give a list of settings to track.
- Hardware. If we suppose that the cluster is homogeneous, it has to be the case. Two CPU having the same type can still
  exhibit different performances (e.g. if they come from two different batches/factories).
- Date of the measurements. A lot of things having an impact can change in time: temperature of the machine room,
  vibrations, BIOS and firmware version, etc.
- Operating system. The whole software stack and how it is compiled can have a huge impact. Also, always observe a delay
  between the boot and the beginning of experiments.
- Kernel configuration. For instance, its version, the scheduling algorithm, technologies like hyperthreading, etc.
- The application itself and the runtime (e.g. the algorithms used for collective operations).
***** Conclusion / future work
- The approach to simulate power consumption is accurate only if the application is regular in time. To handle
  applications with very different computation patterns, we could specify the power consumption for each code
  region. But to do so, Simgrid has to be modified, and there need to be very precise measurements to instantiate the
  model (impossible with the hardware of Grid 5000, sampling rate of only 1Hz).
- In Simgrid, we can currently not have different network models at the same time, to account for local and remote
  communications.  A refactoring of the code is underway to fix this.
*** 2017-02-08 Wednesday
**** DONE Paper reading.
- Notes have been added in the relevant section.
- One paper read today: “Simulating MPI applications: the SMPI approach”.
*** 2017-02-09 Thursday
**** TODO Read [[file:hdr.pdf][Scheduling for Large Scale Distributed Computing Systems: Approaches and Performance Evaluation Issues]] :PAPER:
Bibtex: legrand:tel-01247932
**** DONE Read [[file:SIGOPS_paper.pdf][An Effective Git And Org-Mode Based Workflow For Reproducible Research]] :ORGMODE:GIT:PAPER:
Bibtex: stanisic:hal-01112795
- A branching scheme for git, based on four types of branches.
  + One src branch, where the code to run the experiments is located. This branch is quite light.
  + One xp branch per experiment, that exists only during the period of the experiment. We can find here all the data
    specific to this experiment. Also a light branch, since limited to an experiment.
  + One data branch, in which all xp branches are merged when they are terminated. Quite an heavy branch, a lot of things.
  + One art branch per article, where only the code and data related to the article are pulled from the data branch.
  When an xp branch is merged in data and deleted, a tag is added. Then, we can easily checkout to this experiment in the future.
- Org-mode used as a laboratory notebook. All details about the experiments (what, why, how...) are written here.
  Thanks to literate programming, the command lines to execute are also contained in the notebook.
**** Presentation about org-mode by Christian.                   :ORGMODE:
- Have a per day entry in the journal.  If you work more than an our wthout writing anything in the journal, there is an
  issue.
- Put tags in the headlines, to be able to search them (e.g. :SMPI: or :PRESENTATION:).  Search with “match” key word.
  Hierarchy of tags, described in the headline.
- About papers, tags READ/UNREAD. Also the bibtex included in the file.  Attach files to the org modeo (different than a
  simple link). Use C-a, then move.
- Spacemacs: add a lot of stuff to evil mode.
- Can also use tags to have link on entries, use the CUSTOM_ID tag.
- Can use org mode to put some code.
**** DONE Paper reading.
- One paper read today: “Predicting the Performance and the Power Consumption of MPI Applications With SimGrid”.
- Notes have been added in the relevant section.
**** DONE Apply the things learnt at the org-mode presentation.
*** 2017-02-10 Friday
**** Tried to get good org-mode settings.                        :ORGMODE:
- Cloned org-mode [[/home/tom/.emacs.d/org-mode][git repository]] to have the latest version (success).
- Tried to install [[http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php][Arnaud's configuration file]] (fail).
- Will try Christian's configuration file on Monday.
**** DONE Paper reading.
- One paper read today: “An Effective Git and Org-Mode Based Worflow for Reproducible Research”.
- Notes have been added in the relevant section.
**** Begin looking at the documentation.
- Documentation about the [[http://simgrid.gforge.inria.fr/simgrid/3.13/doc/platform.html][topology]].
**** Run a matrix product MPI code in a fat tree
- Code from the parallel system course.
- Tried [[https://github.com/simgrid/simgrid/blob/master/examples/platforms/cluster_fat_tree.xml][Github]] example (fat tree =2;4,4;1,2;1,2=, 2 levels and 16 nodes).
- Tried a personal example (fat tree =3;4,4,4;1,4,2;1,1,1=, 3 levels and 64 nodes).
**** DONE Find something to automatically draw a fat tree.
  + Maybe there exists some tools? Did not find one however.
  + Maybe Simgrid has a way to export a topology in a graphical way? Would be very nice.
  + Could adapt the Tikz code I wrote during 2015 internship?
*** 2017-02-13 Monday
**** Keep working on the [[file:/home/tom/Documents/Fac/2017_Stage_LIG/small_tests/matmul.c][matrix product]].                      :SMPI:C:BUG:
- Observe strange behavior.
  + Commit: 719a0fd1775340628ef8f1ec0e7aa4033470356b
  + Compilation: smpicc -O4 matmul.c -o matmul
  + Execution: smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 -np 64 -hostfile ./hostfile_64.txt
    -platform ./cluster_fat_tree_64.xml ./matmul 2000
  Then, processes 0 and 63 behave very differently than others.
  + Processes 0 and 63 have a communication time of about 0.21 and a computation time of about 1.52.
  + Other processes have a communication time of about 0.85 and a computation time of about 0.75.
  With other topologies and/or matrix sizes, we still have this behavior (more or less accentuated).
- If we change the order of the loops of the sequential matrix product from i-j-k to k-i-j:
  + The execution time is shorter. Hypothesis: this solution has a better usage of the cache.
  + The computation times are decreased (expected), but the communication times are also decreased (unexpected).
  + Still observe the same trend than above for processes 0 and 63.
- Checked with some printf: all processes are the root of a line broadcast and of a column broadcast exactly once (expected).
- Tried several broadcast algorithms (default, mpich, ompi), still have the same behavior.
- Adding a call to MPI_Barrier at the beginning of the for loop fix the issue for the communication (all processes now
  have a communication time of about 0.22) but not for the computation (still the same differences for processes 0 and
  63).
- When using a smaller numbmer of processes (16 or 4), communication times and computation times are more consistent
  (with still some variability).
- With one process and a matrix size of 250, we have a computation time of 0.10 to 0.12. When we have 64 processes and a
  matrix size of 2000, each block has as ize of 250. Thus, we can extrapolate that the “normal” computation time in this
  case should be about 0.8 (8 iterations, so 8*0.10). Thus, processes 0 and 63 have a non-normal behavior, the others
  are ok.
- Also tried other topologies, e.g. a simple cluster. Still have the same behavior (with different times).
  + Again, normal behavior with less processes (e.g. 16).
  + We get a normal behavior if we take hostfile_1600.txt, very strange.
- Bug fixed, the problem came from the hostfile. For some unknown reason, it missed a end-of-line character at the last
  line. I suspect that two processes (0 and 63) were therefore mapped to a same host, because the last host was not
  parsed correctly by smpi. The two versions of the file have been added to the repository.
- Issue reported on [[https://github.com/simgrid/simgrid/issues/136][Github]].
**** Try to optimize the matrix product code.                     :SMPI:C:
- For the record, the following command yields communication times between 0.27 and 0.31 and computation times between
  0.78 and 0.83, for a total time of about 1.14: smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  -np 64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml ./matmul 2000
- Replaced malloc/free by SMPI_SHARED_MALLOC/SMPI_SHARED_FREE. Got similar times (approximately).
- Added SMPI_SAMPLE_GLOBAL(0.5*size, 0.01) to the outer loop of the sequential matrix product. Got similar times (approximately).
- Remark: we should verify more rigorously that these optimizations do not change the estimated time.
- Greatly reduced simulation time (from 8.2s to 0.5s).
- Other optimization: stop initializing the content of the matrices (since we do not care of their content).
**** Meeting with Arnaud.                                        :MEETING:
- There exists some visualization tools for Simgrid, to see the bandwidth that goes on some links.  May be very useful
  in the future, to have a better understanding of what is going on.
- The characteristics of the jobs (number of nodes, patterns of communication) have an important impact on performances.
  However, it is difficult for us to have access to this, we do not own a supercomputer... Maybe Matthieu can have more
  information (e.g. from Bull's clients)?
**** DONE Add supervisors on Github for the journal.
**** Some quick performance tests.                      :SMPI:EXPERIMENTS:
- Run my matrix product code, with SMPI optimizations.
- Use a 2-level fat-tree made with switches of 48 ports.
- First case: non-tapered. We use all the switches. The fat-tree is 2;24,48;1,24;1,1 (total of 1152 nodes).
  + Use 1089 processes, matrix size of 4950.
  + Time: 1.75s.
  + Communication time: 0.94s.
  + Computation time: 0.81s.
- Second case: tapered. We remove half of the root switches. The fat-tree is 2;24,48;1,12;1,1 (still 1152 nodes).
  + Still uses 1089 processes, matrix size of 4950.
  + Time: 1.78s.
  + Communication time: 0.94s.
  + Computation time: 0.82s.
- The observed difference does not seem significant, but we should check with a carefully designed experiment and
  analysis.
- For the record, running the same application on the same topology with only one process takes a time of 3607s.  Thus,
  we have a speedup of about 2026, so an efficiency of 1.86.  This is a very nice speedup (superlinear). Certainly due
  to cache effects.
- These quick tests suggest that we could remove root switches without impacting the performances, even if we use nearly
  the whole fat-tree (this is obvious if we use a small subtree).
**** DONE Run another benchmark (e.g. HPL), with more carefully designed experiments.
**** DONE The 3-level fat-tree was very long to load (aborted). Find why.
*** 2017-02-14 Tuesday
**** Work on experiment automatization.                           :PYTHON:
- Add Python functions to generate topology and host files from a given fat-tree description.
- Adapt Python script and Jupyter notebook from parallel system course to run experiments.
- The matrix size and the number of processes are fixed. We compute matrix products for various numbers of root switches
  (we test fat-trees (2;24,48;1,n;1,1) for n in [1, 24]).
- Results seem very promising. For a matrix size of 6600, we can have as few as 10 root switches without important
  impact on performances (recall that a typical 2-level fat tree with 48 port switches would have 24 root switches). If
  we keep removing switches, then performances are quickly impacted.
- Repeated the experiment with the same topology and the same matrix size, but with only 576 processes.  We observe a
  same trend, we can remove a lot of root switches without having an impact.
**** DONE Ask if it would be possible to have an SSH access to some dedicated computer.
- Does not need to have a lot of cores (Simgrid is not a parallel program), but it would be nice if it had a fast core.
- Needs to be dedicated so as to not perturbate the experiments.
**** Webinar on reproducible research: [[https://github.com/alegrand/RR_webinars/blob/master/7_publications/index.org][Publication modes favoring reproducible research]] :MEETING:
- Speakers: [[http://khinsen.net/][Konrad Hinsen]] and [[http://www.labri.fr/perso/nrougier/][Nicolas Rougier]].
- Two parts in research: dissemination (of the results/ideas) and evaluation (of the researcher).
- If we want reproducible research to become a norm, researchers should be rewarded for this (their reputation should
  also depend on the reproducibility of their research, not only the number of citations or the impact factor).
- The speaker compares reproducible research for two points of view: human part and computer part, both for
  dissemination and evaluation.
***** [[http://www.activepapers.org/][ActivePapers]]
- Not a tool that one should use (yet), neither a proposition of new standard. It is mainly an idea for computer-aided
  research.
- How to have more trusts on the software? The “ideal” one is reimplementation (e.g. ReScience). The speaker tried this
  on a dozen projects, he never got identical results. Other good ideas: good practices like verison control and
  testing, keep track of the software stack (hardware, OS, tools, etc).
- ActivePapers group scripts, software dependencies and data into a same archive.
***** [[http://rescience.github.io/][ReScience]]
- Idea: replicate science.
- For a great majority of papers, we cannot replicate their reuse their code.
- It is hard to publish replication of an original paper, most journals will reject it since not original.
- This is why ReScience was born. It is (currently) used on Github.
- To publish a new study, do a pull-request on ReScience repository. Then it is reviewed openly by reviewers selected by
  the editor.  The replication is improved until it is publishable.
*** 2017-02-15 Wednesday
**** Use Christian’s config files for org mode                   :ORGMODE:
**** Work on the experiment script
     - Parsing more generic fat-tree descriptions. For instance, our current topology description would be
       2;24,48;1,1:24;1,1.  It means that the L1 switches can have between 1 and 24 up ports.
     - Modify the script for experiments to be more generic.
       + Can give as command line arguments the fat-tree description, the (unique) matrix size, the (unique) number of processes.
       + Use Python’s argparse for a cleaner interface.
**** Re-run experiments with this new script
     - Still observe the same trend: we can afford to remove a lot of up-ports for the L1 switches.
     - Some points seem to be outliers. But we have not a lot of points, so it is difficult to say. We whould do more
       experiments to see if these points are still significantly separated from the rest.
*** 2017-02-16 Thursday
**** DONE Enhance/fix Emacs configuration                        :ORGMODE:
- Translate days and months in English.
- Increase the line length limit (120 columns?).
- Reformat the whole document with such limit.
- Add tags where relevant.
- Attach files, instead of putting a link.
**** Try to use even more SMPI optimizations                        :SMPI:
- Currently, we use the macro SMPI_SAMPLE_GLOBAL only once: for the outer for loop of the sequential matrix product.
- Maybe we can also use it for the two other loops? We could also reduce the number of iterations (currently, it is
  0.5*size). Let’s try.
- Currently, we get communication times of about 0.14s and computation times of about 0.42s, for a total time of 0.57s,
  with the following command:
  smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 -np 64 -hostfile ./hostfile_1152.txt -platform
  ./big_tapered_fat_tree.xml ./matmul 1600
- FAIL. It seems we cannot use imbricated sample blocks. Quite strange, do not understand why...
**** Try to run HPL with Simgrid                                :SMPI:HPL:
- Copied from [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/tree/master/src/hpl-2.2][Christian’s repository]].
- Compilation fails, don’t know why. But binaries are stored in the git repository (don’t know why either), so I can use
  them to do some first tests.
  In fact, file Make.SMPI needed to be modified. Changed =mkdir= by =mkdir -p=, =ln= by =ln -f= and =cp= by =cp -f=.
  Changed top directory.
  Also, the Makefile couldn’t find the shared library atlas. It was in /usr/lib, but named =libatlas.so.3=. Added a
  symbolic link to =libatlas.so=.
- Tested my laptop (with MPI, not SMPI). With a problem size of 10000 and 12 processes, it corresponds to 16.51 Gflops.
- Tested with SMPI, with a problem size of 10000 and 4 processes. Command:
  smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 -platform
  ../../../small_tests/cluster_fat_tree_64.xml -hostfile ../../../small_tests/hostfile_64.txt -np 4 ./xhpl
  Result: 1.849Gflops.
- Same thing, with 12 processes. Very similar: 1.847Gflops. Why is it not faster?
- Same thing, with 64 processes. Very similar: 1.858Gflops. Why is it not faster?
- Retried with a freshly compiled program. Still the same thing.
- Understood the issue: it is not enough to specify the number of processes with =-np 12=, we also have to tell it in the
  file =HPL.dat=.
- Tried with =-np 4=, P=4 and Q=1. Now, 6.6224Gflops. We have a speedup of 3.59, which seems reasonable.
- The number of processes given with =-np= must be greater or equal to P \times Q.
- Tried with =-np 4=, P=1 and Q=4. Did not have a noticeable impact on performances (in comparison with P=4, Q=1).
- Tried with =-np 4=, P=2 and Q=2. Did not have a noticeable impact on performances (in comparison with P=4, Q=1).
- Tried with =-np 64=, P=8 and Q=8. Now, 22.46Gflops. Speedup of 12, very disappointing.
- Tried with =-np 64=, P=8 and Q=8 again, but with a problem size of 20000 (it was 10000). Now 52.2Gflops (speedup of 28.3).
**** Comparison with top 500
- For the record, the order of magnitude for Intel desktop CPU of today is between 10 and 100 Gflops, according to [[https://www.pugetsystems.com/labs/hpc/Linpack-performance-Haswell-E-Core-i7-5960X-and-5930K-594/][this
  website]], [[https://setiathome.berkeley.edu/cpu_list.php][this website]] and [[https://asteroidsathome.net/boinc/cpu_list.php][this website]]. My laptop supposedly has a speed of 3.84 Gflops per core and 15.21 Gflops in
  total according to the last two websites.
- According to [[https://en.wikipedia.org/wiki/Raspberry_Pi#Performance][Wikipedia]], the first generation Raspberry Pi has a speed of 0.041 Gflops, a 64 nodes cluster made of
  those has a speed of 1.14 Gflops.
- The first supercomputer has a speed of about 93Pflops, or 93,000,000Gflops.
- The last one has a speed of about 349Tflops, or 349,000Gflops.
- In June 2005, the first one had a speed of about 136Tflops, the last one 1.2Tflops.
- In our settings with 64 nodes, each node has one core that computes at 1Gflops. Thus, our Rpeak is 64Gflops.  We have
  an efficiency of 52.2/64 = 0.81.  This is not bad, compared to the three first supercomputers of the top 500
  (respectively at 0.74, 0.61 and 0.63). But we should maybe not compare the efficiency of a 64 nodes cluster with these
  supercomputers, since it becomes harder to be efficient with a large topology.
**** DONE SMPI optimization of HPL                              :SMPI:HPL:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-03-22 Wed 17:08]
- State "TODO"       from              [2017-02-16 Thu 16:17]
:END:
- It seems that no SMPI optimization is done in the code obtained from Christian’s repository. Maybe we could speed
  things up?
- Need to check what is the algorithm behing HPL, whether it is regular (to use SMPI_SAMPLE) and data independent (to
  use SMPI_SHARED).
**** DONE Adapt the experience script to run HPL
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-02-17 Fri 15:47]
- State "TODO"       from              [2017-02-16 Thu 17:03]
:END:
- Parse the output (quite ugly to parse, but easy, use methods str.split and list.index).
- Run the same kind of experiments than for the matrix product. Will be much longer if we cannot use SMPI optimizations.
*** 2017-02-17 Friday
**** Refactor the experiment script                               :PYTHON:
- Aim: reuse for HPL the code already done for the matrix product.
- Now, we have a clase =AbstractRunner=, which runs the common logic (e.g. some basic checks on the parameters, or running
  the desired number of experiments).
- We also have classes =MatrixProduct= and =HPL=, containing the piece of codes specific to the matrix product or HPL (e.g. running one experiment).
**** Some strange things with HPL                           :SMPI:BUG:HPL:
- The output has the following format:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2        2000   120     1     1               3.17              1.683e+00
  #+end_example
- Sometimes, the last line is missing, so we do not have any informaiton on time and flops.
- Quite often it is present, but with wrong values: the time is 0.00 and the Gflops are absurdly high (e.g. 2.302e+03
  Gflops for a cluster made of 96 machines of 1 Gflops). It may come from an erroneous measure of the time.
- For instance, with the script of commit =dbdfeabbef3f90a3d4e2ecfbe5e8f505738cac23=, the following command line:
  =./run_measures.py --global_csv /tmp/bla --nb_runs 10 --size 5000 --nb_proc 64 --fat_tree "2;24,48;1,24;1,1"
  --experiment HPL=
  + It may get this output in one experiment:
  #+begin_example
    ================================================================================
    T/V                N    NB     P     Q               Time                 Gflops
    --------------------------------------------------------------------------------
    WR00L2L2        5000   120     8     8               0.00              1.108e+05
  #+end_example
  + And this output in another one:
  #+begin_example
    ================================================================================
    T/V                N    NB     P     Q               Time                 Gflops
    --------------------------------------------------------------------------------
    WR00L2L2        5000   120     8     8               5.35              1.560e+01
  #+end_example
  Note that, for the two experiments, *nothing* has changed. The file =HPL.dat= is the same, the number of processes given
  to the option =-np= is the same, the topology file and the host file are the same.
*** 2017-02-20 Monday
**** Keep investigating on the HPL anomaly
**** Found the issue with HPL                               :SMPI:BUG:HPL:
- Debugging with Christian, to understand what was going on.
- This was a concurrency issue. The private variables of the processes were in fact not private. This caused two
  processes to write a same variable, which led to an inconsistent value when measuring time.
- The function is =HPL_ptimer=, in file =testing/ptest/HPL_pdtest.c=.
- When using simgrid, need to use option =--cfg=smpi/privatize-global-variables:yes= to fix this.
- Used a tool to search for a word, looks nice: =cg= and =vg= (package =cgvg=).
- Another nice thing: =ctags= (command =ctags --fields=+l -R -f ./ctags src testing=).
*** 2017-02-21 Tuesday
**** Test the experiment script for HPL                  :EXPERIMENTS:HPL:
- It seems to work well, the bug is fixed.
- Scalability issue. Testing for a size of 20k already takes a lot of time, and it is still too small to have a good
  efficiency with 1000 processes (performances are worse than with 100 processes).
- Definitely need to use SMPI optimizations if we want to do anything with HPL.
**** Re-do experiments with matrix product 
- Stuck with HPL...
- We also output the speed of the computation, in Gflops (this is redondant with the time, but we can use it for
  comparison with other algorithms like HPL).
- The plot looks nice, but nothing new.
**** Work on the drawing of fat-trees
- Generate all nodes and edges of a fat-tree.
- No drawing yet.
- Will try to output Tikz code.
**** DONE Look at where to put SMPI macros in HPL, with Christian
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-02-22 Thu 13:17]
- State "TODO"       from              [2017-02-21 Tue 15:03]
:END:
- Have a look at a trace, to see where most of the time is spent.
**** Keep working on the drawing of fat-trees.
- Now produce working Tikz code.
- Figure quickly becomes unreadable for large fat-trees (not surprising).
*** 2017-02-22 Wednesday
**** Terminate the work on fat-tree drawing                       :PYTHON:
- We can now do =./draw_topo.py bla.pdf "2;8,16;1,1:8;1,1" "2;4,8;1,1:4;1,1"= to draw all the fat-trees in the file
  =bla.pdf=. Very useful to visualize the differences between the trees.
- No limit on the fat-tree size, they should fit on the pdf (a very large page is generated, then cropped to the right
  dimension). However, a large fat-tree may not be very readable.
**** Tried to move the SMPI_SAMPLE of the matrix product
- Cannot use one SMPI_SAMPLE per loop (don’t know why, but it seems to be forbidden).
- It was used for the outer loop. Tried the inner loops, but performances were greatly degraded (about \times50 in simulation time).
- Reverting the change.
**** DONE Cannot use more than 1024 processes with Simgrid (need to fix) :SMPI:BUG:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-02-23 Thu 10:20]
- State "TODO"       from              [2017-02-22 Wed 14:12]
:END:
- The =open()= system call fails with =EMFILE= error code.
- It used to work, don’t understand what changed in the meantime.
**** Talk with Christian about SMPI optimizations in HPL :PERFORMANCE:HPL:
- He gave me a trace of HPL execution obtained with Simgrid.
- The parts taking most of the time are the following:
  #+begin_example
  50        /home/cheinrich/src/hpl-2.2/src/pgesv/hpl_rollt.c       242          /home/cheinrich/src/hpl-2.2/src/comm/hpl_recv.c     136 190.785263    498
  51        /home/cheinrich/src/hpl-2.2/src/pgesv/hpl_rollt.c       242          /home/cheinrich/src/hpl-2.2/src/comm/hpl_sdrv.c     180 372.272945    996
  52        /home/cheinrich/src/hpl-2.2/src/pgesv/hpl_rollt.c       242          /home/cheinrich/src/hpl-2.2/src/comm/hpl_send.c     133 179.711679    498
  #+end_example
**** Let’s track these piece of code                     :PERFORMANCE:HPL:
- =HPL_rollT.c= has only one function: =HPL_rollT=.
- This function is called only once: at the end of function =HPL_pdlaswp01T= (eponym file).
- This function is called once in function =HPL_pduptateNT= and once in function =HPL_pdupdateTT= (eponym files). There are
  very few differences between these two functions (4 line changes are relevant, which are small variations in the
  arguments of a function, =HPL_dtrsm=). These files have 443 lines: this is a huge copy-paste, very dirty.
- A candidate for the long function we are looking for is =HPL_dlaswp10N= (found by Christian). Has two nested loops.
  This function is also a good candidate for the most terrible piece of code ever written.
- Added a =SMPI_SAMPLE_GLOBAL= after the outer loop, did not reduce the simulation time. Also tried to remove the whole
  code of the function, did not reduce the simulation time either. So we can say this function is not our big consummer.
- Functions =HPL_recv= and =HPL_sdrv= are both called *only* in =HPL_pdmxswp= and =HPL_pdlaswp00N=.
- Function =HPL_pdlaswp00n= is used only in =HPL_pdupdateTN= and =HPL_pdupdateNN=, which are nearly identical. These two
  functions are then used in the =testing= folder, with something like =algo.upfun = HPL_pdupdateNN=. Might be hard to track...
- Function =HPL_pdmxswp= is used in =HPL_pdpancrT=, =HPL_pdpanllT=, =HPL_pdpanllN=, =HPL_pdpanlT=, =HPL_pdpanrlN=,
  =HPL_pdpancrN=. These functions are used in the =testing= folder, with something like =algo.pffun = HPL_pdpancrN=.
- Trying to put some printf.
  We use the command:
  #+begin_src sh
  smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes
  --cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ../../../small_tests/hostfile_64.txt -platform
  ../../../small_tests/cluster_fat_tree_64.xml ./xhpl
  #+end_src
  + Function =HPLpdupdateNN= never used.
  + Function =HPLpdupdateTN= never user.
  + Thus, function =HPL_pdlaswp00n= also never used (verified with printf in this function).
  + Function =HPL_pdmxswp= is used and takes a significant (albeit not huge) amount of time (about 2 seconds when the
    total time is 41 seconds (virtual time)).
*** 2017-02-23 Thursday
**** Try to increase the file limit                             :SMPI:BUG:
- First try, following [[http://askubuntu.com/questions/162229/how-do-i-increase-the-open-files-limit-for-a-non-root-user][this question]] and [[http://stackoverflow.com/questions/21515463/how-to-increase-maximum-file-open-limit-ulimit-in-ubuntu][this question]] from Stackoverflow.
  + Added the following to =/etc/security/limits.conf=:
    #+begin_example
     *     soft    nofile          40000
     *     hard    nofile          40000
    #+end_example
  + Added the following to =/etc/pam.d/common-session=:
    #+begin_example
     session required pam_limits.so
    #+end_example
  + Rebooting.
- Success, =ulimit -Sn= shows =40000= and we can now run experiments with more than 1024 processes.
**** Keep tracking the time consumming piece of code in HPL :PERFORMANCE:HPL:
- Function =HPL_pdmxswp= is used in some functions which are chosen with =algo.pffun= (see above).
- They are then used (through a call to algo.pffun) in functions =HPL_pdrpancrN=, =HPL_pdrpanrlN=, =HPL_pdrpanllN=,
  =HPL_pdpanrlT=, =HPL_pdrpancrT= and =HPL_pdranllT=.
- Again, these functions are not used directly in =src=, there is something like =algo.rffun = HPL_pdrpancrT= in the =testing= folder.
- This =rffun= is used only once, in =HPL_pdfact=.
- Function =HPL_pdfact= takes between 2.5 and 2.8 seconds when the total time is 41 seconds (virtual time). This time includes
  the time spent in =HPL_pdmxswp=.
- Function =HPL_pdffact= is used in functions =HPL_pdgesvK1=, =HPL_pdgesvK2= and =HPL_pdgesv0=. These functions are then called
  in =HPL_pdgesv=.
- Function =HPL_pdgesv= takes a time of about 3 seconds when the total time is 41 seconds (virtual time).
- Strange thing. Deleting the content of this function gives a very short run-time. Maybe the way I measured time (using
  =MPI_WTIME=) is not consistent with the way HPL measure time.
- Identified the long loop in =HPL_pdgesv0=. But cannot put a =SMPI_SAMPLE= here, there are calls to MPI primitives in the block.
- Found the right function to measure time: use =HPL_timer_walltime=, not =MPI_Wtime=.
- Instrumented the code of =HPL_pdgesv0= to have an idea of what takes time. Measures are taken with =HPL_timer_walltime=.
  What takes time is the part “factor and broadcast current panel” in the loop.
  Within this part, the call to =HPL_bcast= and =HPL_pdupdate= are what take most of the (virtual) time. In an execution of
  40.96 seconds:
  #+begin_example
  pdfact = 2.907908, binit = 0.002633, bcast = 11.013843, bwait = 0.000669, pdupdate = 26.709408
  #+end_example
  Obviously there is nothing to do for the broadcast, but there may be hope for =pdupdate=.
- Several versions exist for this function:
  + =HPL_pdupdateTN=
  + =HPL_pdupdateNT=
  + =HPL_pdupdateTT=
  + =HPL_pdupdateNN=
  Only =HPL_pdupdateTT= seems to be used (with our settings).
  Removed body of function =HPL_pdupdateTT=, the simulation time becomes about 8 seconds (was 69 seconds).
- Might be tricky to optimize with SMPI macros, this function mixes computations and communications.
- Tried to insert a =return= line 208 (before comment “The panel has been forwarded at that point, finish the update”. The
  time is not impacted and the correction test are valid, so the part of the code after this point seems useless here.
  Verified by inserting a =printf=, this paprt is never executed.
- Line 143 is executed (just after comment “1 x Q case”).
- Adding a =return= statement line 136 (just before comment “Enable/disable th column panel probing mechanism”) gives a
  simulation time of 8 seconds.
  Same thing line 140, after the broadcast.
- The =if= block of lines 143-258 is never executed in our settings. Explain why acting on line 208 did not have any effect.
- Adding a =return= statement line 358 (just before comment “The panel has been forwarded at that point, finish the
  update”) gives a simulation time of 9.7 seconds.
- The =if= block of lines 360-414 seem to be always executed. The =if= block of lines 366-390 is executed sometimes, but not
  always.
  In this block, we execute the =#else= part of the =#ifdef=.
- In this block, removing the call to =HPL_dgemm= reduce a lot the simulation time (from 68s to 13s).
- Several definitions exist for =HPL_dgemm=: there is an implementation in =src/blas/HPL_dgemm.c=, but also a =#define
  HPL_dgemm cblas_dgemm= in =include/hpl_blas.h=.
- Can disable this =#define= by removing the line =HPL_OPTS = -DHPL_CALL_CBLAS= in the file =Make.SMPI=.
  Then, =HPL_dgemm= is executed, but not the others (=HPL_dgemm0=, =HPL_dgemmTT=, =HPL_dgemmTN=, =HPL_dgemmNT=, =HPL_dgemmNN=). It
  seems that =HPL_dgemm= can call =HPL_dgemm0= which can itself call the four others, but this only happens when
  =HPL_CALL_VSIPL= is defined.
- In fact, there is maybe no need to insert the =SMPI_SAMPLE= macro in =dgemm= function. We can put it inside
  =HPL_pdupdateTT=. For instance, line 360, just above the big =if= block.  However, this performs realy badly. With
  =SMPI_SAMPLE_GLOBAL(10, 0.1)=, the real time becomes about 10 seconds (speedup of \times4) but the virtual time becomes about
  90 seconds (\times2 error). If we increase one of the two numbers, the real times quickly become as large as it was before.
  Same thing with =SMPI_SAMPLE_LOCAL=.  Maybe this code is too irregular? Or we should “zoom in” and insert the SMPI
  optimizations in =dgemm= (which is in an external library, so not that easy).
*** 2017-02-27 Monday
**** Try running matrix product experiment with big fat-trees   :SMPI:BUG:
- Run a medium number of processes on a big fat-tree.
  #+begin_src sh
  ./run_measures.py --global_csv big_global.csv --local_csv big_local.csv --nb_runs 3 --size 9300 --nb_proc 961
  --fat_tree "3;24,24,48;1,24,1:24;1,1,1" --experiment matrix_product
  #+end_src sh
  Seems to work properly, one CPU core is quickly loaded at 100% and one experiment approximately takes two minutes.
- Try a larger number of processes with the same topology and the same matrix size.
  #+begin_src sh
  ./run_measures.py --global_csv big_global.csv --local_csv big_local.csv --nb_runs 3 --size 9300 --nb_proc 8649
  --fat_tree "3;24,24,48;1,24,1:24;1,1,1" --experiment matrix_product
  #+end_src
  The CPU is loaded at about 3% for quite a long time with the script =smpirun=. It finally launches =matmul= and becomes
  loaded at 100%. Then it quickly terminates with a non-null exit code: =Could not map fd 8652 with size 80000: Cannot
  allocate memory=. The memory consumption was only 3% of the total memory, this is strange.
  This happens in function =shm_map=, called by =SMPI_SHARED_MALLOC=.
- Retrying the same command, with =malloc= instead of =SMPI_SHARED_MALLOC= and =free= instead of =SMPI_SHARED_FREE=.
  As expected, larger memory consumption (10.9% of total memory). There is no error this time. The first experiment
  terminates in about 20min. For the record, it achieved 1525 Gflops, with communication time and computation time of
  approximately 0.48 seconds.
- Revert the changes to get back =SMPI_SHARED= macros. Retry to run =smpirun= with the same settings, except the option
  =--cfg=smpi/privatize-global-variables:yes= which is not passed here.  No error either this time, run for 13
  minutes. Also a large memory consumption (13.5%), maybe the 3% we observed was not the final memory consumption, since
  the process exited with an error?
- Remark: for matrix product, there is no global variable. So maybe we can safely remove this option in this case?
  This does not solve the problem since we need it for HPL.
- Try the initial command with a smaller matrix size (size=93, i.e. all processes have a sub-matrix of size 1\times1). Observed the same error.
- Also try to reproduce this with HPL, with this command:
  #+begin_src sh
  ./run_measures.py --global_csv big_global.csv --nb_runs 3 --size 5000 --nb_proc 8649 --fat_tree
  "3;24,24,48;1,24,1:24;1,1,1" --experiment HPL
  #+end_src
  Not any error, although we have a memory consumption of 71.2%.
- Try the initial command, still with a size of 93, but commenting the call to =matrix_product= in =matmul.c=. Thus, there
  is no allocation of temporary buffers, only the initial matrices (3 allocations instead of 5). No error.
- Same thing, with the call to =matrix_product= uncommented, but a =return= statement placed just after the temporary
  buffers allocations. We get the =mmap= error.
- Create a MWE from this, called =mmap_error.c=.
**** Work on a MWE for the mmap error                           :SMPI:BUG:
- File =mmap_error.c= is a MWE for the =mmap= error. It consists in 5 calls to =SMPI_SHARED_MALLOC= with a size of 1, we
  launch it with 8652 processes.
  We also get an error if we do 100k calls to =SMPI_SHARED_MALLOC= with only one process. The total number of calls to
  this macro seem to be the issue. We get the error with or without the option =smpi/privatize-global-variables:yes=.
- The following file =mmap_error.c=:
  #+begin_src c
  #include <stdio.h>
  #include <mpi.h>

  #define N 65471

  int main(int argc, char *argv[]) {

      MPI_Init(&argc, &argv);

      for(int i = 0; i < N; i++) {
          float *a = SMPI_SHARED_MALLOC(1);
      }

      MPI_Barrier(MPI_COMM_WORLD);
      printf("Success\n");
      MPI_Finalize();
      return 0;
  }
  #+end_src
  With the following command (commit =8eb0cf0b6993e174df58607e9492a134b85a4669= of Simgrid):
  #+begin_src sh
  smpicc -O4 mmap_error.c -o mmap_error
  smpirun -np 1 -hostfile hostfile_64.txt -platform cluster_fat_tree_64.xml ./mmap_error
  #+end_src
  Yields an error. Note that the host and topology files are irrelevant here.
  + For =N<65471=, we have no error (=Success= is printed).
  + For =N>65471=, we have the error =Could not map fd 3 with size 1: Cannot allocate memory=.
  + For =N=65471=, we have the error =Memory callocation of 524288 bytes failed=.
- Retried with latest version of Simgrid (commit =c8db21208f3436c35d3fdf5a875a0059719bff43=).  Now have the
  message:
  #+begin_example
  Could not map folded virtual memory (Cannot allocate memory). Do you perhaps need to increase
  the STARPU_MALLOC_SIMULATION_FOLD environment variable or the sysctl vm.max_map_count?
  #+end_example
  Found the issue:
  #+begin_src sh
  $ sysctl vm.max_map_count
  vm.max_map_count = 65530
  #+end_src
  To modify the value of a =sysctl= variable, follow [[https://www.cyberciti.biz/faq/howto-set-sysctl-variables/][this link]].
  Temporary fix:
  #+begin_src sh
  sudo sysctl -w vm.max_map_count=100000
  #+end_src
**** Run the matrix product experiment with 8649 processes
- Using the command:
  #+begin_src sh
  ./run_measures.py --global_csv big_global.csv --local_csv big_local.csv --nb_runs 3 --size 9300 --nb_proc 8649
  --fat_tree "3;24,24,48;1,24,1:24;1,1,1" --experiment matrix_product
  #+end_src
- The experiments are very long, about 30 minutes. The code is already optimized a lot (SMPI macros, no initialization
  of the matrices), a large part of this time is spent outside of the application, so there is not much hope to run it
  faster without modifying Simgrid.
- This shows that we *really* need to optimize HPL if we want to run it with a large number of processes.
- Anyway, without SMPI macros, every floating-point operation of the application is actually performed. Thus, if we are
  simulating a computation made on a 1000 Gflops cluster, using a 1 Gflops laptop, the simulation should take *at least*
  1000 times longer than the same computation on a real 1000 Gflops cluster.
- First results show no large difference in the total time for small or large number of roots. The communication time is
  about twice as large as the computation time, so maybe we should take a larger matrix. When we had 961 processes, each
  one had a sub-matrix of size 300\times300. With 8649 processes, they have a sub-matrix of size 100\times100.
  Problem: if we want to get back to the 300\times300 sub-matrices, we need to multiply the size by 3 and thus the memory
  consumption by 9. It was already about 25%, so not feasible on this laptop. But this is strange, we should have the
  memory of only one process and we successfully ran 300\times300 sub-matrices, need to check.
*** 2017-02-28 Tuesday
**** Other benchmarks on Simgrid                        :SMPI:EXPERIMENTS:
- The paper “Simulating MPI application: the SMPI approach” uses the benchmark [[https://www.nas.nasa.gov/publications/npb.html][NAS EP]] to demonstrate the scalability of
  SMPI. With SMPI optimizations, they ran it with 16384 processes in 200 to 400 seconds (depending on the topology).
  Where is the code for this?
  + Found an [[https://github.com/sbadia/simgrid/tree/master/examples/smpi/NAS][old repository]]. Not clear if it is relevant.
  + Also a (shorter) version in the [[https://github.com/simgrid/simgrid/tree/master/examples/smpi/NAS][official Simgrid repository]].
    Executable located in =simgrid/build/examples/smpi/NAS/=.
    Launch with two arguments: number of processes (don’t know what it does, we already have =-np= option given to
    =smpirun=) and the class to use (S, W, A, B, C, D, E, F).
- The NAS EP benchmark from Simgrid repository seems promising. Added a new class to have a larger problem (maybe we
  could instead give the size as an argument). With a large enough size, we can go to about 3.5 Gflops per process,
  i.e. an efficiency of 3.5 (recall that we use 1 Gflops nodes). It seems large, is it normal?
- Longer than the matrix product, 745 seconds for 1152 processes and class F (custom class with m=42). Only 93 seconds
  were spent in the application, so the code is already correctly optimized (one call to =SMPI_SAMPLE_GLOBAL=).
- Apparently not impacted by a tapered fat tree. Roughly the same speed for =2;24,48;1,24;1,1= and =2;24,48;1,1;1,1=, 1152
  processes and class F: about 3.5 Gflops. The application is made of a computation followed by three =MPI_Allreduce=
  of only one =double=, so very few communications (hence the name “embarassingly parallel”).
**** Talk with Christian about benchmarks
- Get an access to grid 5000.
- Profile the code, with something like smpirun -wrapper “valgrind <param>”.
- To use SMPI macros, run the =HPL_dgemm= implemented in HPL, not the one from the external library.
** 2017-03 March
*** 2017-03-01 Wednesday
**** Trying to use HPL without external BLAS library                 :HPL:
- Failed.
- It seems that three options are available for compilation, according to [[http://www.netlib.org/benchmark/hpl/software.html][this page]]:
  + BLAS Fortran 77 interface (the default),
  + BLAS C interface (option =-DHPL_CALL_CBLAS=),
  + VSIPL library (option =-DHPL_CALL_VSIPL=).
- We currently use the C interface, which rely on an external library (e.g. Atlas).
- There is an implementation of =HPL_dgemm= in HPL, but it seems to need either code from Fortran 77 or from VSIPL.
- According to the [[http://www.netlib.org/benchmark/hpl/][HPL homepage]]:
  #+begin_example
  The HPL software package requires the availibility on your system of an implementation of the Message Passing
  Interface MPI (1.1 compliant). An implementation of either the Basic Linear Algebra Subprograms BLAS or the Vector
  Signal Image Processing Library VSIPL is also needed. Machine-specific as well as generic implementations of MPI, the
  BLAS and VSIPL are available for a large variety of systems.
  #+end_example
  So it seems hopeless to get rid of a BLAS library.
**** Idea: trace calls to =HPL_dgemm= (Arnaud’s idea)     :SMPI:TRACING:HPL:
- To do so, surround them by calls to trivial MPI primitives (e.g. =MPI_Initialized=). For instance:
  #+begin_src c
  #define HPL_dgemm(...) ({int simgrid_test; MPI_Initialized(&simgrid_test); cblas_dgemm(__VA_ARGS__);\
  MPI_Initialized(&simgrid_test);})
  #+end_src
- Then, trace the execution (output in =/tmp/trace=):
  #+begin_src sh
  smpirun -trace -trace-file /tmp/trace --cfg=smpi/trace-call-location:1 --cfg=smpi/bcast:mpich\
  --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16\
  -hostfile ../../../small_tests/hostfile_64.txt -platform ../../../small_tests/cluster_fat_tree_64.xml ./xhpl\
  #+end_src
- Finally, dump this trace in CSV format:
  #+begin_src sh
  pj_dump --user-defined --ignore-incomplete-links trace > trace.dump
  #+end_src
- Did not work, no =MPI_Initialized= in the trace. In fact, this primitive is currently not traced. We could modify SMPI
  to achieve this behavior, or use another MPI primitive that is already traced.
*** 2017-03-02 Thursday
**** Keep trying to trace calls to =HPL_dgemm=            :SMPI:TRACING:HPL:
- A MPI primitive is traced \Leftrightarrow the functions =new_pajePushState= and =new_pagePopState= are called (not sure, this is an
  intuition).
- This function is not called by =MPI_Initialized=, or =MPI_Wtime=.
- It is called by =MPI_Test=, but only if the =MPI_Request= object passed as argument is non-null, so we would need to do a
  fake asynchronous communication just before, which is probably not a good idea.
- Anyway, it looks dirty to use a MPI primitive like this. Wouldn’t it be better to have a custom no-op primitive that
  force the introduction of a trace entry? For instance, something like
  #+begin_src c
  SMPI_Trace {
      HPL_dgemm();
  }
  #+end_src
  or like
  #+begin_src c
  SMPI_BeginTrace();
  HPL_dgemm();
  SMPI_EndTrace();
  #+end_src
- Every MPI primitive is defined by a =#define= with a call to =smpi_trace_set_call_location= followed by a call to the
  function. For instance:
  #+begin_src c
  #define MPI_Test(...) ({ smpi_trace_set_call_location(__FILE__,__LINE__); MPI_Test(__VA_ARGS__); })
  #+end_src
  However, this only record the file name and the line number, I do not think it dumps anything in the trace.
**** Arnaud’s keynote: reproducible research                     :MEETING:
- Intro: article we had in exam, “Is everything we eat associated with cancer?”.
- In most articles, we can read formulae and trust results, but much less often reproduce the results.
- Reproducibility crisis, several scandals with falsified results (intentionnaly or not).
- Video: Brendan Gregg, shouting  in the data center.
**** Discussion with Arnaud                                      :MEETING:
- Regarding the matrix product:
  + Compare the (tapered) fat-tree with “perfect” topology (cluster with no latency and infinit bandwidth).
  + Run it with larger matrices for the same amount of processes. Do not aim at spending as much time in communication
    than computation. We want the communication time to become nearly negligible. In practices, users of a supercomputer
    try to fill the memory of their nodes.
- Regarding HPL:
  + As discussed yesterday, we want to trace the calls to =HPL_dgemm= by putting calls to a MPI primitive just before and after.
  + The short-time goal is to have an idea of the behavior of HPL regarding this function. Are there a lot of different
    calls to =HPL_dgemm= coming from different locations? Do these calls always take the same amount of time (i.e. do we
    always multiply matrices of the same size)?
  + It seems that there is some variability in the duration of =HPL_dgemm= (to be verified with the trace). If HPL really
    use the function to multiply matrices of different size, we cannot do something like
    =SMPI_SAMPLE(){HPL_dgemm()}=, it will not be precise. What we could do however is to generalize =SMPI_SAMPLE=: we could
    parametrize it by a number, representing the size of the problem that is sampled. If this size is always the same,
    then we could do what we are doing now, simply take the average. If this size changes over time, we could do
    something more elaborated for the prediction, like a linear regression.
  + Using MPI functions like =MPI_Test= is not very “clean”, but we do not want to waste time on this currently, so we stick
    with existing MPI primitives. We could try to change this in the future.
  + It is always safe to call =smpi_process_index=. Thus, we could modify =PMPI_Test= to call =TRACE_smpi_testing= functions
    even when the given request is =NULL=.
*** 2017-03-03 Friday
**** Tracing calls to =HPL_dgemm= :SMPI:C:PYTHON:R:EXPERIMENTS:TRACING:PERFORMANCE:HPL:
- Modification of the function =PMPI_Test= of Simgrid so that =MPI_Test= is traced even when the =MPI_Request= handle is
  =NULL=. To do that, we need to get the rank of the process, with =smpi_process_index=. The value returned is always 0 in
  this case. This is a problem, since we could not distinguish between calls to =MPI_Test= from different processes, thus
  it would be impossible to measure time. Reverting the changes.
- To get a non-null =MPI_Request=, did a =MPI_Isend= followed by a =MPI_Recv=:
  #+begin_src c
  #define    HPL_dgemm(...)      ({\
    int my_rank, buff=0;\
    MPI_Request request;\
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
    MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
    MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
    MPI_Wait(&request, MPI_STATUS_IGNORE);\
    cblas_dgemm(__VA_ARGS__);\
    MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
    MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
    MPI_Wait(&request, MPI_STATUS_IGNORE);\
  })
  #+end_src
- Forget this. HPL was executed with only one process (=-np 16= but P and Q were 1 in =HPL.dat=). This is why we only had a
  rank =0= when giving =NULL= as =MPI_Request=. Let’s revert this and use simple =MPI_Test= with =NULL=.
- Calls to =MPI_Test= seem to be correctly traced, but the post-processing of the trace with =pj_dump= crashes:
  #+begin_example
  terminate called after throwing an instance of 'std::out_of_range'
  what():  vector::_M_range_check: __n (which is 4) >= this->size() (which is 4)
  #+end_example
  It also happened with the more complex piece of code that is shown above (with =MPI_Test= instead of =MPI_Wait=).
  Reverting again, to use the bigger piece of code above.
- Now, the call to =pj_dump= succeeds, and we can see calls to =MPI_Wait= in the trace.
- The call to =smpirun= was:
#+begin_src sh
smpirun -trace -trace-file /tmp/trace --cfg=smpi/trace-call-location:1 --cfg=smpi/bcast:mpich\
--cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16\
-hostfile ../../../small_tests/hostfile_64.txt -platform ../../../small_tests/cluster_fat_tree_64.xml ./xhpl
#+end_src
- Processing of the trace.
  Clean the file:
#+begin_src sh
pj_dump --user-defined --ignore-incomplete-links /tmp/trace > /tmp/trace.csv
grep "State," /tmp/trace.csv | grep MPI_Wait | sed -e 's/()//' -e 's/MPI_STATE, //ig'  -e 's/State, //ig' -e 's/rank-//' -e\
's/PMPI_/MPI_/' | grep MPI_  | tr 'A-Z' 'a-z' > /tmp/trace_processed.csv
#+end_src

Clean the paths:
#+begin_src python
import re
reg = re.compile('((?:[^/])*)(?:/[a-zA-Z0-9_-]*)*((?:/hpl-2.2(?:/[a-zA-Z0-9_-]*)*).*)')
def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            for line in in_f:
                match = reg.match(line)
                out_f.write('%s%s\n' % (match.group(1), match.group(2)))
process('/tmp/trace_processed.csv', '/tmp/trace_cleaned.csv')
 #+end_src

 #+RESULTS:
 : None

#+begin_src R :results output :session *R* :exports both
df <- read.csv("/tmp/trace_cleaned.csv", header=F, strip.white=T, sep=",");
names(df) = c("rank", "start", "end", "duration", "level", "state", "Filename", "Linenumber");
head(df)
#+end_src

#+RESULTS:
#+begin_example
  rank    start      end duration level    state
1    8 2.743960 2.743960        0     0 mpi_wait
2    8 2.744005 2.744005        0     0 mpi_wait
3    8 2.744005 2.744005        0     0 mpi_wait
4    8 2.744005 2.744005        0     0 mpi_wait
5    8 2.744005 2.744005        0     0 mpi_wait
6    8 2.744005 2.744005        0     0 mpi_wait
                            Filename Linenumber
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
#+end_example

#+BEGIN_SRC R :results output :session *R* :exports both
duration_compute = function(df) {
    ndf = data.frame();
    df = df[with(df,order(rank,start)),];
    #origin = unique(df$origin)
    for(i in (sort(unique(df$rank)))) {
        start     = df[df$rank==i,]$start;
        end       = df[df$rank==i,]$end;
        l         = length(end);
        end       = c(0,end[1:(l-1)]); # Computation starts at time 0

        startline = c(0, df[df$rank==i,]$Linenumber[1:(l-1)]);
        startfile = c("", as.character(df[df$rank==i,]$Filename[1:(l-1)]));
        endline   = df[df$rank==i,]$Linenumber;
        endfile   = df[df$rank==i,]$Filename;

        ndf       = rbind(ndf, data.frame(rank=i, start=end, end=start,
            duration=start-end, state="Computing",
            startline=startline, startfile=startfile, endline=endline,
            endfile=endfile));
    }
    ndf$idx = 1:length(ndf$duration)
    ndf;
}
durations = duration_compute(df);
durations = durations[durations["startfile"] == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c" & durations["endfile"] == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c" &
    durations["startline"] == durations["endline"],]
#+END_SRC

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
library(dplyr)
options(width=200)
group_by(durations, startfile, startline, endfile, endline) %>% summarise(duration=sum(duration), count=n()) %>% as.data.frame()
#+end_src

#+RESULTS:
:                             startfile startline                             endfile endline  duration count
: 1 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387  683.6677   659
: 2 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 2115.8129  1977

#+begin_src R :file images/trace1_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(durations, aes(x=idx, y=duration, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace1_16.png]]


#+begin_src R :file images/trace2_16.png :results value graphics :session *R* :exports both
ggplot(durations, aes(x=start, y=duration, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace2_16.png]]

Same results, with four processes:

[[file:images/trace1_4.png]]

[[file:images/trace2_4.png]]

**** Seminaire                                                   :MEETING:
On the asymptotic behavior of the price of anarchy, how bad is selfish routing in highly congested networks?
- For instance, cars on a road make their own routing decisions, hence the “selfish” routing. This is not optimal (in
  comparison with a centralized routing).
**** Discussion with Arnaud & Christian                          :MEETING:
- According to the plots, it is impossible to use =SMPI_SAMPLE= as is, since there are huge variations on the duration of =HPL_dgemm=.
- The idea of a parametrized =SMPI_SAMPLE= is also not super. Every process does consecutive calls to =HPL_dgemm=, each call
  being shorter than the previous ones. So we would still have to compute expensive calls.
- A long term idea may be to have a “SimBLAS” library, that simulates the calls to =HPL_dgemm= (and other BLAS
  primitives). Christian will work on this.
- Answers to all my questions from the paper readings.
**** TODO New tasks [3/4] 
:LOGBOOK:
- State "TODO"       from              [2017-03-03 Fri 17:43]
:END:
- [X] Do the linear regression by hand, off-line. Output the sizes of the matrices given to =HPL_dgemm= (with =printf=).
- [X] Register on Grid5000. Compile HPL on one Grid5000 machine.
- [X] Try to run HPL with a very large matrix, by using =SMPI_SHARED_MALLOC= (thus look at where all the allocations of
  matrices are done).
- [ ] Have a look at the code of Simgrid, in particular the routing in fat-trees.
*** 2017-03-06 Monday
**** Output the matrix sizes                        :C:PYTHON:TRACING:HPL:
- Add the following before the relevant calls to =HPL_dgemm=:
  #+begin_src c
  printf("line=%d rank=%d m=%d n=%d k=%d\n", __LINE__+3, rank, mp, nn, jb);
  #+end_src
  Then, run HPL by redirecting =stdout= to =/tmp/output=.
- Process the output, to get a CSV file:
#+begin_src python
import re
import csv
reg = re.compile('line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=([0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('line', 'rank', 'n', 'm', 'k'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    csv_writer.writerow(tuple(match.group(i) for i in range(1,6)))
process('/tmp/output', '/tmp/sizes.csv')
#+end_src

**** Merge the sizes with the durations        :R:EXPERIMENTS:PERFORMANCE:
- Run =smpirun= as stated above, then process the output and the trace as before.
- Process the data:
#+begin_src R :results output :session *R* :exports both
df <- read.csv("/tmp/trace_cleaned.csv", header=F, strip.white=T, sep=",");
names(df) = c("rank", "start", "end", "duration", "level", "state", "Filename", "Linenumber");
head(df)
#+end_src

#+RESULTS:
:   rank    start      end duration level    state                           Filename Linenumber
: 1    8 2.743960 2.743960        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
: 2    8 2.744005 2.744005        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
: 3    8 2.744005 2.744005        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
: 4    8 2.744005 2.744005        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
: 5    8 2.744005 2.744005        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
: 6    8 2.744005 2.744005        0     0 mpi_wait /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222

#+begin_src R :results output :session *R* :exports both
sizes <- read.csv("/tmp/sizes.csv");
head(sizes)
#+end_src

#+RESULTS:
:   line rank    n    m   k
: 1  411   12 4920 4920 120
: 2  387    0 4920 4920 120
: 3  411    8 5000 4920 120
: 4  411    4 5040 4920 120
: 5  411   13 4920 5040 120
: 6  387    1 4920 5040 120

#+begin_src R :results output :session *R* :exports both
durations = duration_compute(df); # same function as above
durations = durations[durations["startfile"] == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c" & durations["endfile"] == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c" &
    durations["startline"] == durations["endline"],]
head(durations)
#+end_src

#+RESULTS:
:     rank     start       end duration     state startline                           startfile endline                             endfile idx
: 481    0  3.153899  6.271075 3.117176 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481
: 486    0  7.047247 10.063367 3.016120 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486
: 491    0 10.648367 13.716045 3.067678 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491
: 496    0 14.104534 17.155418 3.050884 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496
: 977    0 17.557080 20.430869 2.873789 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977
: 982    0 21.104026 24.044767 2.940741 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982


#+begin_src R :results output :session *R* :exports both
insert_sizes = function(durations, sizes) {
    stopifnot(nrow(durations)==nrow(sizes))
    ndf = data.frame();
    for(i in (sort(unique(durations$rank)))) {
        tmp_dur = durations[durations$rank == i,]
        tmp_sizes = sizes[sizes$rank == i,]
        stopifnot(nrow(tmp_dur) == nrow(tmp_sizes))
        stopifnot(tmp_dur$startline == tmp_sizes$line)
        storage.mode(tmp_sizes$m) <- "double" # avoiding integer overflow when taking the product
        storage.mode(tmp_sizes$n) <- "double"
        storage.mode(tmp_sizes$k) <- "double"
        tmp_dur$m = tmp_sizes$m
        tmp_dur$n = tmp_sizes$n
        tmp_dur$k = tmp_sizes$k
        tmp_dur$size_product = tmp_sizes$m * tmp_sizes$n * tmp_sizes$k
        ndf = rbind(ndf, tmp_dur)
    }
    return(ndf);
}
#+end_src

#+RESULTS:

#+begin_src R :results output :session *R* :exports both
result = insert_sizes(durations, sizes)
head(result)
#+end_src

#+RESULTS:
:     rank     start       end duration     state startline                           startfile endline                             endfile idx    m    n   k size_product
: 481    0  3.153899  6.271075 3.117176 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481 4920 4920 120   2904768000
: 486    0  7.047247 10.063367 3.016120 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486 4920 4920 120   2904768000
: 491    0 10.648367 13.716045 3.067678 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491 4920 4920 120   2904768000
: 496    0 14.104534 17.155418 3.050884 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496 4920 4920 120   2904768000
: 977    0 17.557080 20.430869 2.873789 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977 4800 4800 120   2764800000
: 982    0 21.104026 24.044767 2.940741 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982 4800 4800 120   2764800000

**** Plot and linear regression                :R:EXPERIMENTS:PERFORMANCE:

#+begin_src R :file images/trace3_16.png :results value graphics :results output :session *R* :exports both
library(ggplot2)
ggplot(result, aes(x=size_product, y=duration, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm as a function of the sizes")
#+end_src

#+RESULTS:
[[file:images/trace3_16.png]]


#+begin_src R :results output :session *R* :exports both
reg <- lm(duration~I(m*n*k), data=result)
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ I(m * n * k), data = result)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.10066 -0.01700 -0.00085  0.00351  0.57745 

Coefficients:
               Estimate Std. Error  t value Pr(>|t|)    
(Intercept)  -2.476e-03  1.235e-03   -2.005   0.0451 *  
I(m * n * k)  1.062e-09  9.220e-13 1151.470   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.04205 on 2634 degrees of freedom
Multiple R-squared:  0.998,	Adjusted R-squared:  0.998 
F-statistic: 1.326e+06 on 1 and 2634 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :file images/reg_16.png :results value graphics :results output :session *R* :exports both
layout(matrix(c(1,2,3,4),2,2))
plot(reg)
#+end_src

#+RESULTS:
[[file:images/reg_16.png]]
**** Comments on the linear regression                       :EXPERIMENTS:
- The plot of the duration as a function of =m*n*k= looks great. Maybe a bit of heteroscedasticity, but not so much. It is
  clearly linear.
- The linear regression however is not so good. We have a high R-squared (0.998), but the plots look bad. The
  residual-vs-fitted plot shows that the results are clearly heteroscedastic. The normal-QQ shows that they are not
  linear (in =m*n*k=) but rather exponential.
- The plot of the linear regression seems to contradict the first plot, this is strange.
**** Investigating the linear regression                               :C:
- We can print other relevant parameters of =HPL_dgemm=:
  #+begin_src c
  printf("line=%d rank=%d m=%d n=%d k=%d a=%f lead_A=%d lead_B=%d lead_C=%d\n", __LINE__+3,
    rank, mp, nn, jb, -HPL_rone, ldl2, LDU, lda);
  #+end_src
  Here, =a= is a scaling factor applied to the matrix, =lead_A=, =lead_B= and =lead_C= are the leading dimensions of matrices =A=, =B= and
  =C=.
  A sample of what we get is (only some lines are reported here):
  #+begin_example
  line=411 rank=2 m=2240 n=2160 k=120 a=-1.000000 lead_A=2480 lead_B=2160 lead_C=2480
  line=387 rank=3 m=1640 n=1641 k=120 a=-1.000000 lead_A=2480 lead_B=1641 lead_C=2480
  line=387 rank=2 m=680 n=720 k=120 a=-1.000000 lead_A=680 lead_B=720 lead_C=2480
  line=387 rank=2 m=200 n=240 k=120 a=-1.000000 lead_A=200 lead_B=240 lead_C=2480
  177 line=411 rank=1 m=480 n=441 k=120 a=-1.000000 lead_A=2520 lead_B=441 lead_C=2520
  #+end_example
  This trend seems to roughly repeat: =a= is always -1, =lead_C= is always either 2480 or 2520. For small enough values,
  =lead_A= is equal to =m= and =lead_C= is equal to =n=. For larger values, they are not equal anymore, but all are large.
  However, there are still some noticeable variations. For instance:
  #+begin_example
  line=387 rank=0 m=600 n=600 k=120 a=-1.000000 lead_A=2520 lead_B=600 lead_C=2520
  line=411 rank=0 m=600 n=600 k=120 a=-1.000000 lead_A=600 lead_B=600 lead_C=2520
  #+end_example
  In this last example, all parameters are equal, except =lead_A= which is more than four times larger in one case.
- A small leading dimension means a better locality and thus better performances. These differences in the leading
  dimensions could explain the non-linearity and the heteroscedasticity.
*** 2017-03-07 Tuesday
**** And the leading dimensions? :C:PYTHON:R:EXPERIMENTS:TRACING:PERFORMANCE:
- We have this =printf= before the calls to =HPL_dgemm= (same as before, except for the =a= that is removed):
  #+begin_src c
  printf("line=%d rank=%d m=%d n=%d k=%d lead_A=%d lead_B=%d lead_C=%d\n", __LINE__+3,
    rank, mp, nn, jb, ldl2, LDU, lda);
  #+end_src
- The trace is in the file =/tmp/trace=, we process it as before. The output is redirected in the file =/tmp/output=.
- Processing of the output:
#+begin_src python
import re
import csv
reg = re.compile('line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=([0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=([0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    csv_writer.writerow(tuple(match.group(i) for i in range(1,9)))
process('/tmp/output', '/tmp/sizes.csv')
#+end_src

We have the =durations= dataframe, obtained as before:

#+begin_src R :results output :session *R* :exports both
head(durations)
#+end_src

#+RESULTS:
:     rank     start       end duration     state startline                           startfile endline                             endfile idx
: 481    0  4.111176  7.158459 3.047283 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481
: 486    0  7.827329 10.848572 3.021243 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486
: 491    0 11.411456 14.445789 3.034333 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491
: 496    0 14.837377 17.868118 3.030741 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496
: 977    0 18.268679 21.142146 2.873467 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977
: 982    0 21.809954 24.699182 2.889228 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982

Then we get the =sizes= dataframe:
#+begin_src R :results output :session *R* :exports both
sizes <- read.csv("/tmp/sizes.csv");
head(sizes)
#+end_src

#+RESULTS:
:   line rank    n    m   k lead_A lead_B lead_C
: 1  387    0 4920 4920 120   5040   4920   5040
: 2  411    8 5000 4920 120   5000   4920   5000
: 3  411    4 5040 4920 120   5040   4920   5040
: 4  411   12 4920 4920 120   4920   4920   4920
: 5  387    1 4920 5040 120   4920   5040   5040
: 6  411    5 5040 5040 120   5040   5040   5040

#+begin_src R :results output :session *R* :exports both
insert_sizes = function(durations, sizes) {
    stopifnot(nrow(durations)==nrow(sizes))
    ndf = data.frame();
    for(i in (sort(unique(durations$rank)))) {
        tmp_dur = durations[durations$rank == i,]
        tmp_sizes = sizes[sizes$rank == i,]
        stopifnot(nrow(tmp_dur) == nrow(tmp_sizes))
        stopifnot(tmp_dur$startline == tmp_sizes$line)
        storage.mode(tmp_sizes$m) <- "double" # avoiding integer overflow when taking the product
        storage.mode(tmp_sizes$n) <- "double"
        storage.mode(tmp_sizes$k) <- "double"
        storage.mode(tmp_sizes$lead_A) <- "double"
        storage.mode(tmp_sizes$lead_B) <- "double"
        storage.mode(tmp_sizes$lead_C) <- "double"
        tmp_dur$m = tmp_sizes$m
        tmp_dur$n = tmp_sizes$n
        tmp_dur$k = tmp_sizes$k
        tmp_dur$lead_A = tmp_sizes$lead_A
        tmp_dur$lead_B = tmp_sizes$lead_B
        tmp_dur$lead_C = tmp_sizes$lead_C
        tmp_dur$lead_product = tmp_sizes$lead_A * tmp_sizes$lead_B * tmp_sizes$lead_C
        tmp_dur$size_product = tmp_sizes$m * tmp_sizes$n * tmp_sizes$k
        tmp_dur$ratio = tmp_dur$lead_product/tmp_dur$size_product
        ndf = rbind(ndf, tmp_dur)
    }
    return(ndf);
}
#+end_src

#+begin_src R :results output :session *R* :exports both
result = insert_sizes(durations, sizes)
head(result)
#+end_src

#+RESULTS:
#+begin_example
    rank     start       end duration     state startline                           startfile endline                             endfile idx    m    n   k lead_A lead_B lead_C lead_product
481    0  4.111176  7.158459 3.047283 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481 4920 4920 120   5040   4920   5040 124975872000
486    0  7.827329 10.848572 3.021243 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486 4920 4920 120   4920   4920   5040 122000256000
491    0 11.411456 14.445789 3.034333 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491 4920 4920 120   4920   4920   5040 122000256000
496    0 14.837377 17.868118 3.030741 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496 4920 4920 120   4920   4920   5040 122000256000
977    0 18.268679 21.142146 2.873467 Computing       387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     387 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977 4800 4800 120   5040   4800   5040 121927680000
982    0 21.809954 24.699182 2.889228 Computing       411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     411 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982 4800 4800 120   4800   4800   5040 116121600000
    size_product    ratio
481   2904768000 43.02439
486   2904768000 42.00000
491   2904768000 42.00000
496   2904768000 42.00000
977   2764800000 44.10000
982   2764800000 42.00000
#+end_example

#+begin_src R :file images/trace4_16.png :results value graphics :results output :session *R* :exports both
library(ggplot2)
ggplot(result, aes(x=lead_product, y=duration, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm as a function of the leading dimensions")
#+end_src

#+RESULTS:
[[file:images/trace4_16.png]]

#+begin_src R :file images/trace5_16.png :results value graphics :results output :session *R* :exports both
library(ggplot2)
ggplot(result, aes(x=lead_product, y=size_product, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Size of the matrices of HPL_dgemm as a function of the leading dimensions")
#+end_src

#+RESULTS:
[[file:images/trace5_16.png]]

#+begin_src R :file images/trace6_16.png :results value graphics :results output :session *R* :exports both
ggplot(result, aes(x=idx, y=ratio, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Ratios of the leading dimensions by the sizes over time")
#+end_src

#+RESULTS:
[[file:images/trace6_16.png]]

#+begin_src R :results output :session *R* :exports both
reg <- lm(duration~ I(m*n*k) + lead_A+lead_B+lead_C, data=result)
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ I(m * n * k) + lead_A + lead_B + lead_C, 
    data = result)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.09477 -0.01804 -0.00439  0.00850  1.39992 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)  -7.741e-01  9.915e-02  -7.807 8.37e-15 ***
I(m * n * k)  1.069e-09  4.431e-12 241.217  < 2e-16 ***
lead_A        2.965e-06  7.744e-07   3.828 0.000132 ***
lead_B       -7.048e-06  2.799e-06  -2.518 0.011863 *  
lead_C        1.547e-04  1.981e-05   7.810 8.16e-15 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.04981 on 2631 degrees of freedom
Multiple R-squared:  0.9972,	Adjusted R-squared:  0.9972 
F-statistic: 2.361e+05 on 4 and 2631 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :file images/reg2_16.png :results value graphics :results output :session *R* :exports both
layout(matrix(c(1,2,3,4),2,2))
plot(reg)
#+end_src

#+RESULTS:
[[file:images/reg2_16.png]]

**** Discussion about the leading dimensions                 :EXPERIMENTS:
- In the three previous plots, we see that the leading dimensions have two modes, which are directly observable in the
  durations of =HPL_dgemm=.
  + One of the modes seems to be linear in the sizes, we observe a straight line.
  + The other mode is clearly non-linear. Maybe quadratic? Exponential?
- The linear regression shows that the variables =lead_A=, =lead_B= and =lead_C= have a non-negligible impact on the
  performances, albeit smaller than the sizes. We still have terrible plots, adding parameters in the model did not
  change anything.
- This could explain the “bad” plots of the linear regression.
**** Performance analysis of =dgemm= outside of HPL :C:EXPERIMENTS:PERFORMANCE:
- In the above analysis, the raw results come from a trace of HPL. Thus, we cannot control the sizes and/or leading
  dimensions. We only have observational data and not experimental data.
- To fix this, let’s write a short C code, called =dgemm_test=,  that call =cblas_dgemm= (the function to which is aliased =HPL_dgemm=).
- Currently, this code takes six parameters as arguments: the three sizes and the three leading dimensions. Be careful,
  the meaning of these sizes and leading dimensions change depending on how =dgemm= is called: =CblasColMajor= or
  =CblasRowMajor=, and =CblasNoTrans= or =CblasTrans=. In the current code, these are fixed to be the same than in HPL.
- Then, a Python script (called =runner.py=) sample random sizes and leading dimensions (taking care of the constraints
  between the sizes and dimensions) and call =dgemm_test=. It then writes the results in a CSV file.
- Quick analysis of these results in R:
  + We got plots with the same shape (both the plot of the raw results and the plot of the linear regression).
  + The call to =dgemm= is 10 times faster in =dgemm_test= than in HPL. Need to find why. Firstly, what is the time obtained in the HPL
    traces? Is it virtual or real?
  + Similarly than with HPL, the linear regression shows that the ratio has a significative impact, but lower than the
    sizes.
*** 2017-03-08 Wednesday
**** Keep looking at =dgemm= outside of HPL      :C:EXPERIMENTS:PERFORMANCE:
- Use =dgemm_test= at commit =0455edcb0af1eb673725959d216137997fc40fd2=. Run 1000 experiments.
- Here, the variable =product= is sampled randomly and uniformly in [1, 2000^3]. Then, the three sizes are set to \lfloor
  product^(1/3) \rfloor.
- The leading dimensions are equal to the sizes.
- Analysis in R:
  #+begin_src R :results output :session *R* :exports both
  result <- read.csv('~/tmp/3/result.csv')
  head(result)
  #+end_src

  #+RESULTS:
  :       time size_product lead_product ratio    m    n    k lead_A lead_B lead_C
  : 1 0.160235    843908625    843908625     1  945  945  945    945    945    945
  : 2 0.719003   4298942376   4298942376     1 1626 1626 1626   1626   1626   1626
  : 3 0.783674   4549540393   4549540393     1 1657 1657 1657   1657   1657   1657
  : 4 0.472595   2656741625   2656741625     1 1385 1385 1385   1385   1385   1385
  : 5 0.319670   1874516337   1874516337     1 1233 1233 1233   1233   1233   1233
  : 6 1.131936   6676532387   6676532387     1 1883 1883 1883   1883   1883   1883

  #+begin_src R :file images/dgemm_test_raw.png :results value graphics :results output :session *R* :exports both
  library(ggplot2)
  ggplot(result, aes(x=size_product, y=time)) +
      geom_point(shape=1) + ggtitle("Durations of cblas_dgemm as a function of the sizes product.")
  #+end_src

      #+RESULTS:
      [[file:images/dgemm_test_raw.png]]

    #+begin_src R :results output :session *R* :exports both
    reg <- lm(time ~ size_product, result)
    summary(reg)
    #+end_src

    #+RESULTS:
    #+begin_example

    Call:
    lm(formula = time ~ size_product, data = result)

    Residuals:
          Min        1Q    Median        3Q       Max 
    -0.027295 -0.008640 -0.002781  0.005900  0.229935 

    Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
    (Intercept)  1.172e-02  1.087e-03   10.78   <2e-16 ***
    size_product 1.666e-10  2.353e-13  707.87   <2e-16 ***
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

    Residual standard error: 0.01716 on 998 degrees of freedom
    Multiple R-squared:  0.998,	Adjusted R-squared:  0.998 
    F-statistic: 5.011e+05 on 1 and 998 DF,  p-value: < 2.2e-16
    #+end_example


  #+begin_src R :file images/dgemm_test_lm.png :results value graphics :results output :session *R* :exports both
  layout(matrix(c(1,2,3,4),2,2))
  plot(reg)
  #+end_src

  #+RESULTS:
  [[file:images/dgemm_test_lm.png]]

- In the above plots, we can observe similar trends than with =HPL_dgemm=, albeit less important.
  The data is slightly heteroscedastic and the residuals do not follow exactly a normal distribution. It seems that
  there are several “outliers” where =dgemm= takes significantly more time, i.e. the distribution of the residuals is
  skewed to the “right”.
- For instance, the entry n°208 has been obtained with sizes of 1503. It took a time of 0.807207.
  Let’s run this experiment again 100 times (with the command =./dgemm_test 1503 1503 1503 1503 1503 1503=). The min and
  the max over all observed times are respectively 0.5813 and 0.6494. The mean is 0.5897 and the standard deviation
  is 0.0082.
- Thus, it seems that this point is a real outlier. We can suppose that this is also true for the other similar points.
- This outlier is 0.2 seconds larger than the average we got and 0.15 seconds larger than the max. It seems very
  large. Maybe the process had a “bad” context switch (e.g. if it was moved to another core, but the execution time is
  not that high, so it seems unlikely).
- There seems to be a pattern, the outliers look to happen at regular intervals.
  #+begin_src R :results output :session *R* :exports both
  x = df[abs(df$time - (1.666e-10*df$size_product + 1.172e-2)) > 5e-2, ]
  x$id = which(abs(df$time - (1.666e-10*df$size_product + 1.172e-2)) > 5e-2)
  x$prev_id = c(0, x$id[1:(length(x$id)-1)])
  x$id_diff = x$id - x$prev_id
  x
  #+end_src

  #+RESULTS:
  #+begin_example
          time size_product lead_product ratio    m    n    k lead_A lead_B
  37  0.674633   3602686437   3602686437     1 1533 1533 1533   1533   1533
  38  0.409866   2053225511   2053225511     1 1271 1271 1271   1271   1271
  207 1.295097   7055792632   7055792632     1 1918 1918 1918   1918   1918
  208 0.807207   3395290527   3395290527     1 1503 1503 1503   1503   1503
  381 1.079795   5535839609   5535839609     1 1769 1769 1769   1769   1769
  558 0.453775   1869959168   1869959168     1 1232 1232 1232   1232   1232
  657 0.917557   4699421875   4699421875     1 1675 1675 1675   1675   1675
  748 1.233466   6414120712   6414120712     1 1858 1858 1858   1858   1858
  753 0.708934   3884701248   3884701248     1 1572 1572 1572   1572   1572
  914 1.337868   7166730752   7166730752     1 1928 1928 1928   1928   1928
      lead_C  id prev_id id_diff
  37    1533  37       0      37
  38    1271  38      37       1
  207   1918 207      38     169
  208   1503 208     207       1
  381   1769 381     208     173
  558   1232 558     381     177
  657   1675 657     558      99
  748   1858 748     657      91
  753   1572 753     748       5
  914   1928 914     753     161
  #+end_example

  We see here that the differences between the ids do not seem to be uniformly random. Some of them are small (1, 5),
  others are large (161, 169, 173, 177), or in between (37, 91, 99).
- This pattern has been reproduced by runing 1000 experiments with a size of 1503. Among the results, 26 of them are
  larger than 0.7 (mean of 0.6024, standard deviation of 0.0249, min of 0.5811, max of 0.8363).
  Here is the list of the differences between the indices of these elements. The list have been sorted:
  #+begin_example
  [1, 1, 1, 1, 1, 1, 2, 4, 4, 5, 7, 7, 10, 15, 20, 25, 28, 32, 42, 42, 43, 53, 108, 200, 201]
  #+end_example
  A lot of them are small or medium, and two are much larger.
**** Time prediction in HPL       :C:PYTHON:R:EXPERIMENTS:PERFORMANCE:HPL:
- Let’s try to predict the time that will be spend in =HPL_dgemm=, and compare it with the real time.
  The aim is then to have a cheap SimBLAS: replacing calls to the function by a sleep of the predicted time.
  We have this =printf= before the calls to =HPL_dgemm=:
  #+begin_src c
  printf("line=%d rank=%d m=%d n=%d k=%d lead_A=%d lead_B=%d lead_C=%d expected_time=%f\n",
          __LINE__+3, rank, mp, nn, jb, ldl2, LDU, lda, expected_time);
  #+end_src
  We do as before: we run HPL with P=Q=4 and N=20000. The trace is dumped in =/tmp/trace= and =stdout= is redirected to
  =/tmp/output=.
- Processing of the output:
#+begin_src python
import re
import csv
reg = re.compile('line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=([0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=([0-9]+) expected_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'expected_time'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    csv_writer.writerow(tuple(match.group(i) for i in range(1,10)))
process('/tmp/output', '/tmp/sizes.csv')
#+end_src

#+RESULTS:
: None
- We process the trace as before, we get a dataframe =durations=.
  #+begin_src R :results output :session *R* :exports both
  head(durations)
  #+end_src

  #+RESULTS:
  :     rank     start      end duration     state startline                           startfile endline                             endfile idx
  : 481    0  3.480994  6.54468 3.063686 Computing       388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481
  : 486    0  7.225255 10.24889 3.023633 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486
  : 491    0 10.803780 13.82799 3.024215 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491
  : 496    0 14.230774 17.26467 3.033897 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496
  : 977    0 17.676746 20.58197 2.905229 Computing       388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977
  : 982    0 21.258337 24.16961 2.911277 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982

  #+begin_src R :results output :session *R* :exports both
  sizes <- read.csv("/tmp/sizes.csv");
  head(sizes)
  #+end_src

  #+RESULTS:
  :   line rank    n    m   k lead_A lead_B lead_C expected_time
  : 1  413    8 5000 4920 120   5000   4920   5000      3.132548
  : 2  413   12 4920 4920 120   4920   4920   4920      3.082388
  : 3  413    4 5040 4920 120   5040   4920   5040      3.157628
  : 4  388    0 4920 4920 120   5040   4920   5040      3.082388
  : 5  413    5 5040 5040 120   5040   5040   5040      3.234704
  : 6  413    9 5000 5040 120   5000   5040   5000      3.209012

  #+begin_src R :results output :session *R* :exports both
  insert_sizes = function(durations, sizes) {
      stopifnot(nrow(durations)==nrow(sizes))
      ndf = data.frame();
      for(i in (sort(unique(durations$rank)))) {
          tmp_dur = durations[durations$rank == i,]
          tmp_sizes = sizes[sizes$rank == i,]
          stopifnot(nrow(tmp_dur) == nrow(tmp_sizes))
          stopifnot(tmp_dur$startline == tmp_sizes$line)
          storage.mode(tmp_sizes$m) <- "double" # avoiding integer overflow when taking the product
          storage.mode(tmp_sizes$n) <- "double"
          storage.mode(tmp_sizes$k) <- "double"
          storage.mode(tmp_sizes$lead_A) <- "double"
          storage.mode(tmp_sizes$lead_B) <- "double"
          storage.mode(tmp_sizes$lead_C) <- "double"
          tmp_dur$m = tmp_sizes$m
          tmp_dur$n = tmp_sizes$n
          tmp_dur$k = tmp_sizes$k
          tmp_dur$lead_A = tmp_sizes$lead_A
          tmp_dur$lead_B = tmp_sizes$lead_B
          tmp_dur$lead_C = tmp_sizes$lead_C
          tmp_dur$lead_product = tmp_sizes$lead_A * tmp_sizes$lead_B * tmp_sizes$lead_C
          tmp_dur$size_product = tmp_sizes$m * tmp_sizes$n * tmp_sizes$k
          tmp_dur$ratio = tmp_dur$lead_product/tmp_dur$size_product
          tmp_dur$expected_time = tmp_sizes$expected_time
          tmp_dur$absolute_time_diff = tmp_dur$expected_time - tmp_dur$duration
          tmp_dur$relative_time_diff = (tmp_dur$expected_time - tmp_dur$duration)/tmp_dur$expected_time
          ndf = rbind(ndf, tmp_dur)
      }
      return(ndf);
  }
  #+end_src

  #+begin_src R :results output :session *R* :exports both
  result = insert_sizes(durations, sizes)
  head(result)
  #+end_src

  #+RESULTS:
  #+begin_example
      rank     start      end duration     state startline                           startfile endline                             endfile idx    m    n   k lead_A lead_B lead_C lead_product
  481    0  3.480994  6.54468 3.063686 Computing       388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 481 4920 4920 120   5040   4920   5040 124975872000
  486    0  7.225255 10.24889 3.023633 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 486 4920 4920 120   4920   4920   5040 122000256000
  491    0 10.803780 13.82799 3.024215 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 491 4920 4920 120   4920   4920   5040 122000256000
  496    0 14.230774 17.26467 3.033897 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 496 4920 4920 120   4920   4920   5040 122000256000
  977    0 17.676746 20.58197 2.905229 Computing       388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     388 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 977 4800 4800 120   5040   4800   5040 121927680000
  982    0 21.258337 24.16961 2.911277 Computing       413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c     413 /hpl-2.2/src/pgesv/hpl_pdupdatett.c 982 4800 4800 120   4800   4800   5040 116121600000
      size_product    ratio expected_time absolute_time_diff relative_time_diff
  481   2904768000 43.02439      3.082388           0.018702        0.006067374
  486   2904768000 42.00000      3.082388           0.058755        0.019061520
  491   2904768000 42.00000      3.082388           0.058173        0.018872705
  496   2904768000 42.00000      3.082388           0.048491        0.015731634
  977   2764800000 44.10000      2.933742           0.028513        0.009718987
  982   2764800000 42.00000      2.933742           0.022465        0.007657456
#+end_example

#+begin_src R :file images/trace7_16.png :results value graphics :session *R* :exports both
ggplot(result, aes(x=idx, y=absolute_time_diff, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Absolute difference between the expected time and the real time")
#+end_src

#+RESULTS:
[[file:images/trace7_16.png]]

#+begin_src R :file images/trace8_16.png :results value graphics :session *R* :exports both
ggplot(result, aes(x=start, y=absolute_time_diff, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Absolute difference between the expected time and the real time")
#+end_src

#+RESULTS:
[[file:images/trace8_16.png]]

#+begin_src R :file images/trace9_16.png :results value graphics :session *R* :exports both
ggplot(result, aes(x=start, y=relative_time_diff, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Relative difference between the expected time and the real time")
#+end_src

#+RESULTS:
[[file:images/trace9_16.png]]

#+begin_src R :file images/trace10_16.png :results value graphics :session *R* :exports both
ggplot(result[result$start < 200,], aes(x=start, y=relative_time_diff, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Relative difference between the expected time and the real time\n“Large enough” matrices")
#+end_src

#+RESULTS:
[[file:images/trace10_16.png]]

  #+begin_src R :results output :session *R* :exports both
  for(i in (sort(unique(result$rank)))) {
      print(sum(result[result$rank == i,]$absolute_time_diff))
  }
  #+end_src

  #+RESULTS:
  #+begin_example
  [1] 1.494745
  [1] 1.343339
  [1] -2.940891
  [1] -1.11672
  [1] 0.466087
  [1] 1.90049
  [1] -3.441326
  [1] -1.564635
  [1] -2.708597
  [1] -1.647053
  [1] 0.027765
  [1] -4.653833
  [1] 2.878523
  [1] 3.572304
  [1] 1.124928
  [1] 3.749203
#+end_example

- We can see several things.
  + There are very large differences between the ranks. We could already see it in the first plots (=duration= vs
    =size_product=), but it is even more obvious here. We should find why.
  + There are some outliers that may have a very significant impact on the agregated difference between prediction and
    reality.
  + The prediction ability of this is better than =SMPI_Sample=, but still far from perfect.
**** Let’s try a cheap SimBLAS                    :SMPI:C:PERFORMANCE:HPL:
- We can replace the call to =HPL_dgemm= by the following:
  #+begin_src c
  double expected_time = (1.062e-09)*(double)mp*(double)nn*(double)jb - 2.476e-03
  if(expected_time > 0)
      smpi_usleep((useconds_t)(expected_time*1e6));
  #+end_src
- First test: it works pretty well. We roughly got the same results than with the true call to =HPL_dgemm=: 2.329e+01
  Gflops, against 2.332e01, 2.305e01 and 2.315e01 Gflops. The simulation time is much shorter, about 46 seconds, against
  about 495 seconds (8 minutes and 15 seconds). Note than with or without a real call to =HPL_dgemm=, the time spent
  outside of the application is much lower: between 6 and 8 seconds. Thus, there is room for new optimizations.
**** Tracking the other expensive BLAS functions         :PERFORMANCE:HPL:
- In the file =hpl_blas.h=, several functions are defined like =HPL_dgemm=, with =#define= aliasing them to the real =cblas= function.
- We can try to replace them by a no-op, to see if it changes the simulation time significantly.
- The following table sum up the (very approximate) gain we get on simulation time if we remove each of the
  functions. We use the same parameters than above for HPL.

  | Function   | time (s) |
  |------------+----------|
  | =HPL_dswap=  |      0.5 |
  | =HPL_dcopy=  |      N/A |
  | =HPL_daxpy=  |        0 |
  | =HPL_dscal=  |      N/A |
  | =HPL_idamax= |      N/A |
  | =HPL_dgemv=  |        1 |
  | =HPL_dtrsv=  |        0 |
  | =HPL_dger=   |      0.5 |
  | =HPL_dtrsm=  |       10 |

  + The function =HPL_idamax= cannot be removed, since it returns an integer used to index an array.
  + The functions =HPL_dscal= and =HPL_dcopy= cannot be removed either, since removing them causes the following error:
    #+begin_example
    /home/tom/simgrid/src/simix/smx_global.cpp:557: [simix_kernel/CRITICAL] Oops ! Deadlock or code not perfectly clean.
    #+end_example
- It is clear that we should now focus on =HPL_dtrsm=. This function solves a triangular system of equations.
- It is also clear that the time spent in the application is not entirely spent in the BLAS functions, we should look
  for something else.
**** Forgot a call to =HPL_dgemm=                          :PERFORMANCE:HPL:
- I found out that I forgot a place where =HPL_dgemm= was used.
- If we remove all additional occurences of =HPL_dgemm=, we gain 6 seconds (in addition of the high gain we already had).
- I thought that it was used only in =HPL_pduptateTT=, but it appears that it is also used in =HPL_pdrpanllT=.
- The call to =HPL_dgemm= was correctly traced. But I filtered the results in the R script and kept only the ones of =HPL_pdupdateTT=.
- The =printf= function with the parameters was only present in =HPL_pdupdateTT=.
- Consequently, all the visualizations and linear regressions were done with missing data. We should redo them to check
  if this changes anything.
**** Looking at =HPL_dtrsm=                                :PERFORMANCE:HPL:
- This function is used in a lot of functions: =HPL_pdrpan***= and =HPL_pdupdate**= (each has several variants).
- By aliasing this function to =printf("%s\n", __FILE___)= and filtering the output with =awk '!a[$0]++'= (remove duplicates),
  we know that, in our settings, =HPL_dtrsm= is only used in =HPL_pdrpanllT= and =HPL_pdupdateTT=. By sorting with =sort= and
  then counting duplicates with =uniq -dc=, we know that =HPL_pdrpanllT= (resp. =HPL_pdupdateTT=) call our function 78664 times
  (resp. 2636 times).
*** 2017-03-09 Thursday
**** Fix =HPL_dgemm= trace                                   :C:TRACING:HPL:
- In the old version, the calls to =MPI_Wait= were done in the =#include=, so we were sure that every call to =HPL_dgemm= was
  traced by Simgrid. However, the =printf= for the parameters had to be done before every call to =HPL_dgemm=, this is why
  I missed some of them.
- Now, the =printf= is also done in the =#include=. Because we need to have the arguments given to =HPL_dgemm= here, we cannot
  anymore use variadic arguments. We have to put all the parameters.
- The code is now as follows:
  #+begin_src c
  #define  HPL_dgemm(layout, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc)  ({\
      int my_rank, buff=0;\
      MPI_Request request;\
      MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
      double expected_time = (1.062e-09)*(double)M*(double)N*(double)K - 2.476e-03;\
      printf("file=%s line=%d rank=%d m=%d n=%d k=%d lead_A=%d lead_B=%d lead_C=%d expected_time=%f\n", __FILE__, __LINE__+3, my_rank, M, N, K, lda, ldb, ldc, expected_time);\
      MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
      MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
      MPI_Wait(&request, MPI_STATUS_IGNORE);\
      cblas_dgemm(layout, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);\
      MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
      MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
      MPI_Wait(&request, MPI_STATUS_IGNORE);\
  })
  #+end_src
**** Tentative of linear regression of =HPL_dgemm=: failed, there is a bug somewhere :PYTHON:R:EXPERIMENTS:PERFORMANCE:BUG:
- In the other linear regressions, some calls to =HPL_dgemm= were missing. Thus, the analysis need to be done again, just
  to check if it changes anything.
- I tried to run roughly the same process as above, but failed, there seems to be a bug somewhere.
- Everything piece of code is written here. The trace and the output have been obtained with N=5000 and P=Q=4.
Clean the file:
#+begin_src sh
pj_dump --user-defined --ignore-incomplete-links /tmp/trace > /tmp/trace.csv
grep "State," /tmp/trace.csv | grep MPI_Wait | sed -e 's/()//' -e 's/MPI_STATE, //ig'  -e 's/State, //ig' -e 's/rank-//' -e\
's/PMPI_/MPI_/' | grep MPI_  | tr 'A-Z' 'a-z' > /tmp/trace_processed.csv
#+end_src

Clean the paths:
#+begin_src python
import re
reg = re.compile('((?:[^/])*)(?:/[a-zA-Z0-9_-]*)*((?:/hpl-2.2(?:/[a-zA-Z0-9_-]*)*).*)')
def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            for line in in_f:
                match = reg.match(line)
                out_f.write('%s%s\n' % (match.group(1), match.group(2)))
process('/tmp/trace_processed.csv', '/tmp/trace_cleaned.csv')
 #+end_src

 #+RESULTS:
 : None

#+begin_src R :results output :session *R* :exports both
df <- read.csv("/tmp/trace_cleaned.csv", header=F, strip.white=T, sep=",");
names(df) = c("rank", "start", "end", "duration", "level", "state", "Filename", "Linenumber");
head(df)
#+end_src

#+RESULTS:
#+begin_example
  rank    start      end duration level    state
1    8 0.207257 0.207257        0     0 mpi_wait
2    8 0.207275 0.207275        0     0 mpi_wait
3    8 0.207289 0.207289        0     0 mpi_wait
4    8 0.207289 0.207289        0     0 mpi_wait
5    8 0.207309 0.207309        0     0 mpi_wait
6    8 0.207309 0.207309        0     0 mpi_wait
                            Filename Linenumber
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c        222
#+end_example

#+BEGIN_SRC R :results output :session *R* :exports both
duration_compute = function(df) {
    ndf = data.frame();
    df = df[with(df,order(rank,start)),];
    #origin = unique(df$origin)
    for(i in (sort(unique(df$rank)))) {
        start     = df[df$rank==i,]$start;
        end       = df[df$rank==i,]$end;
        l         = length(end);
        end       = c(0,end[1:(l-1)]); # Computation starts at time 0

        startline = c(0, df[df$rank==i,]$Linenumber[1:(l-1)]);
        startfile = c("", as.character(df[df$rank==i,]$Filename[1:(l-1)]));
        endline   = df[df$rank==i,]$Linenumber;
        endfile   = df[df$rank==i,]$Filename;

        ndf       = rbind(ndf, data.frame(rank=i, start=end, end=start,
            duration=start-end, state="Computing",
            startline=startline, startfile=startfile, endline=endline,
            endfile=endfile));
    }
    ndf$idx = 1:length(ndf$duration)
    ndf;
}
durations = duration_compute(df);
durations = durations[as.character(durations$startfile) == as.character(durations$endfile) &
    durations$startline == durations$endline,]
#+END_SRC

#+BEGIN_SRC R :results output :session *R* :exports both
head(durations)
#+END_SRC

#+RESULTS:
#+begin_example
  rank    start      end duration     state startline
2    0 0.207097 0.207149  5.2e-05 Computing       222
3    0 0.207149 0.207179  3.0e-05 Computing       222
4    0 0.207179 0.207179  0.0e+00 Computing       222
5    0 0.207179 0.207194  1.5e-05 Computing       222
6    0 0.207194 0.207194  0.0e+00 Computing       222
7    0 0.207194 0.207207  1.3e-05 Computing       222
                           startfile endline                            endfile
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
7 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     222 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
  idx
2   2
3   3
4   4
5   5
6   6
7   7
#+end_example

#+BEGIN_SRC R :results output :session *R* :exports both
unique(durations[c("startfile", "startline")])
#+END_SRC

#+RESULTS:
:                               startfile startline
: 2    /hpl-2.2/src/pfact/hpl_pdrpanllt.c       222
: 14         /hpl-2.2/src/comm/hpl_sdrv.c       191
: 478      /hpl-2.2/src/pgesv/hpl_rollt.c       242
: 481 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       384
: 486 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       407

We need to check each of these to see if this is indeed a call to =HPL_dgemm=, or something else.
It appears that =HPL_rollT= and =HPL_sdrv= are not calling =HPL_dgemm=, they are just calling =MPI_Wait=. Thus, we have to
remove them.

#+BEGIN_SRC R :results output :session *R* :exports both
durations = durations[durations$startfile != "/hpl-2.2/src/comm/hpl_sdrv.c" & durations$startfile != "/hpl-2.2/src/pgesv/hpl_rollt.c",]
unique(durations[c("startfile", "startline")])
#+END_SRC


#+RESULTS:
:                               startfile startline
: 2    /hpl-2.2/src/pfact/hpl_pdrpanllt.c       222
: 481 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       384
: 486 /hpl-2.2/src/pgesv/hpl_pdupdatett.c       407

Now, let us get what was output by the =printf=.

Processing the output:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=([0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=([0-9]+) expected_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'expected_time'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 11))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/parameters.csv')
#+end_src

#+begin_src R :results output :session *R* :exports both
parameters <- read.csv("/tmp/parameters.csv");
head(parameters)
#+end_src

#+RESULTS:
#+begin_example
                                file line rank    n  m k lead_A lead_B lead_C
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 1320 60 0   1320    120   1320
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    8 1200 60 0   1200    120   1200
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 1320 30 0   1320    120   1320
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    4 1280 60 0   1280    120   1280
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 1320 16 0   1320    120   1320
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 1320  8 0   1320    120   1320
  expected_time
1     -0.002476
2     -0.002476
3     -0.002476
4     -0.002476
5     -0.002476
6     -0.002476
#+end_example

A first remark: we see that some rows have k=0, which is a bit surprising. I double-checked by adding some =printf= in the
files, this is not a bug. This only happens in =HPL_pdrpanllT= so it was unnoticed until now.

#+begin_src R :results output :session *R* :exports both
nrow(parameters)
nrow(durations)
nrow(parameters[parameters$file == "/hpl-2.2/src/pfact/hpl_pdrpanllt.c",])
nrow(durations[durations$startfile == "/hpl-2.2/src/pfact/hpl_pdrpanllt.c",])
#+end_src

#+RESULTS:
: [1] 20300
: [1] 29964
: [1] 19664
: [1] 29328

- There is obviously something wrong. We should have a one-to-one correspondance between the elements of the =parameters=
  dataframe and the elements of the =durations= dataframe. It seems here that SMPI has produced additional entries in the
  trace, or some of the =printf= I put disapeared.
- This is not an error in parsing the output (e.g. some lines not parsed because of a wrong format/regexp). The output
  file has 20359 lines.
- Tried puting a =printf("blabla\n")= just before =HPL_dgemm= in the file =HPL_pdrpanllT.c= and counted the number of times it
  appeared. Exactly the same number, so definitely not an issue with the parsing or the definition with the =#define=.
- Checked the =durations= dataframe. Nothing apparently wrong, all the entries for this file are at the same line, so I
  did not miss a hidden =MPI_Wait= somewhere else in this same file).
**** Using another way to measure durations :C:PYTHON:R:EXPERIMENTS:TRACING:PERFORMANCE:HPL:
- Let’s use something else than SMPI trace to measure durations. We will measure the time directly in the code. But
  first we need to check that this new measure is consistent with what we got with the traces.
- Now, =HPL_dgemm= is defined as:
#+begin_src c
#define  HPL_dgemm(layout, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc)  ({\
  int my_rank, buff=0;\
  MPI_Request request;\
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
  double expected_time = (1.062e-09)*(double)M*(double)N*(double)K - 2.476e-03;\
  struct timeval before = {};\
  struct timeval after = {};\
  gettimeofday(&before, NULL);\
  MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
  MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
  MPI_Wait(&request, MPI_STATUS_IGNORE);\
  cblas_dgemm(layout, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);\
  gettimeofday(&after, NULL);\
  MPI_Isend(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, &request);\
  MPI_Recv(&buff, 1, MPI_INT, my_rank, 0, MPI_COMM_WORLD, NULL);\
  MPI_Wait(&request, MPI_STATUS_IGNORE);\
  double time_before = (double)(before.tv_sec) + (double)(before.tv_usec)*1e-6;\
  double time_after = (double)(after.tv_sec) + (double)(after.tv_usec)*1e-6;\
  double real_time = time_after-time_before;\
  printf("file=%s line=%d rank=%d m=%d n=%d k=%d lead_A=%d lead_B=%d lead_C=%d real_time=%f expected_time=%f\n", __FILE__, __LINE__, my_rank, M, N, K, lda, ldb, ldc, real_time, expected_time);\
})
#+end_src
- We run the same code than above to get the =durations= frame.
#+BEGIN_SRC R :results output :session *R* :exports both
head(durations)
#+END_SRC

#+RESULTS:
#+begin_example
  rank    start      end duration     state startline
2    0 0.275856 0.275896  4.0e-05 Computing       224
3    0 0.275896 0.275929  3.3e-05 Computing       224
4    0 0.275929 0.275929  0.0e+00 Computing       224
5    0 0.275929 0.275948  1.9e-05 Computing       224
6    0 0.275948 0.275948  0.0e+00 Computing       224
7    0 0.275948 0.275965  1.7e-05 Computing       224
                           startfile endline                            endfile
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
7 /hpl-2.2/src/pfact/hpl_pdrpanllt.c     224 /hpl-2.2/src/pfact/hpl_pdrpanllt.c
  idx
2   2
3   3
4   4
5   5
6   6
7   7
#+end_example

Now, we process the parameters:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=([0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=([0-9]+) real_time=(-?[0-9]+.[0-9]+) expected_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'real_time', 'expected_time'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 12))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/parameters.csv')
#+end_src

#+begin_src R :results output :session *R* :exports both
parameters <- read.csv("/tmp/parameters.csv");
head(parameters)
#+end_src

#+RESULTS:
#+begin_example
                                file line rank    n  m k lead_A lead_B lead_C
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320 60 0   1320    120   1320
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320 30 0   1320    120   1320
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320 16 0   1320    120   1320
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320  8 0   1320    120   1320
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320  4 0   1320    120   1320
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  224    0 1320  2 0   1320    120   1320
  real_time expected_time
1   8.1e-05     -0.002476
2   0.0e+00     -0.002476
3   0.0e+00     -0.002476
4   0.0e+00     -0.002476
5   1.0e-06     -0.002476
6   0.0e+00     -0.002476
#+end_example

We merge the =durations= and =parameters= dataframes, but only the entries for the file =hpl_pdupdatett.c= (we cannot do it
for the other file since we have a mismatch).
#+begin_src R :results output :session *R* :exports both
insert_sizes = function(durations, sizes) {
    stopifnot(nrow(durations)==nrow(sizes))
    ndf = data.frame();
    for(i in (sort(unique(durations$rank)))) {
        tmp_dur = durations[durations$rank == i,]
        tmp_sizes = sizes[sizes$rank == i,]
        stopifnot(nrow(tmp_dur) == nrow(tmp_sizes))
        stopifnot(tmp_dur$startline == tmp_sizes$line)
        storage.mode(tmp_sizes$m) <- "double" # avoiding integer overflow when taking the product
        storage.mode(tmp_sizes$n) <- "double"
        storage.mode(tmp_sizes$k) <- "double"
        storage.mode(tmp_sizes$lead_A) <- "double"
        storage.mode(tmp_sizes$lead_B) <- "double"
        storage.mode(tmp_sizes$lead_C) <- "double"
        tmp_dur$m = tmp_sizes$m
        tmp_dur$n = tmp_sizes$n
        tmp_dur$k = tmp_sizes$k
        tmp_dur$lead_A = tmp_sizes$lead_A
        tmp_dur$lead_B = tmp_sizes$lead_B
        tmp_dur$lead_C = tmp_sizes$lead_C
        tmp_dur$lead_product = tmp_sizes$lead_A * tmp_sizes$lead_B * tmp_sizes$lead_C
        tmp_dur$size_product = tmp_sizes$m * tmp_sizes$n * tmp_sizes$k
        tmp_dur$ratio = tmp_dur$lead_product/tmp_dur$size_product
        tmp_dur$real_time = tmp_sizes$real_time
        tmp_dur$expected_time = tmp_sizes$expected_time
        tmp_dur$absolute_time_diff = tmp_dur$expected_time - tmp_dur$duration
        tmp_dur$relative_time_diff = (tmp_dur$expected_time - tmp_dur$duration)/tmp_dur$expected_time
        ndf = rbind(ndf, tmp_dur)
    }
    return(ndf);
}
#+end_src

#+begin_src R :results output :session *R* :exports both
result = insert_sizes(durations[durations$startfile == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c",], parameters[parameters$file == "/hpl-2.2/src/pgesv/hpl_pdupdatett.c",])
#+end_src

Now we plot the time measured by SMPI traces against the time measured by =gettimeofday=.

#+begin_src R :file images/gettimeofday.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(result, aes(x=duration, y=real_time)) +
    geom_point(shape=1) + ggtitle("Time measured by SMPI against time measured by gettimeofday")
#+end_src

#+RESULTS:
[[file:images/gettimeofday.png]]

Checking with a linear regression, just to be sure:
#+begin_src R :results output :session *R* :exports both
summary(lm(duration~real_time, data=result))
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ real_time, data = result)

Residuals:
       Min         1Q     Median         3Q        Max 
-4.917e-05 -4.088e-06  1.075e-06  5.261e-06  6.181e-05 

Coefficients:
              Estimate Std. Error    t value Pr(>|t|)    
(Intercept) -2.617e-06  6.285e-07     -4.163 3.57e-05 ***
real_time    9.999e-01  7.058e-06 141678.252  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.034e-05 on 634 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:      1 
F-statistic: 2.007e+10 on 1 and 634 DF,  p-value: < 2.2e-16
#+end_example

It is not perfect, but it looks pretty great. So, let’s use this to measure time.

**** Now we can finally re-do the analysis of =HPL_dgemm= :R:EXPERIMENTS:PERFORMANCE:HPL:
- There are less things to do, since all the data come from the output file.
- Recall the aim of doing this again: in the previous analysis, some calls to =HPL_dgemm= were missing. Thus, it needs to
  be done again, just to check if it changes anything.
- Generate the CSV file by runing the same Python script as in the previous section (the output format did not change).
- Then, analysis in R:
#+begin_src R :results output :session *R* :exports both
results <- read.csv("/tmp/parameters.csv");
head(results)
#+end_src

#+RESULTS:
#+begin_example
                                file line rank    n  m k lead_A lead_B lead_C
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 60 0   5040    120   5040
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 30 0   5040    120   5040
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 16 0   5040    120   5040
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040  8 0   5040    120   5040
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040  4 0   5040    120   5040
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    8 5000 60 0   5000    120   5000
  real_time expected_time
1   5.7e-05     -0.002476
2   7.0e-06     -0.002476
3   0.0e+00     -0.002476
4   0.0e+00     -0.002476
5   0.0e+00     -0.002476
6   9.0e-06     -0.002476
#+end_example

#+begin_src R :results output :session *R* :exports both
process_results = function(results) {
    storage.mode(results$m) <- "double" # avoiding integer overflow when taking the product
    storage.mode(results$n) <- "double"
    storage.mode(results$k) <- "double"
    storage.mode(results$lead_A) <- "double"
    storage.mode(results$lead_B) <- "double"
    storage.mode(results$lead_C) <- "double"
    results$lead_product = results$lead_A * results$lead_B * results$lead_C
    results$size_product = results$m * results$n * results$k
    results$ratio = results$lead_product/results$size_product
    results$absolute_time_diff = results$expected_time - results$real_time
    results$relative_time_diff = (results$expected_time - results$real_time)/results$expected_time
    results$idx = 1:length(results$rank)
    return(results);
}
#+end_src

#+begin_src R :results output :session *R* :exports both
results = process_results(results)
head(results)
#+end_src

#+RESULTS:
#+begin_example
                                file line rank    n  m k lead_A lead_B lead_C
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 60 0   5040    120   5040
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 30 0   5040    120   5040
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040 16 0   5040    120   5040
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040  8 0   5040    120   5040
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    0 5040  4 0   5040    120   5040
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222    8 5000 60 0   5000    120   5000
  real_time expected_time lead_product size_product ratio absolute_time_diff
1   5.7e-05     -0.002476   3048192000            0   Inf          -0.002533
2   7.0e-06     -0.002476   3048192000            0   Inf          -0.002483
3   0.0e+00     -0.002476   3048192000            0   Inf          -0.002476
4   0.0e+00     -0.002476   3048192000            0   Inf          -0.002476
5   0.0e+00     -0.002476   3048192000            0   Inf          -0.002476
6   9.0e-06     -0.002476   3000000000            0   Inf          -0.002485
  relative_time_diff idx
1           1.023021   1
2           1.002827   2
3           1.000000   3
4           1.000000   4
5           1.000000   5
6           1.003635   6
#+end_example

#+begin_src R :file images/trace_gettimeofday1_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results, aes(x=idx, y=real_time, color=factor(file))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace_gettimeofday1_16.png]]


This is the plot of the duration of =HPL_dgemm= over time (analogous to the plot =duration= vs =start= that we had). The part
for =hpl_pduptatett= looks exactly as before. We see that the calls to =HPL_dgemm= in =hpl_pdrpanllt= are always very short.

#+begin_src R :file images/trace_gettimeofday2_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results, aes(x=size_product, y=real_time, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace_gettimeofday2_16.png]]

Without surprise, we find exactly the same kind of plot as before, since all the new calls to =HPL_dgemm= are very short
and thus hidden in the left part of the graph.


#+begin_src R :results output :session *R* :exports both
reg <- lm(duration~I(m*n*k), data=result)
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = duration ~ I(m * n * k), data = result)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.004843 -0.001337 -0.000024  0.000280  0.055746 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.393e-04  2.182e-04   1.097    0.273    
I(m * n * k) 1.064e-09  2.615e-12 406.932   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.003594 on 634 degrees of freedom
Multiple R-squared:  0.9962,	Adjusted R-squared:  0.9962 
F-statistic: 1.656e+05 on 1 and 634 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :file images/reg_gettimeofday_16.png :results value graphics :results output :session *R* :exports both
layout(matrix(c(1,2,3,4),2,2))
plot(reg)
#+end_src

#+RESULTS:
[[file:images/reg_gettimeofday_16.png]]

The summary of the linear regression shows that the factor =m*n*k= barely changed. The intercept is very different, but
its t-value is too low, so it is not meaning-full.
The residuals vs fitted plot seems to look better, with no more heteroscedasticity. My guess is that we added a lot of
points with very low values, so their weight hide the problem.
The QQ-plot still looks problematic.
**** Replacing =HPL_dgemm= by =smpi_usleep= again       :SMPI:PERFORMANCE:HPL:
- As for the =printf=, we will put the =smpi_usleep= in the =#define=. We take the coefficients of the latest linear regression.
- Testing: we still get the same number of Gflops (about 23 Gflops) but the simulation runs in 41 seconds now.
*** 2017-03-10 Friday
**** Tracing =HPL_dtrsm=        :C:PYTHON:R:EXPERIMENTS:TRACING:PERFORMANCE:
- The goal is to do something similar for =HPL_dtrsm=. In a first time, we will trace the parameters used to call it and
  its durations, then we will do a linear regression, to finally replace it by a =smpi_usleep=.
- Recall that this function solves a triangular set of equations. It takes as input two m \times n matrices. We expect the
  complexity to be O(m*n).
- Replace the definition of =HPL_dtrsm= in =hpl_blas.h= by the following:
#+begin_src c
#define HPL_dtrsm(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({\
    int my_rank, buff=0;\
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
    struct timeval before = {};\
    struct timeval after = {};\
    gettimeofday(&before, NULL);\
    cblas_dtrsm(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb);\
    gettimeofday(&after, NULL);\
    double time_before = (double)(before.tv_sec) + (double)(before.tv_usec)*1e-6;\
    double time_after = (double)(after.tv_sec) + (double)(after.tv_usec)*1e-6;\
    double real_time = time_after-time_before;\
    printf("file=%s line=%d rank=%d m=%d n=%d lead_A=%d lead_B=%d real_time=%f\n", __FILE__, __LINE__, my_rank, M, N, lda, ldb, real_time);\
})
#+end_src
- Run the simulation:
#+begin_src sh
smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes\
--cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ../../../small_tests/hostfile_64.txt -platform\
../../../small_tests/cluster_fat_tree_64.xml ./xhpl > /tmp/output
#+end_src
- Process the output file:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) real_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'n', 'm', 'lead_A', 'lead_B', 'real_time'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 9))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/parameters.csv')
#+end_src

- Analysis in R:
#+begin_src R :results output :session *R* :exports both
results <- read.csv("/tmp/parameters.csv");
head(results)
#+end_src

#+RESULTS:
:                                 file line rank  n m lead_A lead_B real_time
: 1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 60 0    120    120  0.000102
: 2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 30 0    120    120  0.000013
: 3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 16 0    120    120  0.000000
: 4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  8 0    120    120  0.000000
: 5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  4 0    120    120  0.000000
: 6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  2 0    120    120  0.000000

#+begin_src R :results output :session *R* :exports both
process_results = function(results) {
    storage.mode(results$m) <- "double" # avoiding integer overflow when taking the product
    storage.mode(results$n) <- "double"
    storage.mode(results$lead_A) <- "double"
    storage.mode(results$lead_B) <- "double"
    results$lead_product = results$lead_A * results$lead_B
    results$size_product = results$m * results$n
    results$ratio = results$lead_product/results$size_product
 #  results$absolute_time_diff = results$expected_time - results$real_time
 #  results$relative_time_diff = (results$expected_time - results$real_time)/results$expected_time
    results$idx = 1:length(results$rank)
    return(results);
}
#+end_src

#+begin_src R :results output :session *R* :exports both
results = process_results(results)
head(results)
#+end_src

#+RESULTS:
#+begin_example
                                file line rank  n m lead_A lead_B real_time
1 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 60 0    120    120  0.000102
2 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 30 0    120    120  0.000013
3 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8 16 0    120    120  0.000000
4 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  8 0    120    120  0.000000
5 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  4 0    120    120  0.000000
6 /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171    8  2 0    120    120  0.000000
  lead_product size_product ratio idx
1        14400            0   Inf   1
2        14400            0   Inf   2
3        14400            0   Inf   3
4        14400            0   Inf   4
5        14400            0   Inf   5
6        14400            0   Inf   6
#+end_example

#+begin_src R :file images/trace_dtrsm1_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results, aes(x=idx, y=real_time, color=factor(file))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace_dtrsm1_16.png]]

We can observe a trend similar to =HPL_dgemm=. The function is only used in two places, =HPL_pdrpanllT= and
=HPL_pdupdateTT=. In the former, all the calls are very short, whereas in the later, the calls are long at the beginning
and become shorter throughout the execution. We also have some outliers.

#+begin_src R :file images/trace_dtrsm2_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results, aes(x=size_product, y=real_time, color=factor(rank))) +
    geom_point(shape=1) + ggtitle("Durations of HPL_dgemm")
#+end_src

#+RESULTS:
[[file:images/trace_dtrsm2_16.png]]

As expected, the duration looks proportional to the product of the sizes.


#+begin_src R :results output :session *R* :exports both
reg <- lm(real_time~I(m*n), data=results)
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = real_time ~ I(m * n), data = results)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.002999  0.000010  0.000010  0.000010  0.043651 

Coefficients:
              Estimate Std. Error  t value Pr(>|t|)    
(Intercept) -1.042e-05  2.445e-06   -4.263 2.02e-05 ***
I(m * n)     9.246e-08  3.915e-11 2361.957  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.0006885 on 81298 degrees of freedom
Multiple R-squared:  0.9856,	Adjusted R-squared:  0.9856 
F-statistic: 5.579e+06 on 1 and 81298 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :file images/reg_dtrsm_16.png :results value graphics :results output :session *R* :exports both
layout(matrix(c(1,2,3,4),2,2))
plot(reg)
#+end_src

#+RESULTS:
[[file:images/reg_dtrsm_16.png]]

The R-squared is high and both the intercept and sizes have a significant impact.
However, the outliers are even more concerning than with =HPL_dgemm=. The Q-Q plot shows a large tail, and the residual vs
leverage shows that these outliers are non-negligible in the linear regression (i.e. if we removed them, the
coefficients would change significantly).
**** Replacing =HPL_dtrsm= by =smpi_sleep=              :SMPI:PERFORMANCE:HPL:
- Similarly to what have been done with =HPL_dgemm=, we use the coefficients found with the linear regression to replace
  the function by a sleep.
#+begin_src c
#define HPL_dtrsm(layout, Side, Uplo, TransA, Diag, M, N, alpha, A, lda, B, ldb) ({\
    double expected_time = (9.246e-08)*(double)M*(double)N - 1.024e-05;\
    if(expected_time > 0)\
        smpi_usleep((useconds_t)(expected_time*1e6));\
})
#+end_src
- Running HPL again. We get the expected speed (about 23 Gflops) and a simulation time of 29 seconds (gain of 12 seconds).
**** Having a look at =malloc=                    :PYTHON:R:PERFORMANCE:HPL:
- To run HPL with larger matrices, we need to replace some calls to =malloc= (resp. =free=) by =SMPI_SHARED_MALLOC=
  (resp. =SMPI_SHARED_FREE=).
- Firstly, let’s see where the big allocations are.
- Define =MY_MALLOC= in =hpl.h= as follows:
#+begin_src c
#define MY_MALLOC(n) ({\
    int my_rank;\
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
    printf("file=%s line=%d rank=%d size=%lu\n", __FILE__, __LINE__, my_rank, n);\
    malloc(n);\
})
#+end_src
- Replace all the calls to =malloc= in the files by =MY_MALLOC=:
#+begin_src sh
grep -l malloc testing/**/*.c src/**/*.c | xargs sed -i 's/malloc/MY_MALLOC/g'
#+end_src
- Run =smpirun= (N=20000, P=Q=4) and redirect the output to =/tmp/output=.
- Process the output file:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) size=([0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'size'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 5))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/malloc.csv')
#+end_src
- Analysis in R:
#+begin_src R :results output :session *R* :exports both
results <- read.csv("/tmp/malloc.csv");
head(results)
#+end_src

#+RESULTS:
:                             file line rank size
: 1 /hpl-2.2/src/grid/hpl_reduce.c  127    0    4
: 2 /hpl-2.2/src/grid/hpl_reduce.c  127    1    4
: 3 /hpl-2.2/src/grid/hpl_reduce.c  127    2    4
: 4 /hpl-2.2/src/grid/hpl_reduce.c  127    3    4
: 5 /hpl-2.2/src/grid/hpl_reduce.c  127    4    4
: 6 /hpl-2.2/src/grid/hpl_reduce.c  127    5    4

#+begin_src R :file images/trace_malloc1_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results, aes(x=file, y=size)) +
    geom_boxplot() + ggtitle("Sizes of malloc") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
#+end_src

#+RESULTS:
[[file:images/trace_malloc1_16.png]]

#+begin_src R :results output :session *R* :exports both
storage.mode(results$size) <- "double" # avoiding integer overflow when taking the product
aggregated_results = aggregate(results$size, by=list(file=results$file), FUN=sum)
head(aggregated_results)
#+end_src

#+RESULTS:
:                                    file           x
: 1         /hpl-2.2/src/comm/hpl_packl.c     9034816
: 2        /hpl-2.2/src/grid/hpl_reduce.c     3200736
: 3 /hpl-2.2/src/panel/hpl_pdpanel_init.c 11592866048
: 4  /hpl-2.2/src/panel/hpl_pdpanel_new.c        3456
: 5     /hpl-2.2/src/pauxil/hpl_pdlange.c     2560032
: 6       /hpl-2.2/src/pfact/hpl_pdfact.c     2645504

#+begin_src R :file images/trace_malloc2_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(aggregated_results, aes(x=file, y=x)) +
    geom_boxplot() + ggtitle("Sizes of malloc") + theme(axis.text.x = element_text(angle = 90, hjust = 1))
#+end_src

#+RESULTS:
[[file:images/trace_malloc2_16.png]]


There are several things to notice:
- The biggest chunks are allocated in =HPL_pdtest=. These are the local matrices of each process.
- However, regarding the total quantity of allocated memory, =HPL_pdpanel_init= is the clear winner.
- In these tests, =htop= reported that about 20% of the 16GB of my laptop’s memory were used, i.e. about 3.2GB. We use a
  matrix of size 20000, each element is of type =double= (8 bytes), so the total amount of memory for the whole matrix is
  20000^2/8 = 3.2GB.
- Thus, it seems that the =malloc= used in =HPL_pdpanel_init= are in fact negligible. An hypothesis is that they are quickly
  followed by a =free=.
- Verifying that every process allocates the same thing:

#+begin_src R :file images/trace_malloc3_16.png :results value graphics :session *R* :exports both
library(ggplot2)
ggplot(results[results$file == "/hpl-2.2/testing/ptest/hpl_pdtest.c",], aes(x="", y=size, fill=factor(rank))) +
    coord_polar("y", start=0) +
    geom_bar(width=1, stat="identity") +
    ggtitle("Sizes of malloc in HPL_pdtest")
#+end_src

#+RESULTS:
[[file:images/trace_malloc3_16.png]]

#+begin_src R :results output :session *R* :exports both
res_pdtest = results[results$file == "/hpl-2.2/testing/ptest/hpl_pdtest.c",]
unique(res_pdtest[order(res_pdtest$size),]$size)
#+end_src

#+RESULTS:
: [1] 193729992 196879432 198454152 200080072 201680392 203293512

- The different calls to =malloc= in =HPL_pdtest= have approximately the same size, but not exactly. This understandable, P
  and Q may not divide the matrix sizes. Maybe this could cause =SMPI_SHARED_MALLOC= to not work properly?
**** Tentative to use =SMIP_SHARED_MALLOC= and =SMPI_SHARED_FREE= in HPL :SMPI:PERFORMANCE:BUG:HPL:
- Revert the previous changes regarding =malloc=.
- In file =hpl_pdtest.c=, replace =malloc= by =SMPI_SHARED_MALLOC= and =free= by =SMPI_SHARED_FREE=.
- Run HPL with Simgrid. Two issues:
  + The memory consumption stays the same, about 20% of my laptop’s memory. A first guess would be that the
    =SHARED_MALLOC= did not work, a new allocation was made for every process. Maybe because different sizes were given?
  + The execution time (both virutal and real) decreased significantly. The virtual time dropped from 233 to 223 seconds,
    the real time from 28 to 15 seconds. If we forget the first point, a guess could be that =SHARED_MALLOC= worked
    properly and resulted in a lower number of cache misses (since all processes share the same sub-matrix) and thus
    improved performances. It is an experimental bias, we should avoid it.
  The fact that we have these two issues combined is very surprising.
- Let’s try to see if the =SHARED_MALLOC= makes only one allocation or not, by adding some =printf= in its implementation.
  + The path =shmalloc_global= is taken.
  + The =bogusfile= is created only once, as expected.
  + Then, every process maps the file in memory, chunk by chunk. The base adress is not the same for every process, but
    this is not an issue (we are speaking of virtual memory here).
- Tested my matrix product program. Got 34% memory utilization, 44 virtual seconds and 8 real seconds with =SMPI_SHARED_MALLOC=, but 11% memory utilization, 81
  virtual seconds and 7 real seconds with =malloc=. Very strange.
- Hypothesis: either the measure of the memory consumption is broken, or =SHARED_MALLOC= is broken.
- Try to use something else than =htop=:
  #+begin_src sh
  watch -n 0,1 cat /proc/meminfo
  #+end_src
  + With =malloc= and =free=, the available memory drop from 14.4 GB to 11.0 GB.
  + With =SMPI_SHARED_MALLOC= and =SMPI_SHARED_FREE=, the available memory drop from 14.4 GB to 14.1 GB.
  This seems more coherent, so =htop= would be a bad tool to measure memory consumption when using =SMPI_SHARED_MALLOC=.
  But this does not solve the time issue.
*** 2017-03-12 Sunday
**** Experiment with SMPI macros in the matrix product code :C:R:EXPERIMENTS:PERFORMANCE:
- Use the matrix product code, at commit =91633ea99463109736b900c92f2eacc84630e5b5=. Run 10 tests with or without
  =SMPI_SHARED_MALLOC= and =SMPI_SAMPLE= with a matrix size of 4000 and 64 processes, by running the command:
  #+begin_src sh
  ./smpi_macros.py 10 /tmp/results.csv
  #+end_src
- Analysis, in R:
#+begin_src R :results output :session *R* :exports both
results <- read.csv("/tmp/results.csv");
head(results)
#+end_src

#+RESULTS:
:       time size smpi_sample smpi_malloc
: 1 2.134820 4000           1           1
: 2 2.608971 4000           0           0
: 3 3.767625 4000           1           0
: 4 2.412387 4000           0           1
: 5 3.767162 4000           1           0
: 6 2.497480 4000           0           0

We already see that the case where we use =SMPI_SAMPLE= but not =SMPI_SHARED_MALLOC= seems to be different than the others.

#+begin_src R :results output :session *R* :exports both
res_aov = aov(time~(smpi_sample + smpi_malloc)^2, data=results)
summary(res_aov)
#+end_src

#+RESULTS:
:                         Df Sum Sq Mean Sq F value   Pr(>F)    
: smpi_sample              1  1.202   1.202   9.227  0.00442 ** 
: smpi_malloc              1  4.579   4.579  35.163 8.62e-07 ***
: smpi_sample:smpi_malloc  1  8.332   8.332  63.981 1.68e-09 ***
: Residuals               36  4.688   0.130                     
: ---
: Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

#+begin_src R :file images/smpi_macros_1.png :results value graphics :results output :session *R* :exports both
suppressWarnings(suppressMessages(library(FrF2))) # FrF2 outputs a bunch of useless messages...
MEPlot(res_aov, abbrev=4, select=c(1, 2), response="time")
#+end_src

#+RESULTS:
[[file:images/smpi_macros_1.png]]

#+begin_src R :file images/smpi_macros_2.png :results value graphics :results output :session *R* :exports both
IAPlot(res_aov, abbrev=4, show.alias=FALSE, select=c(1, 2))
#+end_src

#+RESULTS:
[[file:images/smpi_macros_2.png]]

#+begin_src R :results output :session *R* :exports both
mean(results[results$smpi_sample == 0 & results$smpi_malloc == 0,]$time)
mean(results[results$smpi_sample == 0 & results$smpi_malloc == 1,]$time)
mean(results[results$smpi_sample == 1 & results$smpi_malloc == 0,]$time)
mean(results[results$smpi_sample == 1 & results$smpi_malloc == 1,]$time)
#+end_src

#+RESULTS:
: [1] 2.513953
: [1] 2.750056
: [1] 3.773385
: [1] 2.183901

- In this small experiment, we see that both macros have a non-negligible impact on the time estimated by SMPI. When
  none of the optimizations is used, adding one of them will decreases the application’s performances. When one of the
  optimizations is already used, adding the other one increases the application’s performances.
- When I have added the SMPI macros in =matmul.c=, I have firstly added =SMPI_SHARED_MALLOC= and then =SMPI_SAMPLE_GLOBAL=
  (see the entry for 13/02/2017). According to the tests above, here the variation is not huge (I did not try the
  configuration with =SMPI_SAMPLE_GLOBAL= and without =SMPI_SHARED_MALLOC=). Furthermore, I did not perform extensive
  tests. This may explain why I did not notice this sooner.
*** 2017-03-13 Monday
**** Let’s play with Grid 5000                                       :G5K:
- Connect to Grenoble’s site:
  #+begin_src sh
  ssh tocornebize@access.grid5000.fr
  ssh grenoble
  #+end_src
- Reserve a node and deploy:
  #+begin_src sh
  oarsub -I -l nodes=1,walltime=7 -t deploy
  kadeploy3 -f $OAR_NODE_FILE -e jessie-x64-big -k
  #+end_src
- Connect as root on the new node:
  #+begin_src sh
  ssh root@genepi-33.grenoble.grid5000.fr
  #+end_src
- Install Simgrid:
  #+begin_src sh
  wget https://github.com/simgrid/simgrid/archive/c8db21208f3436c35d3fdf5a875a0059719bff43.zip -O simgrid.zip
  unzip simgrid.zip
  cd simgrid-*
  mkdir build
  cd build
  cmake -Denable_documentation=OFF ..
  make -j 8
  make install
  #+end_src
- Copy HPL on the machine, with =scp=.
- Change the variable =TOPDIR= in the file =Make.SMPI=.
- Do not forget to clean HPL directory when copying it, otherwise the modification of the variable =TOPDIR= will not be
  applied on the sub-makefiles.
- Success of compilation and execution of HPL with Simgrid on one Grid5000 node.
- Strange thing: the virtual time did not change much (228 seconds, or 23.3 Gflops), although the simulation time
  changed a lot (50 seconds, against 15 seconds on my laptop) and I used the same value for the option =running-power=.
**** Scrit for automatic installation                          :SHELL:G5K:
- A small bash script to install Simgrid and compile HPL. Store it in file =deploy.sh=. It assume that archives for
  Simgrid and HPL are located in =/home/tocornebize=.
  #+begin_src sh
function abort {
    echo -e "\e[1;31m Error:" $1 "\e[0m"
    exit 1
}

rm -rf hpl* simgrid*
cp /home/tocornebize/{hpl,simgrid}.zip . &&\
unzip hpl.zip
unzip simgrid.zip
if [ $? -ne 0 ]
then
    abort "Could not copy or extract the archives."
fi

echo ""
echo -e "\e[1;34m Installing Simgrid\e[0m"
cd simgrid* &&\
mkdir build &&\
cd build &&\
cmake -Denable_documentation=OFF .. &&\
make -j 8 &&\
make install &&\
cd ../..
if [ $? -ne 0 ]
then
    abort "Could not install Simgrid."
fi

echo ""
echo -e "\e[1;34m Installing HPL\e[0m"
cd hpl* &&\
sed -ri "s|TOPdir\s+=.+|TOPdir="`pwd`"|g" Make.SMPI &&\ # fixing TOPdir variable
make startup -j 8 arch=SMPI &&\
make -j 8 arch=SMPI &&\
cd ..
if [ $? -ne 0 ]
then
    abort "Could not compile HPL."
fi

echo ""
echo -e "\e[1;32m Everything was ok\e[0m"
  #+end_src
- Given a node obtained with =oarsub= and =kadeploy3=, connect in ssh to it. Then, just run:
  #+begin_src sh
  /home/tocornebize/deploy.sh
  #+end_src
**** Recurrent failure in HPL with =SMPI_SHARED_MALLOC=       :SMPI:BUG:HPL:
- The following error often happens when runing HPL with =SMPI_SHARED_MALLOC=:
  #+begin_example
  src/simix/smx_global.cpp:557: [simix_kernel/CRITICAL] Oops ! Deadlock or code not perfectly clean.
  #+end_example
- It does not seem to happen without =SMPI_SHARED_MALLOC=.
- It does not always happen with =SMPI_SHARED_MALLOC=.
- I do not understand what is happening.
**** Another failure in HPL with =SMPI_SHARED_MALLOC=         :SMPI:BUG:HPL:
- Similarly, the tests on the matrix at the end of HPL are never computed when we use =SMPI_SHARED_MALLOC, because of an
  error. For instance:
  #+begin_example
  HPL ERROR from process # 0, on line 331 of function HPL_pdtest:
  >>> Error code returned by solve is 1021, skip <<<
  #+end_example
- Example of error code: 1021, 1322, 1324, 1575... These values appear nowhere in the code.
**** Tracking the error in HPL
- Put some =printf= to track the error.
*** 2017-03-14 Tuesday
**** Keep tracking the error.                       :SMPI:GIT:TRACING:BUG:
- Add the option =--cfg=smpi/simulate-computation:0= to have a deterministic execution.
- The error code is the field =info= of the matrix. It is modified in the execution path =HPL_pdgesv= \to =HPL_pdgesv0= \to
  =HPL_pdpanel_free=, by the following line:
  #+begin_src c
  if( PANEL->pmat->info == 0 ) PANEL->pmat->info = *(PANEL->DINFO);
  #+end_src
  Thus, we now have to track the values of the =DINFO= field in the panel.
- Strange thing, the field =DINFO= is a pointer to a =float=.
- To track this, use this function:
  #+begin_src c
  void print_info(HPL_T_panel *PANEL, int line) {
     if(PANEL->grid->myrow == 0 && PANEL->grid->mycol == 0) {
          printf("info = %f, line = %d\n", *PANEL->DINFO, line);
     }
  }
  #+end_src
  Put some calls to it at nearly every line of the target file (when you are done with a file, remove these calls).
- Field =DINFO= is modified in the execution path =HPL_pdgesv0= \to =HPL_pdfact= \to =panel->algo->rffun=. The pointer =rffun= is
  one of the functions =HPL_pdrpan***=. In our settings, =HPL_pdrpanllT= is used.
- Field =DINFO= is modified by =PANEL->algo->pffun=, which is one of the functions =HPL_pdpan***=. In our settings,
  =HPL_pdpanllT= is used.
- Then it is modified by the first call to =HPL_dlocswpT=. This function directly modifies the value of =DINFO= with the
  line:
  #+begin_src c
  if( *(PANEL->DINFO) == 0.0 )
     *(PANEL->DINFO) = (double)(PANEL->ia + JJ + 1);
  #+end_src
- If we remove this line, as expected the message about the error code disappears. So it confirms the error code come
  from here.
- Looking at =HPL_pdpanel_init.c=,
  + =DINFO= is a pointer to a part of =DPIV=:
    #+begin_src c
    PANEL->DINFO = PANEL->DPIV + JB;
    #+end_src
  + =DPIV= is a pointer to a part of =L1=:
    #+begin_src c
    PANEL->DPIV  = PANEL->L1    + JB * JB;
    #+end_src
  + =L1= is an (aligned) alias for =WORK=, which is itself a block of memory allocated with =malloc=:
    #+begin_src c
    PANEL->WORK = (void*) malloc((size_t)(lwork) * sizeof(double));
    // [...]
    PANEL->L1    = (double *)HPL_PTR( PANEL->WORK, dalign );
    #+end_src
  L1 is the jb \times jb upper block of the local matrix. It is used for computations. Thus, it seems that HPL expects a
  particular cell of this local matrix to have the value 0. This cell is not always the same.
  Interpretation: HPL is checking that the matrix is correctly factorized (it uses LU factorization, so it computes L
  and U such that A=LU, L is lower-triangular and U is upper-triangular). Since we use shared memory, it is not
  surprising that the correctness check do not pass anymore.
  What is more surprising is that this particular check was still passing when the two BLAS functions were replaced by
  =smpi_usleep=. A guess: the fact that the resulting matrices are triangular only depends on the correctness of the
  swapping of rows.
- Thus, it seems that the error code is explained. This is a normal behavior, considered what we are doing.
- The deadlock happening in some executions is not explained however.
**** Webinar                                                     :MEETING:
- Enabling open and reproducible research at computer system’s conferences: good, bad and ugly
- Grigori Fursin
- The speaker created an [[http://ctuning.org/][organization]] about reproducible research.
- Artifact evaluation is about peer review of experiments.
- How it works: papers accepted to a conference can ask for an artifact evaluation. If they pass it, they would get a
  nice stamp on the paper. If they fail it, nobody will know. View this as a bonus for a paper. For the evaluation of
  the artifacts, the conference nominates several reviewers.
- ACM conferences also start using this kind of things, with several different stamps.
- But artifact evaluation is not easy to do. Firstly, there is a lot of artifacts to evaluate, hard to scale. Some
  artifact evaluations require proprietary software and/or rare hardware (e.g. supercomputers). Also, hard to find a
  reviewer with suitable skills for some cases.
- Also, it is difficult to reproduce empirical results (changing software and hardware). Everyone has its own scripts,
  so hard to standardize a universal workflow.
- Other [[http://cknowledge.org][website]].
*** 2017-03-15 Wednesday
**** Hunting the deadlock                  :SMPI:PYTHON:R:TRACING:BUG:HPL:
- With N=40000, P=Q=4 and the option =--cfg=smpi/simulate-computation:0=, it seems we always have a deadlock.
- Let’s trace it, with option =-trace -trace-file /tmp/trace --cfg=smpi/trace-call-location:1=.
- Processing the trace file:
#+begin_src sh
pj_dump --user-defined --ignore-incomplete-links /tmp/trace > /tmp/trace.csv
grep "State," /tmp/trace.csv | sed -e 's/()//' -e 's/MPI_STATE, //ig'  -e 's/State, //ig' -e 's/rank-//' -e\
's/PMPI_/MPI_/' | grep MPI_  | tr 'A-Z' 'a-z' > /tmp/trace_processed.csv
#+end_src

Clean the paths:
#+begin_src python
import re
reg = re.compile('((?:[^/])*)(?:/[a-zA-Z0-9_-]*)*((?:/hpl-2.2(?:/[a-zA-Z0-9_-]*)*).*)')
def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            for line in in_f:
                match = reg.match(line)
                out_f.write('%s%s\n' % (match.group(1), match.group(2)))
process('/tmp/trace_processed.csv', '/tmp/trace_cleaned.csv')
 #+end_src

Analysis:
#+begin_src R :results output :session *R* :exports both
trace <- read.csv("/tmp/trace_cleaned.csv", header=F, strip.white=T, sep=",");
names(trace) = c("rank", "start", "end", "duration", "level", "state", "Filename", "Linenumber");
trace$idx = 1:length(trace$rank)
head(trace)
#+end_src

#+RESULTS:
#+begin_example
  rank    start      end duration level    state
1    8 0.000000 0.000000 0.000000     0 mpi_init
2    8 0.000000 0.000202 0.000202     0 mpi_recv
3    8 0.000202 0.000403 0.000201     0 mpi_recv
4    8 0.000403 0.000806 0.000403     0 mpi_recv
5    8 0.000806 0.000806 0.000000     0 mpi_send
6    8 0.000806 0.001612 0.000806     0 mpi_recv
                               Filename Linenumber idx
1 /hpl-2.2/testing/ptest/hpl_pddriver.c        109   1
2        /hpl-2.2/src/grid/hpl_reduce.c        165   2
3        /hpl-2.2/src/grid/hpl_reduce.c        165   3
4        /hpl-2.2/src/grid/hpl_reduce.c        165   4
5        /hpl-2.2/src/grid/hpl_reduce.c        159   5
6     /hpl-2.2/src/grid/hpl_broadcast.c        130   6
#+end_example

#+begin_src R :results output :session *R* :exports both
get_last_event = function(df) {
    result = data.frame() 
    for(rank in (sort(unique(trace$rank)))) {
        tmp_trace = trace[trace$rank == rank,]
        result = rbind(result, tmp_trace[which.max(tmp_trace$idx),])
    }
    return(result)
}
get_last_event(trace)[c(1, 2, 3, 6, 7, 8)]
#+end_src

#+RESULTS:
#+begin_example
      rank    start      end    state                         Filename
18756    0 67.01313 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
9391     1 66.84201 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
7865     2 66.92821 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
7048     3 67.01313 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
6242     4 67.08334 67.10575 mpi_send   /hpl-2.2/src/pgesv/hpl_rollt.c
4699     5 66.93228 67.10575 mpi_wait   /hpl-2.2/src/pgesv/hpl_rollt.c
3174     6 67.02313 67.10575 mpi_wait   /hpl-2.2/src/pgesv/hpl_rollt.c
2358     7 67.08334 67.10575 mpi_send   /hpl-2.2/src/pgesv/hpl_rollt.c
1554     8 67.08334 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
17201    9 66.93228 67.10575 mpi_send /hpl-2.2/src/pgesv/hpl_spreadt.c
15675   10 67.02313 67.10575 mpi_send /hpl-2.2/src/pgesv/hpl_spreadt.c
14858   11 67.08334 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
14053   12 67.06093 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
12516   13 66.88778 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
10998   14 66.97831 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
10189   15 67.06093 67.10575 mpi_recv /hpl-2.2/src/pgesv/hpl_spreadt.c
      Linenumber
18756        321
9391         321
7865         321
7048         321
6242         235
4699         242
3174         242
2358         235
1554         321
17201        351
15675        351
14858        321
14053        321
12516        321
10998        321
10189        321
#+end_example

If the trace is correct, the deadlock happens in functions =HPL_rollT= and =HPL_spreadT=.
Some =printf= confirm that the deadlock is indeed happening in these places.
**** Found the deadlock                                   :SMPI:C:BUG:HPL:
- Let’s add some =printf= in files =HPL_spreadT.c= and =HPL_rollT.c=.
  First, add the functions:
  #+begin_src c
  int local_rank_to_global(int local_rank, MPI_Comm local_communicator) {
      int result;
      MPI_Group local_group, world_group;
      MPI_Comm_group(local_communicator, &local_group);
      MPI_Comm_group(MPI_COMM_WORLD, &world_group);
      MPI_Group_translate_ranks(local_group, 1, &local_rank, world_group, &result);
      return result;
  }
  void print_info(int src_rank, int dst_rank, char *function, int line, char *file) {
      printf("src=%d dst=%d function=%s line=%d file=%s\n", src_rank, dst_rank, function,
  line, file);
  }
  #+end_src
  Then, add a call to =print_info= before each of the four lines we found:
  + =HPL_spreadT.c=, line 321:
    #+begin_src c
    int local_rank = local_rank_to_global(IPMAP[SRCDIST+partner], comm);
    print_info(my_rank, local_rank, "mpi_recv", __LINE__, __FILE__);
    #+end_src
  + =HPL_spreadT.c=, line 351:
    #+begin_src c
    int local_rank = local_rank_to_global(IPMAP[SRCDIST+partner], comm);
    print_info(my_rank, local_rank, "mpi_send", __LINE__, __FILE__);
    #+end_src
  + =HPL_rollT.c=, line 235:
    #+begin_src c
    int local_rank = local_rank_to_global(partner, comm);
    print_info(my_rank, local_rank, "mpi_send", __LINE__, __FILE__);
    #+end_src
  + =HPL_rollT.c=, line 242:
    #+begin_src c
    int local_rank = local_rank_to_global(partner, comm);
    print_info(my_rank, local_rank, "mpi_wait", __LINE__, __FILE__);
    #+end_src
- Then, run HPL with =stdout= redirected to a file =/tmp/output=.
- For each rank, look for the last time this rank was the caller of a blocking MPI primitive. For instance, for rank =15=:
  #+begin_src sh
  RANK="15 " && grep "src="$RANK /tmp/output | tail -n 1
  #+end_src
  Observe the destination and the function.
  With P=Q=4, we had these dependencies:
  #+begin_example
           12
            |
  mpi_recv  |
            |
            v     mpi_recv
            4 <——————————————+
            |                |
            |                |
  mpi_wait  |                |
            |                |
            v                |
            8 —————————————> 0
                 mpi_send
  #+end_example
  There is the same pattern for {1, 5, 9, 13}, {2, 6, 10, 14} and {3, 7, 11, 15}.
- This exact deadlock has been reproduced on Grid 5000, with the same parameters.
*** 2017-03-16 Thursday
**** Still looking for the deadlock                         :SMPI:BUG:HPL:
- When HPL is ran with =smpi_usleep= but without =SMPI_SHARED_{MALLOC,FREE}=, there is no deadlock, even with the same parameters (N=40000,
  P=Q=4). Warning: testing with N=40000 require a lot of memory, about 12GB.
- When HPL is ran with =SMPI_SHARED_{MALLOC,FREE}= but without =smpi_usleep=, there is a deadlock. Note that we still use
  the option =--cfg=smpi/simulate-computation:0=. It happens in the same location, but the deadlock is different. Now, it
  is like this (and is located only in =HPL_spreadT=):
  #+begin_example
            4
            |
  mpi_recv  |
            |
            v     mpi_recv
            0 <——————————————+
            |                |
            |                |
  mpi_send  |                |
            |                |
            v                |
           12 —————————————> 8
                 mpi_send
  #+end_example
  There is the same pattern for {1, 5, 9, 13}, {2, 6, 10, 14} and {3, 7, 11, 15}.
**** Understanding HPL code                         :SMPI:TRACING:BUG:HPL:
- In file =HPL_spreadT.c=.
- In our settings, the following =if= statement is never taken:
  #+begin_src c
  if(SIDE == HplLeft)
  #+end_src
- In the =else= part, there is a big =do while= loop. Some initializations happen before this loop.
- =npm1=: initialized to =nprow - SRCDIST - 1=, not modified during the loop.
- =ip2=: initialized to the biggest power of 2 smaller or equal to =npm1=. Divided by =2= at each step. The loop stops when
  =ip2= is =0=.
- =mask=: initialized to =ip2*2-1= (=ip2= is a single bit set to =1= followed by a bunch of =0=, =mask= is the same bit set to =1=
  followed by a bunch of =1=). At the beginning of each step, the first =1= of =mask= is flipped, so =mask= is =ip2-1= after this
  statement.
- =IPMAP=: mapping of the processes.
- =IPMAPM1=: inverse mapping (=IPMAPM1[IPMAP[i]]= is equal to =i=).
- =mydist=: initialized to =IPMAP1[myrow]=, not modified after.
- =partner=: at each step, set to =mydist^ip2=, i.e. we flip exactly one bit of =mydist=.
- We do the communications only when =mydist & mask= is =0= and when =lbuf > 0=.
  + If =mydist & ip2= is not =0=, we receive.
  + If =mydist & ip2= is =0=, we send.
- Print the content of =IPMAP=. Add the following line before the =do while=:
  #+begin_src c
  printf("IPMAP: my_rank=%d, %d %d %d %d \n", my_rank,
    local_rank_to_global(IPMAP[0], comm), local_rank_to_global(IPMAP[1], comm),
    local_rank_to_global(IPMAP[2], comm), local_rank_to_global(IPMAP[3], comm));
  #+end_src
  We get this output:
  #+begin_example
  IPMAP: my_rank=0, 0 4 12 8
  IPMAP: my_rank=12, 0 4 12 8
  IPMAP: my_rank=8, 0 4 8 12
  IPMAP: my_rank=4, 0 4 12 8
  IPMAP: my_rank=0, 0 4 12 8
  IPMAP: my_rank=4, 0 4 12 8
  IPMAP: my_rank=1, 1 5 13 9
  IPMAP: my_rank=5, 1 5 13 9
  IPMAP: my_rank=13, 1 5 13 9
  IPMAP: my_rank=9, 1 5 9 13
  IPMAP: my_rank=5, 1 5 13 9
  IPMAP: my_rank=1, 1 5 13 9
  IPMAP: my_rank=2, 2 6 14 10
  IPMAP: my_rank=3, 3 7 15 11
  IPMAP: my_rank=6, 2 6 14 10
  IPMAP: my_rank=7, 3 7 15 11
  IPMAP: my_rank=10, 2 6 10 14
  IPMAP: my_rank=11, 3 7 11 15
  IPMAP: my_rank=14, 2 6 14 10
  IPMAP: my_rank=15, 3 7 15 11
  IPMAP: my_rank=6, 2 6 14 10
  IPMAP: my_rank=2, 2 6 14 10
  IPMAP: my_rank=7, 3 7 15 11
  IPMAP: my_rank=3, 3 7 15 11
  #+end_example
  Recall that our communicators are {n, n+4, n+8, n+12} for n in {0, 1, 2, 3}. We see a pattern here: when processes
  have a local rank in {0, 1, 3}, their =IPMAP= is {0, 1, 3, 2} (local ranks), but when the local rank is 2, then =IPMAP= is
  {0, 1, 2, 3}.
- Now, let’s print the other parameters. Add the following line just after the modification of =mask= at the beginning of
  the =do while=:
  #+begin_src c
  printf("### my_rank=%d (%d) id_func=%d mask=%d ip2=%d mydist=%d", my_rank,
    my_local_rank, id_func, mask, ip2, mydist);
  #+end_src
  Here, =id_func= is a static variable initialized to =-1= and incremented at the beginning of every function call.
  Later in the code, add these:
  #+begin_src c
  printf(" partner=%d", partner);
  #+end_src
  and
  #+begin_src c
  printf(" mpi_recv(%d)\n", IPMAP[SRCDIST+partner]);
  #+end_src
  or
  #+begin_src c
  printf(" mpi_send(%d)\n", IPMAP[SRCDIST+partner]);
  #+end_src
  (depending on if we do a send or a receive).
  We have this output for {0, 4, 8, 12} (this is similar for other communicators):
  #+begin_src bash
  grep "my_rank=0 " output | grep "###"
  ### my_rank=0 (0) id_func=0 mask=1 ip2=2 mydist=0 partner=2 mpi_send(3)
  ### my_rank=0 (0) id_func=0 mask=0 ip2=1 mydist=0 partner=1 mpi_send(1)
  ### my_rank=0 (0) id_func=1 mask=1 ip2=2 mydist=0 partner=2 mpi_send(3)
  grep "my_rank=4 " output | grep "###"
  ### my_rank=4 (1) id_func=0 mask=1 ip2=2 mydist=1
  ### my_rank=4 (1) id_func=0 mask=0 ip2=1 mydist=1 partner=0 mpi_recv(0)
  ### my_rank=4 (1) id_func=1 mask=1 ip2=2 mydist=1
  ### my_rank=4 (1) id_func=1 mask=0 ip2=1 mydist=1 partner=0 mpi_recv(0)
  grep "my_rank=8 " output | grep "###"
  ### my_rank=8 (2) id_func=0 mask=1 ip2=2 mydist=2 partner=0 mpi_recv(0)
  grep "my_rank=12 " output | grep "###"
  ### my_rank=12 (3) id_func=0 mask=1 ip2=2 mydist=2 partner=0 mpi_recv(0)
  ### my_rank=12 (3) id_func=0 mask=0 ip2=1 mydist=2 partner=3 mpi_send(2)
  #+end_src
  We see that the pattern of communication looks like a binary tree. At each function call, in the first step 0 sends to
  12, in the second step 0 sends to 4 and 12 sends to 8. The problem is that all the =mpi_recv= match the =mpi_send= except
  for the node 8. This node calls =mpi_recv= with node 0 for the source, but we would expect it to have 12 for source.
  The same pattern is observed for other communicators.
- We saw that the nodes with local rank 2 call =MPI_Recv= with an unexpected source. These nodes also have a different
  =IPMAP=. Hypothesis: these different =IPMAP= are a bug.
- Doing the same experiment without =SMPI_SHARED_{MALLOC,FREE}= (the case where we do not have a deadlock). Here, we
  observe that the values of =IPMAP= are the same in all processes. Also, there is a matching =MPI_Recv= for every =MPI_Send=,
  as expected.
- Thus, to fix the deadlock, we should search where =IPMAP= is defined.
**** Seminar                                                     :MEETING:
- Taking advantage of application structure for visual performance analysis
- Lucas Mello Schnorr
- Context: two models. Explicit programming (e.g. MPI) or task-based programming (e.g. Cilk).
- In task-based programming, no clear phases (contrarily to things like MPI, where we have communication phases and
  computation phases). Thus, hard to understand the performances when visualizing a trace.
- The scheduler has to assign tasks, anticipate the critical path and minimize data movements. The difficulty is that it
  does not know the whole DAG at the beginning.
- Workflow based on several tools: =pj_dump=, =R=, =tidyverse=, =ggplot2=, =plotly=. Everything can be done in org-mode. Agile
  workflow, fail fast if the idea is not working, easily share experiments with colleagues.
*** 2017-03-17 Friday
**** Let’s look at =IPMAP=                          :SMPI:C:TRACING:BUG:HPL:
- =IPMAP= is given as an argument to =HPL_spreadT=.
- The function =HPL_spreadT= is usedd in =HPL_pdlaswp01T= and =HPL_equil=.
- In our settings, all processes begin by a call to =HPL_pdlaswp01T=. Then, all processes with local ranks =0= and =1= do a
  call to =HPL_equil= (local ranks =2= and =3= are already deadlocked). Values of =IPMAP= are the same between the two different
  calls.
  We thus have to look at =HPL_pdlaswp01T=.
- =IPMAP= is defined in this function with other variables. They are all a contiguous block in =PANEL->IWORK=:
  #+begin_src c
  iflag  = PANEL->IWORK;
  // [...]
  k = (int)((unsigned int)(jb) << 1);  ipl = iflag + 1; ipID = ipl + 1;
  ipA     = ipID + ((unsigned int)(k) << 1); lindxA = ipA + 1;
  lindxAU = lindxA + k; iplen = lindxAU + k; ipmap = iplen + nprow + 1;
  ipmapm1 = ipmap + nprow; permU = ipmapm1 + nprow; iwork = permU + jb;
  #+end_src
- =PANEL->IWORK= is allocated in =HPL_pdpanel_init= with a simple =malloc=. So the bugs does not come from here.
- The content of =IPMAP= is defined in the function =HPL_plindx10=.
- Function =HPL_plindx10= firstly compute the content of array =IPLEN=, then call function =HPL_logsort= to compute =IPMAP= (the
  content of =IPMAP= depends on the content of =IPLEN=).
- Printing the content of =IPLEN= just after its initialization.
  Add this code just before the call to =HPL_logsort=:
  #+begin_src c
  int my_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
  printf(">> my_rank=%d, icurrow=%d, IPLEN =", my_rank, icurrow);
  for(i = 0; i <= nprow; i++) {
       printf(" %d", IPLEN[i]);
  }
  printf("\n");
  #+end_src
  Here are the contents of =IPLEN= for ranks {0, 4, 8, 12}.
  + With =SMPI_SHARED_{MALLOC,FREE}=:
    | Rank | IPLEN[0] | IPLEN[1] | IPLEN[2] | IPLEN[3] | IPLEN[4] |
    |------+----------+----------+----------+----------+----------|
    |    0 |        0 |      103 |       14 |        1 |        2 |
    |    4 |        0 |      102 |       15 |        1 |        2 |
    |    8 |        0 |      102 |       14 |        2 |        2 |
    |   12 |        0 |      102 |       14 |        1 |        3 |
  + Without =SMPI_SHARED_{MALLOC,FREE}=:
    | Rank | IPLEN[0] | IPLEN[1] | IPLEN[2] | IPLEN[3] | IPLEN[4] |
    |------+----------+----------+----------+----------+----------|
    |    0 |        0 |       31 |       24 |       26 |       39 |
    |    4 |        0 |       31 |       24 |       26 |       39 |
    |    8 |        0 |       31 |       24 |       26 |       39 |
    |   12 |        0 |       31 |       24 |       26 |       39 |

  We can note two things. Firstly, without =SMPI_SHARED_{MALLOC,FREE}=, all processes have an =IPLEN= with the same
  content. This is not the case with =SMPI_SHARED_{MALLOC,FREE}=. Furthermore, values in =IPLEN= are closer in the
  =malloc/free= case.
  Thus, the issue is very likely to come from =IPLEN=.
**** Let’s look at =IPLEN= and =IPID=                   :SMPI:TRACING:BUG:HPL:
- The content of =IPLEN= depends on the content of =IPID=.
- Add a =printf= to get its content. Every element it contains is present exactly twice in the array.
- With =SHARED_{MALLOC,FREE}=,
  + =IPID= has a size of 300 for local rank =0=, 302 for the others.
  + =IPID= of local rank =1= is equal to =IPID= of local rank =0= plus twice the element =120=.
  + =IPID= of local rank =2= is equal to =IPID= of local rank =0= plus twice the element =240=.
  + =IPID= of local rank =3= is equal to =IPID= of local rank =0= plus twice the element =360=.
- Without =SHARED_{MALLOC,FREE}=,
  + =IPID= has a size of 478 for all ranks.
  + All =IPID= are equal.
- =IPID= is computed in function =HPL_pipid=.
- The content of =IPID= depends on the content of the array =PANEL->DPIV=. This array is made of =120= elements. These
  elements are of type =double=. The function cast them to =int= and do some comparisons using them, which is strange.
- Add a =printf= to get its content.
- With =SHARED_{MALLOC,FREE}=,
  + The =DPIV= of the processes having the same local rank are equal.
  + The 30 first elements of the arrays =DPIV= of the processes of a same communicator are equal. The following elements
    are different.
  + For the processes of local rank =0=, these following elements are 30, 31, 32,..., 119. In other words, for =i > 29=, we
    have =DPIV[i]= equal to =i=.
  + For the processes of local rank =1=, these elements are all equal to 120. For local rank =2=, they are equal to 240. For
    local rank =3=, they are equal to =360.
- Without =SHARED_{MALLOC,FREE}=,
  + All the =DPIV= of all processes are equal.
  + All its elements are present exactly once, except =4143= which is present twice.
– Thus, it seems that the issue come from =PANEL->DPIV=.
**** Summing up                                             :SMPI:BUG:HPL:
- The values of =IPMAP= depends on the values of =IPLEN=.
- The values of =IPLEN= depends on the values of =IPID=.
- The values of =IPID= depends on the values of =PANEL->DPIV=.
- For all these arrays, we can observe some strange things in the case =SMPI_SHARED_{MALLOC,FREE}= (comparing to the case
  =malloc/free=):
  + The content of the arrays is not the same for different ranks.
  + The content itself looks kind of strange (e.g. =DPIV= has a lot of identical values).
**** So, why do we have these =DPIV=?                         :SMPI:BUG:HPL:
- The content of =DPIV= is defined at the end of function =HPL_pdmxswp=, by the line:
  #+begin_src c
  (PANEL->DPIV)[JJ] = WORK[2];
  #+end_src
  With some =printf=, we see that =DPIV= is filled in order. The values are the same that the ones already observed in =DPIV=.
*** 2017-03-20 Monday
**** Write a small Python script to monitor memory usage          :PYTHON:
- Based on command =smemstat=.
- Run the command every second in quiet mode with json output. Parse the json file and output the information on screen,
  nicely formated.
- Future work:
  + Different sampling rate passed as a command line argument.
  + Export in CSV. This would allow to plot memory consumption over time.
**** Failed tentatives for =DPIV=                             :SMPI:BUG:HPL:
- Tried to hard-code the values of =DPIV= with something like that:
  #+begin_src c
  (PANEL->DPIV)[JJ] = 42;
  #+end_src
  Got a segmentation fault.
**** Discussion with Arnaud                         :SMPI:BUG:HPL:MEETING:
- Had a look at HPL code.
- Next steps to try to find the issue:
  + Try another block size for global =SMPI_SHARED_MALLOC=.
  + Retry local =SMPI_SHARED_MALLOC=.
  + Try other matrix sizes, other process grids.
  + In =HPL_pdmxswp=, print the values of =WORK[{0,1,2,3}]= before and after the execution.
**** Looking at =WORK[{0,1,2,3}]=                           :SMPI:C:BUG:HPL:
- Meaning of the first values of this array:
  + =WORK[0]= : local maximum absolute value scalar,
  + =WORK[1]= : corresponding local  row index,
  + =WORK[2]= : corresponding global row index,
  + =WORK[3]= : coordinate of process owning this max.
- Just before the call to =HPL_pdxswp=, these values are computed locally. Then, =HPL_pdxswp= does some computations to get
  the global values.
- Adding some =printf=.
- Without =SHARED_{MALLOC,FREE}=, the absolute value of =WORK[0]= increases at each call and quickly becomes very large. It
  reaches =3.8e+302=, then it is =NaN=. This happens regardless of if we replace BLAS operations by =smpi_usleep=.
- With =SHARED_{MALLOC,FREE}=, the absolute value of =WORK[0]= is relatively small.
- If we replace the value of =WORK[0]= by a (small) constant, the simulations terminates without deadlock.
- Recall that we run the simulation with N=40000 and P=Q=4.
- The simulation takes 197 seconds, where 170 seconds are actual computations of the application (thus, there is still
  room for optimization).
- The estimated performances are 27.5 Gflops. This is a bit higher than what we had before with the matrix of
  size 20000. We need to check if this difference is due to the higher matrix size (expected and ok) or our dirty hack
  (not ok).
*** 2017-03-21 Tuesday
**** Checking the consistency of =IPMAP=       :SMPI:PYTHON:TRACING:BUG:HPL:
- Before the modification on =WORK[0]=, the =IPMAP= were not consistent on the different processes of a same communicator
  (see the entry of =16/03/2017=).
- Let’s check if this issue is fixed.
- Add the following in =HPL_spreadT.c=:
  #+begin_src c
  int my_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
  printf("my_rank=%d IPMAP=%d,%d,%d,%d\n", my_rank, IPMAP[0], IPMAP[1], IPMAP[2], IPMAP[3]);
  #+end_src
- Run HPL with =stdout= redirected to =/tmp/output=.
- Check that at each step, the values of =IPMAP= are the same for the processes of a same communicator. Recall that the
  communicators are ={0,4,8,12}=, ={1,5,9,13}=, ={2,6,10,14}= and ={3,7,11,15}=.
#+begin_src python :results output :exports both
import re
reg = re.compile('my_rank=([0-9]+) IPMAP=(.+)')

def process(in_file):
    results = {n: [] for n in range(16)}
    with open(in_file, 'r') as in_f:
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                n = int(match.group(1))
                ipmap = match.group(2)
                results[n].append(ipmap)
    for comm in range(4):
        print('Number of entries for communicator %d: %d' % (comm, len(results[comm])))
        for rank in range(1, 4):
            assert results[comm] == results[comm+4*rank]
    print('OK')
process('/tmp/output')
#+end_src

 #+RESULTS:
 : Number of entries for communicator 0: 913
 : Number of entries for communicator 1: 904
 : Number of entries for communicator 2: 907
 : Number of entries for communicator 3: 910
 : OK
- We see here that the values of =IPMAP= are consistent.
**** Comparison with previous code                              :SMPI:HPL:
- Let’s compare with the previous version of the code (without the modification on =WORK[0]=). We use N=20000, P=Q=4.
  | Code   | Virtual time |    Gflops | Total simulation time | Time for application computations |
  |--------+--------------+-----------+-----------------------+-----------------------------------|
  | *Before* |       222.27 | 2.400e+01 |               19.2529 |                           10.0526 |
  | *After*  |       258.28 | 2.065e+01 |               48.2851 |                           41.7249 |
- We find that both the virtual time and the real time are longer, due to an higher ammount of time spent in the
  application.
- Do not forget to remove the option =--cfg=smpi/simulate-computation:0= when testing for things like that. At first, I
  did not removed it. The real time was higher but the virtual time was unchanged.
- It seems that the modification of =WORK[0]= has led to a modification of the behavior of the application, which yields
  significant differences in terms of performances.
**** Having a look at what takes time :SMPI:PYTHON:R:EXPERIMENTS:PERFORMANCE:HPL:
- Using the settings N=20000, P=Q=4. Recall that with these settings, the simulation time was nearly 52 seconds.
- Simulation time drops to 30 seconds if we disable the calls to =HPL_dgemv= (this was not the case before, according to
  experimentations of =08/03/2017=).
- There was no deadlock with N=20000, so we can compare the cases with and without the modification of =WORK[0]=.
- Modify the definition of =HPL_dgemv= in =HPL_blas.h= for both cases:
  #+begin_src c
  #define    HPL_dgemv(Order, TransA, M, N, alpha, A, lda, X, incX, beta, Y, incY) ({\
      int my_rank, buff=0;\
      MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
      struct timeval before = {};\
      struct timeval after = {};\
      gettimeofday(&before, NULL);\
      cblas_dgemv(Order, TransA, M, N, alpha, A, lda, X, incX, beta, Y, incY);\
      gettimeofday(&after, NULL);\
      double time_before = (double)(before.tv_sec) + (double)(before.tv_usec)*1e-6;\
      double time_after = (double)(after.tv_sec) + (double)(after.tv_usec)*1e-6;\
      double real_time = time_after-time_before;\
      printf("file=%s line=%d rank=%d m=%d n=%d lead_A=%d inc_X=%d inc_Y=%d real_time=%f\n", __FILE__, __LINE__, my_rank, M, N, lda, incX, incY, real_time);\
  })
  #+end_src
- Run HPL for both cases, with =stdout= redirected to some file (=/tmp/output_before= when =WORK[0]= is unmodified,
  =/tmp/output_after= when it is modified).
- Process the outputs:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) lead_A=([0-9]+) inc_X=([0-9]+) inc_Y=([0-9]+) real_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'm', 'n', 'lead_A', 'inc_X', 'inc_Y', 'real_time'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 10))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output_before', '/tmp/parameters_before.csv')
process('/tmp/output_after', '/tmp/parameters_after.csv')
#+end_src

- Analysis with R:
#+begin_src R :results output :session *R* :exports both
parameters_before <- read.csv("/tmp/parameters_before.csv");
parameters_after <- read.csv("/tmp/parameters_after.csv");
head(parameters_before)
head(parameters_after)
#+end_src

#+RESULTS:
#+begin_example
                               file line rank    m n lead_A inc_X inc_Y
1 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    4 5040 1   5040   120     1
2 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207   12 4920 1   4920   120     1
3 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    0 5039 1   5040   120     1
4 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    8 5000 1   5000   120     1
5 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207   12 4920 1   4920   120     1
6 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    4 5040 1   5040   120     1
  real_time
1  0.000034
2  0.000034
3  0.000030
4  0.000156
5  0.000043
6  0.000031
                               file line rank    m n lead_A inc_X inc_Y
1 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    4 5040 1   5040   120     1
2 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207   12 4920 1   4920   120     1
3 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    0 5039 1   5040   120     1
4 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    8 5000 1   5000   120     1
5 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207   12 4920 1   4920   120     1
6 /hpl-2.2/src/pfact/hpl_pdpanllt.c  207    8 5000 1   5000   120     1
  real_time
1  0.000026
2  0.000030
3  0.000030
4  0.000123
5  0.000030
6  0.000028
#+end_example

#+begin_src R :results output :session *R* :exports both
sum(parameters_before$real_time)
sum(parameters_after$real_time)
#+end_src

#+RESULTS:
: [1] 0.127207
: [1] 2.61086

- There is a clear difference between the two cases. When =WORK[0]= is modified, the time spent in the function =HPL_dgemv=
  is 20 times higher. However, this makes a difference of about 2.5 seconds, whereas a difference of 20 seconds was
  observed when removing =HPL_dgemv=.
- Therefore, it seems that removing the calls to =HPL_dgemv= triggers a modification of the behavior of the application,
  resulting in a lower time, but this is not this function itself which takes time.
- Note that this was not the case for functions =HPL_dgemm= and =HPL_dtrsm=: this was the calls to these functions which
  took time, not a consequence of the calls (just tested: taking the sum of all the times gives a total of about 75
  seconds for =HPL_dtrsm= and about 2896 seconds for =HPL_dgemm=).
- In experimentation of =08/03/2017=, removing =HPL_dgemv= only resulted in a drop of 1 second for the execution time.
- Thus, it seems that modifying =WORK[0]= has increased the execution time, which is cancelled if we then remove
  =HPL_dgemv=. Therefore, we should not treat this function as =HPL_dgemm= and =HPL_dtrsm= (replacing it by a =smpi_usleep=), we
  should simply remove it.
- If we remove it, we get a virtual time of 226 seconds, i.e. 23.6 Gflops. This is much closer to what we used to
  have. Now the simulation time is 26 seconds, this is worse than what we used to have, but still better than what we
  had after the modification of =WORK[0]=.
*** 2017-03-22 Wednesday
**** Better usability of HPL                                  :SMPI:C:HPL:
- Before, HPL code had to be changed by hand to enable or disable SMPI optimizations (=SHARED_{MALLOC,FREE}= and
  =smpi_usleep=) and to enable or disable the tracing of BLAS function calls.
- Now, thanks to some preprocessor macros, these different settings can be configured on the command line when
  compiling:
  #+begin_src sh
  # Compile vanilla HPL for SMPI
  make arch=SMPI
  # Compile HPL for SMPI with the tracing of BLAS function calls
  make SMPI_OPTS=-DSMPI_MEASURE
  # Compile HPL for SMPI with the SMPI optimizations (shared malloc/free, smpi_usleep)
  make SMPI_OPTS=-DSMPI_OPTIMIZATION
  #+end_src
- Next step: automation of the computation of the linear regression coefficients, to also pass these coefficients as
  preprocessor variables.
**** Script to parse the output file and do the linear regression :SMPI:PYTHON:EXPERIMENTS:TRACING:HPL:
- Everything is done in Python (linear regression included) to simplify the procedure for the user.
- Given an output file =/tmp/output= as produced by a call to HPL (compiled with =-DSMPI_MEASURE= option), call the script:
  #+begin_src sh :results output :exports both
  python3 ../scripts/linear_regression.py /tmp/output
  #+end_src

  #+RESULTS:
  : -DSMPI_DGEMM_COEFF=1.097757e-09 -DSMPI_DTRSM_COEFF=9.134754e-08
  It outputs the list of the coefficients found by the linear regressions for the relevant BLAS functions. This list
  should then be passed to the variable =SMPI_OPTS= when compiling with =-DSMPI_OPTIMIZATION=.
**** Discussion with Arnaud         :SMPI:ORGMODE:PERFORMANCE:HPL:MEETING:
- Possible trip to Bordeaux on the week of [2017-04-10 Mon]-[2017-04-14 Fri]. The goal is to discuss with contributors
  of Simgrid.
- Found very strange the different time we get when we modify =WORK[0]=, especially because it is computation time (would
  be more understandable if it was communication time, since the communication patterns for the pivot exchange are very
  likely to be impacted). Should do a profiling.
- Some tips regarding org-mode (tags).
**** DONE Profile HPL
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-03-27 Mon 17:56]
- State "TODO"       from              [2017-03-22 Wed 18:08]
:END:
- Use Valgrind with [[http://valgrind.org/docs/manual/cl-manual.html][Callgrind]] and [[http://kcachegrind.sourceforge.net/html/Shot3.html][Kcachegrind]] or [[https://sourceware.org/binutils/docs/gprof/][Gprof]].
- Do the profiling on unmodified HPL and modified HPL, to see if there is any obvious difference.
*** 2017-03-23 Thursday
**** Profiling vanilla HPL :EXPERIMENTS:PERFORMANCE:PROFILING:VALGRIND:SMPI:HPL:
- Profiling with Valgrind of vanilla HPL (no time measurements nor SMPI optimizations). Add the option =-g= in the Makefile.
- HPL commit: =4494976bc0dd67e04e54abec2520fd468792527a=.
- Settings: N=5000, P=Q=4.
- Compile with the command:
  #+begin_src sh
  make -j 4 arch=SMPI
  #+end_src
- Run with the command:
  #+begin_src sh
  smpirun -wrapper "valgrind --tool=callgrind" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ./hostfile_64.txt -platform
  ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- At first, the package =libatlas3-base= was used for the BLAS functions. The name of these functions were not shown in
  Kcachegrind.
  + [[file:callgrind/callgrind.out.19313][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.19313.png]]
- Then, removed this package and installed =libatlas-cpp-0.6-2-dbg=.
  + [[file:callgrind/callgrind.out.26943][Output file]]
  + Visualization
    [[file:callgrind/callgrind.26943.png]]
- So now we have the names of the BLAS functions, but the layout is very different.
- Also, the executions with this library take more time, especially with Vallgrind. It also impacts the virtual time and
  the Gflops.
- What we observe with this new library seems to be consistent with what we observed previously: =dgemm= is the most time
  consumming function (by far), =dtrsm= comes after. So maybe this library is good enough to understand what happens, and
  then we could switch back to the previous library to have good performances.
**** Profiling modified HPL :EXPERIMENTS:PERFORMANCE:PROFILING:VALGRIND:SMPI:HPL:
- Profiling with Valgrind of modified HPL. Add the option =-g= in the Makefile.
- HPL commit: =4494976bc0dd67e04e54abec2520fd468792527a=. Then for each case, a small piece of the code has been modified.
- Settings: N=5000, P=Q=4.
- Compile with the command:
  #+begin_src sh
  make SMPI_OPTS=-DSMPI_OPTIMIZATION -j 4 arch=SMPI
  #+end_src
- Run with the command:
  #+begin_src sh
  smpirun -wrapper "valgrind --tool=callgrind" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ./hostfile_64.txt -platform
  ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Using the library from =libatlas-cpp-0.6-2-dbg=.
- First experiment, the call to =HPL_dgemv= is a no-op and =WORK[0]= is set to a constant.
  + [[file:callgrind/callgrind.out.5388][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.5388.png]]
- Second experiment, the call to =HPL_dgemv= is aliased to =cblas_dgemv= and =WORK[0]= is set to a constant.
  + [[file:callgrind/callgrind.out.10043][Output file]]
  + Visualization: similar.
- Third experiment, the call to =HPL_dgemv= is aliased to =cblas_dgemv= and =WORK[0]= is not modified.
  + [[file:callgrind/callgrind.out.12915][Output file]]
  + Visualization: similar.
- It is clear that we can shrink even further the simulation by removing the code that initialize the matrices (this is
  the code that calls the function =HPL_rand=).
- There is no explanation for the differences observed with =HPL_dgemv= and =WORK[0]=, the figures look similar. However the
  differences observed between the three cases are quite small (in terms of execution time or Gflops).
**** Comparison of the code                                     :SMPI:HPL:
- Let’s compare again the different versions of the code, this time with the new CBLAS library (package
  =libatlas-cpp-0.6-2-dbg=). We use N=20000, P=Q=4.
  | Code                           | Virtual time |    Gflops | Total simulation time | Time for application computations |
  |--------------------------------+--------------+-----------+-----------------------+-----------------------------------|
  | *WORK[0] unmodified, real dgemv* |       223.81 | 2.383e+01 |               15.5049 |                            9.5045 |
  | *WORK[0] modified, real dgemv*   |       223.74 | 2.384e+01 |               25.9935 |                           20.0480 |
  | *WORK[0] modified, no-op dgemv*  |       226.28 | 2.357e+01 |               26.3907 |                           20.3201 |
  Remark: for the first version of the code, the experiment had to be ran twice, since the first run ended in a deadlock.
- The two first rows correspond to the two rows of the table of [2017-03-21 Tue].
- There is no significant difference for the virtual time and the Gflops.
- There is a significant difference for the total simulation time and the time spent for aapplication computations, but
  it is less important than what we previously observed.
- It is strange that this difference in the computation time does not appear in the virtual time. Note that the option
  =--cfg=smpi/simulate-computation:0= was not used, so it does not come from here.
**** Seminar                                                     :MEETING:
- Decaf: Decoupled Dataflows for In Situ High-Performance Workflows
- Mathieu Dreher
- They do some physics experiment (with a particle collider). Then, they analyze the results and build a model. Thus,
  the whole process has three major steps.
- In current systems, the bottleneck is the I/O. It will be even worse for future systems (computation speed will be
  increased, not I/O speed). This is why we should have in-situ workflows (less data to move).
- In the “classical workflow”, we compute all the iterations, then we analyze them.
- In the “in situ workflow”, two things are possible. Time partitionning: we compute one iteration, then analyze it,
  then go back to the computation. Space partitioning: the analysis is done in parallel on other nodes.
**** Profiling modified HPL, bigger matrices :EXPERIMENTS:PERFORMANCE:PROFILING:VALGRIND:SMPI:HPL:
- Profiling with Valgrind of modified HPL. Add the option =-g= in the Makefile.
- HPL commit: =4494976bc0dd67e04e54abec2520fd468792527a=. Then for each case, a small piece of the code has been modified.
- Settings: N=20000, P=Q=4.
- Compile with the command:
  #+begin_src sh
  make SMPI_OPTS=-DSMPI_OPTIMIZATION -j 4 arch=SMPI
  #+end_src
- Run with the command:
  #+begin_src sh
  smpirun -wrapper "valgrind --tool=callgrind" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ./hostfile_64.txt -platform
  ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Using the library from =libatlas-cpp-0.6-2-dbg=.
- First experiment, the call to =HPL_dgemv= is a no-op and =WORK[0]= is set to a constant.
  + [[file:callgrind/callgrind.out.6159][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.6159.png]]
- Second experiment, the call to =HPL_dgemv= is aliased to =cblas_dgemv= and =WORK[0]= is set to a constant.
  + [[file:callgrind/callgrind.out.2590][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.2590.png]]
- Third experiment, the call to =HPL_dgemv= is aliased to =cblas_dgemv= and =WORK[0]= is not modified.
  + [[file:callgrind/callgrind.out.31804][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.31804.png]]
- The three figures have roughly the same pattern.
- However, some numbers of the two first figures are twice as large as the corresponding numbers of the third
  figure. For instance, the biggest =HPL_rand= has 70803540000 in the third figure and 141607080000 in the two first ones.
- The reason for that is that, in the two first cases, =HPL_pdmatgen= is called 32 times, whereas in the last case it is
  called only 16 times. In our settings, we would expect this function to be called 16 times, since we simulate 16 processes.
- This is very strange that =WORK[0]= has an impact on the behavior of matrices generation.
*** 2017-03-24 Friday
**** Why =WORK[0]= impacts the number of calls to =HPL_pdmatgen=    :SMPI:HPL:
- Everything happens in the file =HPL_pdtest.c=. This is in relation with the error code issue discussed on [2017-03-14 Tue].
- When we use SMPI optimizations (=smpi_usleep= and =SMPI_SHARED_MALLOC=) without modifying =WORK[0]=, HPL detects an error in
  the data of the matrices and returns an error code. If we also fix =WORK[0]= to some constant, HPL does not detect this
  error.
- After doing the factorization, if no error code has been returned, HPL does some additional tests on the values of the
  matrix. These tests are quite long, and imply to re-generate the matrix, by calling =HPL_pdmatgen=.
- This explains why =WORK[0]= has an impact on the simulation time and the number of time =HPL_pdmatgen= is called.
- This does not explain the difference in terms of virtual time observed on [2017-03-21 Tue], since this is only the
  time needed for the factorization and not the time for the initialization and the checks.
- This difference of virtual time was not re-observed on [2017-03-23 Thu]. Note that another BLAS library was used.
**** Comparison of the code                                     :SMPI:HPL:
- Let’s compare again the different versions of the code, with the “old” version of the CBLAS library (package
  =libatlas3-base=). We use N=20000, P=Q=4.
- This is the same experiment than [2017-03-23 Thu], except for the BLAS library.
- The two first rows correspond to the two rows of the table of [2017-03-21 Tue].
  | Code                           | Virtual time |    Gflops | Total simulation time | Time for application computations |
  |--------------------------------+--------------+-----------+-----------------------+-----------------------------------|
  | *WORK[0] unmodified, real dgemv* |       223.68 | 2.385e+01 |               15.8909 |                            9.5658 |
  | *WORK[0] modified, real dgemv*   |       257.79 | 2.069e+01 |               47.9488 |                           41.5125 |
  | *WORK[0] modified, no-op dgemv*  |       225.91 | 2.361e+01 |               26.2768 |                           20.1776 |
- The experiment of [2017-03-21 Tue] is replicated: the two first rows look similar.
- There is still the big gap in terms of both simulation time and virtual time. The former could be explained by the
  checks done at the end of HPL, but not the later (see previous entry of the journal).
- Interestingly, the first and the last rows look very similar to the first and the last row of the [2017-03-23 Thu]
  experiment, although the BLAS library has changed. The middle row however is very different.
- These gaps are not replicated when using Valgrind. For all simulations, we have a virtual time of about 262 to 263
  seconds, which is about 2.03e+01 Gflops.
**** Removing initialization and checks   :SMPI:PERFORMANCE:PROFILING:HPL:
- The previous experiments demonstrated that the initialization (done after the factorization) and the checks (done
  after) take a significant amount of time. They do not account for the estimation of the Gflops, so we can safely
  remove them.
- Quick experiment, with HPL at commit =cb54a92b8304e0cd2f1728b887cc4cc615334c2d=, N=20000 and P=Q=4, using library from
  package =libatlas3-base=.
- We get a virtual time of 227.35, which is 2.346e+01 Gflops. It confirms that the initialization and the checks are not
  accounted in this measure.
- The simulation time is now 9.63 seconds, with 3.53 seconds spent for actual computations of the application.
- We see here that the simulation is already well optimized, there is not much room for additional gains.
- Profiling with Valgrind:
  + [[file:callgrind/callgrind.out.5744][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.5744.png]]
- We see here that a large part of the time is spent in functions called by Simgrid (e.g. =memcpy=).
**** Work on the experiment script                                :PYTHON:
- Add three features:
  + “Dump simulation and application times in the CSV.”
  + “Dump physical and virtual memory in the CSV.”
  + “Experiments with random sizes and number of processors.”
- Example of usage:
  #+begin_src sh
  ./run_measures.py --global_csv /tmp/bla.csv --nb_runs 10 --size 1000:2000,4000:5000,20000:21000 --nb_proc 1:4,8,16,32,64
  --fat_tree "2;8,8;1,1:4;1,1" --experiment HPL
  #+end_src
  This will run 10 times, in a random order, all combinations of the parameters:
  + Matrix size in [1000,2000]\cup[4000,5000]\cup[20000,21000]
  + Number of processes in {1,2,3,4,8,16,32,64}
  + Fat-trees =2;8,8;1,1;1,1= and =2;8,8;1,2;1,1= and =2;8,8;1,3;1,1= and =2;8,8;1,4;1,1=.
  The results are dumped in a CSV file. For each experiment, we store all the parameters (topology, size, number of
  processes) as well as the interesting metrics (virtual time, Gflops, simulation time, time spent in application, peak
  physical and virtual memory used).
*** 2017-03-25 Saturday
**** Time and memory efficiency of HPL simulation :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- HPL commit: =cb54a92b8304e0cd2f1728b887cc4cc615334c2d=
- Script commit: =8af35470776a0b6f2041cf8e0121739f94fdc34d=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl2.csv --nb_runs 3 --size 100,5000,10000,15000,20000,25000,30000,35000,40000
  --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Plots:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  do_plot <- function(my_plot, title) {
      return(my_plot +
          stat_summary(geom="line", fun.y=mean)+
          stat_summary(fun.data = mean_sdl)+
          ggtitle(title)
      )
  }
  results <- read.csv('hpl_analysis/hpl.csv')
  head(results)
  #+end_src

  #+RESULTS:
  #+begin_example
         topology nb_roots nb_proc  size    time Gflops simulation_time
  1 2;8,8;1,8;1,1        8      48 40000  593.10 71.940        60.75480
  2 2;8,8;1,8;1,1        8      40 20000  144.88 36.820        24.53460
  3 2;8,8;1,8;1,1        8       8 30000 1290.99 13.940        13.39820
  4 2;8,8;1,8;1,1        8      56 10000   37.93 17.580        12.92780
  5 2;8,8;1,8;1,1        8       1 30000 9609.94  1.873         3.67895
  6 2;8,8;1,8;1,1        8      64 10000   27.20 24.510         9.96141
    application_time       uss         rss
  1         14.47840 701091840 13509701632
  2          6.44959 327905280  3533713408
  3          6.14242 217612288  7422472192
  4          2.55716 211193856  1016156160
  5          3.58312   5619712  7209476096
  6          2.10660 179879936   984698880
  #+end_example

  #+begin_src R :file hpl_analysis/1.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(results, aes(x=size, y=simulation_time, group=nb_proc, color=nb_proc)),
     "Simulation time vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/1.png]]

  #+begin_src R :file hpl_analysis/2.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(results, aes(x=nb_proc, y=simulation_time, group=size, color=size)),
      "Simulation time vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/2.png]]

  #+begin_src R :file hpl_analysis/3.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(results, aes(x=size, y=uss, group=nb_proc, color=nb_proc)),
      "Physical memory consumption vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/3.png]]

  #+begin_src R :file hpl_analysis/4.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(results, aes(x=nb_proc, y=uss, group=size, color=size)),
     "Physical memory consumption vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/4.png]]

- We see here that despite all the optimizations:
  + The simulation time seems to be quadratic in the matrix size.
  + The simulation time seems to be (roughly) linear in the number of processes.
  + The memory consumption seems to be linear in the matrix size.
  + The memory consumption seems to be (roughly) linear in the number of processes.
- There are some irregularities regarding the time and memory vs the number of processes. An hypothesis is that it is
  due to different virtual topologies. In this experiment, the number of processes are multiple of 8. So, some of these
  numbers are square numbers, others are not. It seems that we achieve the best performances when the number of
  processes is a square.
  To generate P and Q, the sizes of the process grid, we try to find two divisors of the number of processes that are
  reasonably close (if possible). Thus, when the number of processes is a square, we have P=Q.
*** 2017-03-27 Monday
**** DONE Remaining work on HPL (following discussion with Arnaud) [3/3] :SMPI:HPL:MEETING:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-03-27 Mon 17:01]
- State "TODO"       from "TODO"       [2017-03-27 Mon 13:27]
- State "TODO"       from "TODO"       [2017-03-27 Mon 12:33]
- State "TODO"       from "TODO"       [2017-03-27 Mon 12:33]
- State "TODO"       from              [2017-03-27 Mon 11:26]
:END:
- [X] Do not look further regarding the =WORK[0]= anomaly.
- [X] Do careful experiments to validate the optimizations.
- [X] Currently, the simulation will not scale in memory. Track the sizes of the =malloc= in =HPL_panel_init=.
**** More detailed analysis of =malloc=          :R:TRACING:PERFORMANCE:HPL:
- We saw that the memory consumption is still too high, we need to reduce it.
- Let’s take back the results from [2017-03-17 Fri]. The corresponding CSV file has been copied in repository =hpl_malloc=.
- Recall that this is a trace of all the =malloc=, with N=20000 and P=Q=4.
- We will focus on the file =HPL_pdpanel_init.c= since we suppose that these are the biggest allocations (after the
  allocation of the matrix).
  #+begin_src R :results output :session *R* :exports both
  results <- read.csv("hpl_malloc/malloc.csv");
  results <- results[results$file == "/hpl-2.2/src/panel/hpl_pdpanel_init.c",]
  results$idx <- 0:(length(results$size)-1)
  head(results)
  #+end_src

  #+RESULTS:
  :                                      file line rank    size idx
  : 99  /hpl-2.2/src/panel/hpl_pdpanel_init.c  245    0 4839432   0
  : 100 /hpl-2.2/src/panel/hpl_pdpanel_init.c  339    0    5344   1
  : 101 /hpl-2.2/src/panel/hpl_pdpanel_init.c  245    0 4839432   2
  : 102 /hpl-2.2/src/panel/hpl_pdpanel_init.c  339    0    5344   3
  : 106 /hpl-2.2/src/panel/hpl_pdpanel_init.c  245    2 9640392   4
  : 107 /hpl-2.2/src/panel/hpl_pdpanel_init.c  339    2    5344   5

  #+begin_src R :file hpl_malloc/1.png :results value graphics :results output :session *R* :exports both
  library(ggplot2)
  ggplot(results, aes(x=idx, y=size, color=factor(line))) + geom_point(alpha=.2) + ggtitle("Sizes of malloc in HPL_pdpanel_init (N=20000, P=Q=4)")
  #+end_src

  #+RESULTS:
  [[file:hpl_malloc/1.png]]

- Now that we have removed the matrix allocation, the panel allocation is clearly the one responsible of the high memory
  consumption. Here, for 16 processes and a matrix of size 20000, this allocation is responsible for 160MB of memory.
- The =malloc= of line 245 is the one that is a concern. It is made for the =WORK= attribute.
- The =malloc= of line 339 is not a concern. It is made for the =IWORK= attribute.
- It is strange that all these allocations are made. Why not allocating the panel once, and then reusing it until the
  end?
- It may be difficult to split the panel in two parts (one =SHARED_MALLOC= and one classical =malloc=). In
  =HPL_pdpanel_init.c=, we can find this comment:
  #+begin_example
  * L1:    JB x JB in all processes
  * DPIV:  JB      in all processes
  * DINFO: 1       in all processes
  * We make sure that those three arrays are contiguous in memory for the
  * later panel broadcast.  We  also  choose  to put this amount of space
  * right  after  L2 (when it exist) so that one can receive a contiguous
  * buffer.
  #+end_example
**** Validation of the optimizations              :SMPI:R:EXPERIMENTS:HPL:
- Let’s compare vanilla HPL with optimized HPL, to see if the simulation is still faithful.
- Results for optimized HPL are those of [2017-03-25 Sat].
- Results for vanilla HPL have been freshly generated:
  + Using HPL commit =6cc643a5c2a123fa549d02a764bea408b5ad6114=
  + Using script commit =7a9e467f9446c65a9dbc2a76c4dab7a3d8209148=
  + Command:
    #+begin_src sh
    ./run_measures.py --global_csv hpl_vanilla.csv --nb_runs 1 --size 100,5000,10000,15000,20000 --nb_proc
    1,8,16,24,32,40,48,56,64 --fat_tree "2;8,8;1,8;1,1" --experiment HPL
    #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  optimized_results <- read.csv('hpl_analysis/hpl.csv')
  vanilla_results <- read.csv('hpl_analysis/hpl_vanilla.csv')
  optimized_results$hpl = 'optimized_hpl'
  vanilla_results$hpl = 'vanilla_hpl'
  results = rbind(optimized_results, vanilla_results)
  head(results)
  #+end_src

  #+RESULTS:
  #+begin_example
         topology nb_roots nb_proc  size    time Gflops simulation_time
  1 2;8,8;1,8;1,1        8      48 40000  593.10 71.940        60.75480
  2 2;8,8;1,8;1,1        8      40 20000  144.88 36.820        24.53460
  3 2;8,8;1,8;1,1        8       8 30000 1290.99 13.940        13.39820
  4 2;8,8;1,8;1,1        8      56 10000   37.93 17.580        12.92780
  5 2;8,8;1,8;1,1        8       1 30000 9609.94  1.873         3.67895
  6 2;8,8;1,8;1,1        8      64 10000   27.20 24.510         9.96141
    application_time       uss         rss           hpl
  1         14.47840 701091840 13509701632 optimized_hpl
  2          6.44959 327905280  3533713408 optimized_hpl
  3          6.14242 217612288  7422472192 optimized_hpl
  4          2.55716 211193856  1016156160 optimized_hpl
  5          3.58312   5619712  7209476096 optimized_hpl
  6          2.10660 179879936   984698880 optimized_hpl
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  plot_results <- function(nb_proc) {
      ggplot(results[results$nb_proc==nb_proc,], aes(x=size, y=Gflops, color=hpl)) +
          geom_point() + geom_line() +
          expand_limits(x=0, y=0) +
          ggtitle(paste("Gflops vs size, nb_proc = ", nb_proc))
  }
  #+end_src

  #+begin_src R :file hpl_analysis/5.png :results value graphics :results output :session *R* :exports both
  plot_results(1)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/5.png]]

  #+begin_src R :file hpl_analysis/6.png :results value graphics :results output :session *R* :exports both
  plot_results(8)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/6.png]]

  #+begin_src R :file hpl_analysis/7.png :results value graphics :results output :session *R* :exports both
  plot_results(16)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/7.png]]

  #+begin_src R :file hpl_analysis/8.png :results value graphics :results output :session *R* :exports both
  plot_results(24)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/8.png]]

  #+begin_src R :file hpl_analysis/9.png :results value graphics :results output :session *R* :exports both
  plot_results(32)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/9.png]]

  #+begin_src R :file hpl_analysis/10.png :results value graphics :results output :session *R* :exports both
  plot_results(40)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/10.png]]

  #+begin_src R :file hpl_analysis/11.png :results value graphics :results output :session *R* :exports both
  plot_results(48)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/11.png]]

  #+begin_src R :file hpl_analysis/12.png :results value graphics :results output :session *R* :exports both
  plot_results(56)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/12.png]]

  #+begin_src R :file hpl_analysis/13.png :results value graphics :results output :session *R* :exports both
  plot_results(64)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/13.png]]

- From the above plots, it seems that optimized HPL is always too optimistic in terms of performances. However, the
  difference is not so important.

  #+begin_src R :file hpl_analysis/14.png :results value graphics :results output :session *R* :exports both
  merged_results = merge(x=vanilla_results, y=optimized_results, by=c("nb_proc", "size"))
  merged_results$error = abs((merged_results$Gflops.x - merged_results$Gflops.y)/merged_results$Gflops.y)
  ggplot(merged_results, aes(x=factor(size), y=error)) +
      geom_boxplot() + geom_jitter(aes(color=nb_proc)) +
      ggtitle("Error vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/14.png]]

  #+begin_src R :file hpl_analysis/15.png :results value graphics :results output :session *R* :exports both
  ggplot(merged_results, aes(x=factor(nb_proc), y=error)) +
      geom_boxplot() + geom_jitter(aes(color=size)) +
      ggtitle("Error vs nb_proc")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/15.png]]

- We see here that the biggest errors made by the simulation are for a size of 100 and 1 process. For larger sizes and
  numbers of processes, the error never goes above 10%. In average, it is lower than 5%.

  #+begin_src R :file hpl_analysis/16.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=simulation_time, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Simulation time vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/16.png]]

  #+begin_src R :file hpl_analysis/17.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=uss, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Real memory vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/17.png]]

- There is a very important gain in terms of memory consumption and simulation time.
*** 2017-03-28 Tuesday
**** Booked the plane tickets for Bordeaux
**** Tentative of allocation hack in HPL_pdpanel_init :SMPI:C:PERFORMANCE:HPL:
- Greatly inspired from what is done for the global =SMPI_SHARED_MALLOC=.
- The idea is to reserve a large block of virtual addresses. The first part is mapped to a (short) buffer in a cyclic
  way. The second part is kept private.
- Currently some bugs (invalid writes, leading to a segmentation fault).
*** 2017-03-29 Wednesday
**** Keep trying to use some shared memory for =PANEL->WORK= :SMPI:C:PERFORMANCE:BUG:HPL:
- The invalid writes of yesterday were on accesses to =WORK= buffer. Forgot the space needed for the buffer =U= at the end
  of =WORK=. Now fixed.
- Add some =printf= to see the start and end addresses of the different buffers. Everything seems fine.
- Add a check. We fill the private zone (=DPIV= and =DINFO=) with 0. Then we fill the shared zone with garbage. Finally we
  check that the private zone is still full 0.
- Now, there is an invalid write of 4 bytes, by =HPL_plindx1=, located just after the buffer =IWORK= (the allocation of this
  buffer did not change).
- Test for N=5000, P=Q=4. Found that in file =HPL_plindx1.c=, variable =ipU= reaches 120 in the buggy case, but only 119 in
  the normal case. So it is likely that the array is not too short, but rather that this variable is incremented too much.
- In the =for= loop where this happens, =ipU= is incremented when some conditions are fulfilled. One of these conditions is
  the combination of these two =if=:
  #+begin_src c
  if( srcrow == icurrow ) {
      if( ( dstrow == icurrow ) && ( dst - ia < jb ) )
  // [...]
  #+end_src
  When =ipU= reaches 120, the illegal write is:
  #+begin_src c
  iwork[ipU] = IPLEN[il];
  #+end_src
  When this happens, the variable =dst= is 0 and thus the condition =dst-ia<jb= is met. But intuitively, this condition
  should not be met like this (=jb= is always positive).
  A bit earlier in the loop, =dst= is set with:
  #+begin_src c
  dst = IPID[i+1];
  #+end_src
  Printing this array in the buggy case and in the normal case, we see that the last element of the array is sometimes 0
  in the buggy case, but never in the normal case. Thus, it seems that there is an issue with =IPID=.
- Note that we also had issues with =IPID= when using =SHARED_MALLOC=.
**** Looking at =PANEL->DPIV= (again)                         :SMPI:BUG:HPL:
- Add a =printf= in =HPL_pipid.c= (function which compute =IPID=, using =DPIV=) to see the content of =DPIV=.
- In the buggy case, sometimes, the array =DPIV= is full of 0. It does not happen in the normal case. If we put something
  else in =DPIV= when it is allocated, then this is shown instead of the zeroes (e.g. if we put 0, 1, 2...). Thus, in
  these cases, =DPIV= is never filled after its initialization.
- Hypothesis: when the pannels are sent with MPI, the size is too short and =DPIV= is not sent.
**** Discussion with Arnaud and Augustin                         :MEETING:
- Instead of putting an empty space between the shared block and the private block (for alignment), make them really
  contiguous (and do not share the last page of the “shared” block).
**** Reimplement the shared allocation            :SMPI:C:PERFORMANCE:HPL:
- The code was a mess, let’s restart something better, using Augustin’s idea.
- The interface is as follows:
  #+begin_src c
  void *allocate_shared(int size, int start_private, int stop_private);
  #+end_src
  It allocates a contiguous block of virtual addresses of given size that all fit in a small block of physical memory,
  except for a contiguous block located between the indices start_private (included) and stop_private (excluded).
  Calling =allocate_shared(size, size, size)= is (semantically) equivalent to calling =SMPI_SHARED_MALLOC(size)=.
  Calling =allocate_shared(size, 0, size)= is (semantically) equivalent to calling =malloc(size)=.
- Similarly to =SHARED_MALLOC=, we map the shared zones by block, on a same range of addresses. The “best” block size is
  to discuss.
- Since every call to =mmap= is a syscall, we should try to not have a too low block size. Used 0x1000 at the beginning,
  the performances were terrible.
- Still for performance reasons, if the size is too low, we should simply do a malloc (and thus not have any shared zone).
- Valgrind does not report any error (it was the case with the previous implementation). There are some small memory
  leaks however.
- Performances are good. Tested with N=40000, P=Q=8. Simulation time increased from 85 seconds to 112 seconds. Memory
  consumption decreased from 675 MB to 95 MB. The virtual time and the Gflops were not impacted.
**** DONE Remaining work for shared allocation [4/4]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-04-05 Wed 17:24]
- State "TODO"       from "TODO"       [2017-03-30 Thu 09:52]
- State "TODO"       from "TODO"       [2017-03-30 Thu 09:52]
- State "TODO"       from "TODO"       [2017-03-30 Thu 09:18]
- State "TODO"       from              [2017-03-29 Wed 18:31]
:END:
- [X] Track the memory leaks (unclosed file?).
- [X] Clean the block size definition. Put it somewhere accessible by both =HPL_pdpanel_init= and =HPL_pdpanel_free=. Maybe
  use two different values for the block size and the condition to switch to a simple =malloc=.
- [X] Find the best value(s) for the block size (and maybe the =malloc= condition).
- [X] Contribute this function to Simgrid.
*** 2017-03-30 Thursday
**** Quick work on shared allocations                         :SMPI:C:HPL:
- Clean the size definitions.
  + Use a separate file that is imported in =HPL_pdpanel_init.c= and =HPL_pdpanel_free.c=.
  + Use two different sizes: the block size, and the size at which we switch for =malloc=.
- Quick look at the possibilities for the sizes
  + Some quick experiments with N=40000, P=Q=8.
  + With =BLOCK_SIZE= and =MALLOC_MAX_SIZE= equal to 0x10000:
    - Simulation time: 112 seconds
    - Physical memory: 96 MB
  + With =BLOCK_SIZE= equal to 0x10000 and =MALLOC_MAX_SIZE= equal to 0 (never do a =malloc=):
    - Simulation time: also 112 seconds
    - Physical memory: also 96 MB
  + With =BLOCK_SIZE= equal to 0x10000 and =MALLOC_MAX_SIZE= equal to 0x40000 (4 times greater):
    - Simulation time: 137 seconds
    - Physical memory: 93 MB
  + Thus, it seems that the gain of using =malloc= is not so important. Worst: it can yield a significant loss. Let’s
    remove it.
  + With =BLOCK_SIZE= equal to 0x100000 and =malloc= removed: execution cancelled, all the physical memory was used.
- Stop using =malloc=. Also move back the size definition in =HPL_pdpanel_init.c= 
- The code is simpler like this, and the =malloc= trick did not give better performances.
- Do not bother with the memory leak. It was already here before the shared allocation.
- *Warning:* calling =munmap= with a size of 0 gives a huge memory consumption. It should be called with the correct size.
**** Implement the partial =shared_malloc= in Simgrid
- Even more generic implementation than the one done in HPL. Now, we give a list of offsets of block that should be
  shared. Thus, we can have an arbitrary mix of shared zones with private zones inside an allocated block.
- Tests currently fail. To run a single test and see its output, run:
  #+begin_src sh
  ctest --verbose -R tesh-smpi-macro-shared-thread
  #+end_src
  I suspect (but did not check) that this is because we currently share only blocks aligned on the block size.
  It would be better to share blocks aligned on the page size (need to fix it). But this does not change the fact that
  some parts will not be shared. This is expected, we should modify the tests.
**** Time and memory efficiency of the partial =shared_malloc= :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- We switch back to the implementation of partial =shared_malloc= done in HPL, to measure its performances.
- Simgrid commit: =c8db21208f3436c35d3fdf5a875a0059719bff43= (the same commit that for the previous performance analysis)
- HPL commit: =7af9eb0ec54418bf1521c5eafa9acda1b150446f=
- Script commit: =7a9e467f9446c65a9dbc2a76c4dab7a3d8209148=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl_partial_shared.csv --nb_runs 1 --size 100,5000,10000,15000,20000,25000,30000,35000,40000
  --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  partial_shared_results <- read.csv('hpl_analysis/hpl_partial_shared.csv')
  optimized_results <- read.csv('hpl_analysis/hpl.csv')
  vanilla_results <- read.csv('hpl_analysis/hpl_vanilla.csv')
  partial_shared_results$hpl = 'partial_shared_hpl'
  optimized_results$hpl = 'optimized_hpl'
  vanilla_results$hpl = 'vanilla_hpl'
  results = rbind(partial_shared_results, optimized_results, vanilla_results)
  head(results)
  #+end_src

  #+RESULTS:
  #+begin_example
         topology nb_roots nb_proc  size     time    Gflops simulation_time
  1 2;8,8;1,8;1,1        8      24 25000   319.37 32.620000      25.8119000
  2 2;8,8;1,8;1,1        8      24  5000    13.03  6.399000       2.7273300
  3 2;8,8;1,8;1,1        8      24 35000   781.76 36.570000      49.3234000
  4 2;8,8;1,8;1,1        8      40   100     0.23  0.003028       0.0779319
  5 2;8,8;1,8;1,1        8       1 35000 15257.68  1.873000       5.8686300
  6 2;8,8;1,8;1,1        8      64 40000   488.99 87.260000     111.7290000
    application_time      uss         rss                hpl
  1        8.0867100 55365632  5274730496 partial_shared_hpl
  2        0.6131710 14643200   257220608 partial_shared_hpl
  3       16.0733000 74350592 10180751360 partial_shared_hpl
  4        0.0196671        0           0 partial_shared_hpl
  5        5.7156200  4775936  9809465344 partial_shared_hpl
  6       29.3046000 95391744 13475909632 partial_shared_hpl
#+end_example

#+begin_src R :results output :session *R* :exports both
plot_results <- function(nb_proc) {
    ggplot(results[results$nb_proc==nb_proc,], aes(x=size, y=Gflops, color=hpl)) +
        geom_point() + geom_line() +
        expand_limits(x=0, y=0) +
        ggtitle(paste("Gflops vs size, nb_proc = ", nb_proc))
}
#+end_src

#+begin_src R :file hpl_analysis/18.png :results value graphics :results output :session *R* :exports both
plot_results(32)
#+end_src

#+RESULTS:
[[file:hpl_analysis/18.png]]

#+begin_src R :file hpl_analysis/19.png :results value graphics :results output :session *R* :exports both
plot_results(64)
#+end_src

#+RESULTS:
[[file:hpl_analysis/19.png]]

- It seems that this new optimization did not change the accuracy of the simulation. Let’s have a look at the time and
  memory.

  #+begin_src R :file hpl_analysis/20.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=simulation_time, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Simulation time vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/20.png]]

  #+begin_src R :file hpl_analysis/21.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=uss, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Real memory vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/21.png]]

- We see here that sharing some parts of the =PANEL->WORK= buffer has two effects. The simulation time is a bit larger,
  but the memory consumption is much lower.
- Let’s have a look in more details at this version of HPL.

  #+begin_src R :results output :session *R* :exports both
  do_plot <- function(my_plot, title) {
      return(my_plot +
          geom_point() + geom_line() +
          ggtitle(title)
      )
  }
  #+end_src

  #+begin_src R :file hpl_analysis/22.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_results, aes(x=size, y=simulation_time, group=nb_proc, color=nb_proc)),
     "Simulation time vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/22.png]]

  #+begin_src R :file hpl_analysis/23.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_results, aes(x=nb_proc, y=simulation_time, group=size, color=size)),
      "Simulation time vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/23.png]]

  #+begin_src R :file hpl_analysis/24.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_results, aes(x=size, y=uss, group=nb_proc, color=nb_proc)),
      "Physical memory consumption vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/24.png]]


  #+begin_src R :file hpl_analysis/25.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_results, aes(x=nb_proc, y=uss, group=size, color=size)),
      "Physical memory consumption vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/25.png]]

- The trend for the simulation time looks similar to what we got previously.
- The memory consumption still looks linear in the size and in the number of processes. However, it is almost flat for
  the number of processes.
**** Regression of Time and memory efficiency of the partial =shared_malloc=  (Arnaud) :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:

#+begin_src R :results output :session *R* :exports both
results$hpl=factor(results$hpl)
data = results[results$hpl=="partial_shared_hpl" & 
               results$nb_proc > 1 & results$size > 1000, # get rid of particularly small values
               c("nb_proc","size","Gflops","simulation_time","uss")]
head(data)
#+end_src

#+RESULTS:
:   nb_proc  size Gflops simulation_time      uss
: 1      24 25000 32.620        25.81190 55365632
: 2      24  5000  6.399         2.72733 14643200
: 3      24 35000 36.570        49.32340 74350592
: 6      64 40000 87.260       111.72900 95391744
: 7      24 10000 16.600         6.22743 26472448
: 8      40 40000 55.990       100.31300 91209728

#+begin_src R :results output graphics :file hpl_analysis/26.png :exports both :width 600 :height 400 :session *R*
plot(data)
#+end_src

#+RESULTS:
[[file:hpl_analysis/26.png]]

#+begin_src R :results output :session *R* :exports both
reg_rss = lm(data=data,uss ~ size+nb_proc) # Interactions do not bring much
summary(reg_rss)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = uss ~ size + nb_proc, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-6941093 -1573650  -348763  1611008  8790400 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 7.827e+05  1.030e+06   0.760     0.45    
size        2.054e+03  3.045e+01  67.449  < 2e-16 ***
nb_proc     1.717e+05  1.903e+04   9.022 7.85e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2791000 on 61 degrees of freedom
Multiple R-squared:  0.987,	Adjusted R-squared:  0.9866 
F-statistic:  2315 on 2 and 61 DF,  p-value: < 2.2e-16
#+end_example

#+begin_src R :results output graphics :file hpl_analysis/27.png :exports both :width 600 :height 400 :session *R*
par(mfrow=c(2,3)) ; 
  plot(data=data,uss~size); 
  plot(data=data,uss~nb_proc);
  plot(reg_rss); 
par(mfrow=c(1,1))
#+end_src

#+RESULTS:
[[file:hpl_analysis/27.png]]

The Stampede HPL output indicates:
#+BEGIN_EXAMPLE
The following parameter values will be used:

N        : 3875000 
NB       :    1024 
PMAP     : Column-major process mapping
P        :      77 
Q        :      78 
PFACT    :   Right 
NBMIN    :       4 
NDIV     :       2 
RFACT    :   Crout 
BCAST    :  BlongM 
DEPTH    :       0 
SWAP     : Binary-exchange
L1       : no-transposed form
U        : no-transposed form
EQUIL    : no
ALIGN    :    8 double precision words
#+END_EXAMPLE

We aim at ~size=3875000~ and ~nb_proc=77*78~.

#+begin_src R :results output :session *R* :exports both
data[data$nb_proc==64 & data$size==40000,]
data[data$nb_proc==64 & data$size==40000,]$uss/1E6 # in MB
example=data.frame(size=c(3875000,40000), nb_proc=c(77*78,64));
predict(reg_rss, example, interval="prediction", level=0.95)/1E6
#+end_src

#+RESULTS:
:   nb_proc  size Gflops simulation_time      uss
: 6      64 40000  87.26         111.729 95391744
: [1] 95.39174
:          fit        lwr        upr
: 1 8991.32610 8664.69163 9317.96056
: 2   93.93216   88.10931   99.75501

So we should need around 8 to 9 GB. Good.

#+begin_src R :results output :session *R* :exports both
reg_time = lm(data=data,simulation_time ~ poly(size,3)*poly(nb_proc,2)) # Interactions do not bring much
summary(reg_time)
reg_time = lm(data=data,simulation_time ~ poly(size,3)+poly(nb_proc,2)+I(size*nb_proc)) # Interactions do not bring much
summary(reg_time)
reg_time = lm(data=data,simulation_time ~ poly(size,2)+poly(nb_proc,1)+I(size*nb_proc)) # Interactions do not bring much
summary(reg_time)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = simulation_time ~ poly(size, 3) * poly(nb_proc, 
    2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.6972  -2.8188   0.1211   1.4618  23.6037 

Coefficients:
                                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)                       34.3882     0.8715  39.458  < 2e-16 ***
poly(size, 3)1                   200.7402     6.9721  28.792  < 2e-16 ***
poly(size, 3)2                    37.6113     6.9721   5.395 1.71e-06 ***
poly(size, 3)3                     0.9386     6.9721   0.135   0.8934    
poly(nb_proc, 2)1                110.2551     6.9721  15.814  < 2e-16 ***
poly(nb_proc, 2)2                 -9.0383     6.9721  -1.296   0.2006    
poly(size, 3)1:poly(nb_proc, 2)1 619.6089    55.7771  11.109 2.43e-15 ***
poly(size, 3)2:poly(nb_proc, 2)1 101.1174    55.7771   1.813   0.0756 .  
poly(size, 3)3:poly(nb_proc, 2)1  -2.3618    55.7771  -0.042   0.9664    
poly(size, 3)1:poly(nb_proc, 2)2 -54.5865    55.7771  -0.979   0.3323    
poly(size, 3)2:poly(nb_proc, 2)2 -13.4280    55.7771  -0.241   0.8107    
poly(size, 3)3:poly(nb_proc, 2)2  -6.7984    55.7771  -0.122   0.9035    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.972 on 52 degrees of freedom
Multiple R-squared:  0.9597,	Adjusted R-squared:  0.9511 
F-statistic: 112.5 on 11 and 52 DF,  p-value: < 2.2e-16

Call:
lm(formula = simulation_time ~ poly(size, 3) + poly(nb_proc, 
    2) + I(size * nb_proc), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.9992  -3.5157   0.0224   2.7090  25.8055 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)       -2.954e+00  3.452e+00  -0.856  0.39567    
poly(size, 3)1     4.863e+01  1.527e+01   3.184  0.00236 ** 
poly(size, 3)2     3.761e+01  6.930e+00   5.427 1.22e-06 ***
poly(size, 3)3     9.386e-01  6.930e+00   0.135  0.89275    
poly(nb_proc, 2)1 -4.186e+01  1.527e+01  -2.740  0.00818 ** 
poly(nb_proc, 2)2 -9.038e+00  6.930e+00  -1.304  0.19742    
I(size * nb_proc)  4.610e-05  4.125e-06  11.176 5.47e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.93 on 57 degrees of freedom
Multiple R-squared:  0.9563,	Adjusted R-squared:  0.9517 
F-statistic:   208 on 6 and 57 DF,  p-value: < 2.2e-16

Call:
lm(formula = simulation_time ~ poly(size, 2) + poly(nb_proc, 
    1) + I(size * nb_proc), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.8123  -3.6614   0.2628   2.4029  25.7019 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)       -2.954e+00  3.444e+00  -0.858  0.39442    
poly(size, 2)1     4.863e+01  1.524e+01   3.191  0.00227 ** 
poly(size, 2)2     3.761e+01  6.914e+00   5.440 1.07e-06 ***
poly(nb_proc, 1)  -4.186e+01  1.524e+01  -2.747  0.00797 ** 
I(size * nb_proc)  4.610e-05  4.115e-06  11.202 3.08e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.914 on 59 degrees of freedom
Multiple R-squared:  0.955,	Adjusted R-squared:  0.952 
F-statistic: 313.1 on 4 and 59 DF,  p-value: < 2.2e-16
#+end_example


#+begin_src R :results output graphics :file hpl_analysis/28.png :exports both :width 600 :height 400 :session *R*
par(mfrow=c(2,3)) ; 
  plot(data=data,simulation_time~size); 
  plot(data=data,simulation_time~nb_proc);
  plot(reg_time); 
par(mfrow=c(1,1))
#+end_src

#+RESULTS:
[[file:hpl_analysis/28.png]]


#+begin_src R :results output :session *R* :exports both
data[data$nb_proc==64 & data$size==40000,]
predict(reg_time, example, interval="prediction", level=0.95)/3600 # in hours
#+end_src

#+RESULTS:
:   nb_proc  size Gflops simulation_time      uss
: 6      64 40000  87.26         111.729 95391744
:            fit          lwr          upr
: 1 467.31578577 385.82615026 548.80542127
: 2   0.03431702   0.03008967   0.03854438

Aouch. This would be a 3 weeks simulation. :( We need to speed things
up.
*** 2017-03-31 Friday
**** Found a bug in the last commits of Simgrid             :SMPI:BUG:HPL:
- Issue reported on [[https://github.com/simgrid/simgrid/issues/147][Github]].
- Bug fixed.
- There are still some problems with HPL, some unitialized values used for comparisons:
  #+begin_example
  ==3320== Memcheck, a memory error detector
  ==3320== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al.
  ==3320== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info
  ==3320== Command: ./xhpl --cfg=surf/precision:1e-9 --cfg=network/model:SMPI --cfg=network/TCP-gamma:4194304 --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes --cfg=smpi/shared-malloc:local --cfg=smpi/privatize-global-variables:1 ./cluster_fat_tree_64.xml smpitmp-apprXPdW8
  ==3320== 
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'surf/precision' to '1e-9'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/model' to 'SMPI'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'network/TCP-gamma' to '4194304'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/bcast' to 'mpich'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/running-power' to '6217956542.969'
  [0.000000] [xbt_cfg/INFO] Option smpi/running-power has been renamed to smpi/host-speed. Consider switching.
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/display-timing' to 'yes'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatize-global-variables' to 'yes'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/shared-malloc' to 'local'
  [0.000000] [xbt_cfg/INFO] Configuration change: Set 'smpi/privatize-global-variables' to '1'
  [0.000000] [smpi_coll/INFO] Switch to algorithm mpich for collective bcast
  ================================================================================
  HPLinpack 2.2  --  High-Performance Linpack benchmark  --   February 24, 2016
  Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
  Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
  Modified by Julien Langou, University of Colorado Denver
  ================================================================================
  
  An explanation of the input/output parameters follows:
  T/V    : Wall time / encoded variant.
  N      : The order of the coefficient matrix A.
  NB     : The partitioning blocking factor.
  P      : The number of process rows.
  Q      : The number of process columns.
  Time   : Time in seconds to solve the linear system.
  Gflops : Rate of execution for solving the linear system.
  
  The following parameter values will be used:
  
  N      :      29       30       34       35 
  NB     :       1        2        3        4 
  PMAP   : Row-major process mapping
  P      :       2        1        4 
  Q      :       2        4        1 
  PFACT  :    Left    Crout    Right 
  NBMIN  :       2        4 
  NDIV   :       2 
  RFACT  :    Left    Crout    Right 
  BCAST  :   1ring 
  DEPTH  :       0 
  SWAP   : Mix (threshold = 64)
  L1     : transposed form
  U      : transposed form
  EQUIL  : yes
  ALIGN  : 8 double precision words
  
  --------------------------------------------------------------------------------
  
  - The matrix A is randomly generated for each test.
  - The following scaled residual check will be computed:
        ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
  - The relative machine precision (eps) is taken to be               1.110223e-16
  - Computational tests pass if scaled residuals are less than                16.0
  
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x42447D: HPL_pipid (HPL_pipid.c:144)
  ==3320==    by 0x418ED8: HPL_pdlaswp00T (HPL_pdlaswp00T.c:171)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x42476D: HPL_plindx0 (HPL_plindx0.c:246)
  ==3320==    by 0x418EF6: HPL_pdlaswp00T (HPL_pdlaswp00T.c:172)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4247A9: HPL_plindx0 (HPL_plindx0.c:250)
  ==3320==    by 0x418EF6: HPL_pdlaswp00T (HPL_pdlaswp00T.c:172)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Use of uninitialised value of size 8
  ==3320==    at 0x420413: HPL_dlaswp01T (HPL_dlaswp01T.c:240)
  ==3320==    by 0x418BDD: HPL_pdlaswp00T (HPL_pdlaswp00T.c:194)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4E779CC: idamax_ (in /usr/lib/libblas/libblas.so.3.6.0)
  ==3320==    by 0x4E779FA: idamaxsub_ (in /usr/lib/libblas/libblas.so.3.6.0)
  ==3320==    by 0x4E4796F: cblas_idamax (in /usr/lib/libblas/libblas.so.3.6.0)
  ==3320==    by 0x4134F0: HPL_dlocmax (HPL_dlocmax.c:125)
  ==3320==    by 0x40B277: HPL_pdpanllT (HPL_pdpanllT.c:167)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x417083: HPL_pdmxswp (HPL_pdmxswp.c:238)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x417098: HPL_pdmxswp (HPL_pdmxswp.c:238)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4170A2: HPL_pdmxswp (HPL_pdmxswp.c:239)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4170A4: HPL_pdmxswp (HPL_pdmxswp.c:239)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4170A6: HPL_pdmxswp (HPL_pdmxswp.c:239)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4150D5: HPL_dlocswpT (HPL_dlocswpT.c:134)
  ==3320==    by 0x40B4D2: HPL_pdpanllT (HPL_pdpanllT.c:222)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4150D7: HPL_dlocswpT (HPL_dlocswpT.c:134)
  ==3320==    by 0x40B4D2: HPL_pdpanllT (HPL_pdpanllT.c:222)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x40B4DF: HPL_pdpanllT (HPL_pdpanllT.c:223)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x40B4E1: HPL_pdpanllT (HPL_pdpanllT.c:223)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x42483B: HPL_plindx0 (HPL_plindx0.c:255)
  ==3320==    by 0x418EF6: HPL_pdlaswp00T (HPL_pdlaswp00T.c:172)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x424877: HPL_plindx0 (HPL_plindx0.c:269)
  ==3320==    by 0x418EF6: HPL_pdlaswp00T (HPL_pdlaswp00T.c:172)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Use of uninitialised value of size 8
  ==3320==    at 0x420B90: HPL_dlaswp02N (HPL_dlaswp02N.c:199)
  ==3320==    by 0x418570: HPL_pdlaswp00T (HPL_pdlaswp00T.c:198)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Use of uninitialised value of size 8
  ==3320==    at 0x422901: HPL_dlaswp04T (HPL_dlaswp04T.c:259)
  ==3320==    by 0x418CC3: HPL_pdlaswp00T (HPL_pdlaswp00T.c:329)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x41F06D: HPL_pdpanel_free (HPL_pdpanel_free.c:79)
  ==3320==    by 0x41AF31: HPL_pdgesv0 (HPL_pdgesv0.c:141)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4248A5: HPL_plindx0 (HPL_plindx0.c:258)
  ==3320==    by 0x418EF6: HPL_pdlaswp00T (HPL_pdlaswp00T.c:172)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4203FF: HPL_dlaswp01T (HPL_dlaswp01T.c:237)
  ==3320==    by 0x418BDD: HPL_pdlaswp00T (HPL_pdlaswp00T.c:194)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Use of uninitialised value of size 8
  ==3320==    at 0x4205A0: HPL_dlaswp01T (HPL_dlaswp01T.c:245)
  ==3320==    by 0x418BDD: HPL_pdlaswp00T (HPL_pdlaswp00T.c:194)
  ==3320==    by 0x40E878: HPL_pdupdateTT (HPL_pdupdateTT.c:271)
  ==3320==    by 0x41AF9F: HPL_pdgesv0 (HPL_pdgesv0.c:152)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x4170B5: HPL_pdmxswp (HPL_pdmxswp.c:240)
  ==3320==    by 0x40B4C2: HPL_pdpanllT (HPL_pdpanllT.c:221)
  ==3320==    by 0x4243C8: HPL_pdfact (HPL_pdfact.c:129)
  ==3320==    by 0x41AF61: HPL_pdgesv0 (HPL_pdgesv0.c:146)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  ==3320== Conditional jump or move depends on uninitialised value(s)
  ==3320==    at 0x41F06D: HPL_pdpanel_free (HPL_pdpanel_free.c:79)
  ==3320==    by 0x41F040: HPL_pdpanel_disp (HPL_pdpanel_disp.c:89)
  ==3320==    by 0x41AFCD: HPL_pdgesv0 (HPL_pdgesv0.c:161)
  ==3320==    by 0x40EFC4: HPL_pdgesv (HPL_pdgesv.c:103)
  ==3320==    by 0x406F64: HPL_pdtest (HPL_pdtest.c:197)
  ==3320==    by 0x401D38: smpi_simulated_main_ (HPL_pddriver.c:223)
  ==3320==    by 0x525BCDA: smpi_main_wrapper (smpi_global.cpp:366)
  ==3320==    by 0x5129B8D: operator() (functional.hpp:48)
  ==3320==    by 0x5129B8D: std::_Function_handler<void (), simgrid::xbt::MainFunction<int (*)(int, char**)> >::_M_invoke(std::_Any_data const&) (functional:1740)
  ==3320==    by 0x5151BB1: operator() (functional:2136)
  ==3320==    by 0x5151BB1: operator() (Context.hpp:92)
  ==3320==    by 0x5151BB1: simgrid::kernel::context::RawContext::wrapper(void*) (ContextRaw.cpp:303)
  ==3320== 
  [0.884470] /home/degomme/simgrid/src/simix/smx_global.cpp:567: [simix_kernel/CRITICAL] Oops ! Deadlock or code not perfectly clean.
  [0.884470] [simix_kernel/INFO] 16 processes are still running, waiting for something.
  [0.884470] [simix_kernel/INFO] Legend of the following listing: "Process <pid> (<name>@<host>): <status>"
  [0.884470] [simix_kernel/INFO] Process 1 (0@host-0.hawaii.edu): waiting for communication synchro 0xfb4beb0 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 2 (1@host-1.hawaii.edu): waiting for communication synchro 0xfb4b0c0 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 3 (2@host-2.hawaii.edu): waiting for communication synchro 0xfb49760 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 4 (3@host-3.hawaii.edu): waiting for communication synchro 0xfb47590 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 5 (4@host-4.hawaii.edu): waiting for synchronization synchro 0xf8a1ae0 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 6 (5@host-5.hawaii.edu): waiting for synchronization synchro 0xf8a1f10 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 7 (6@host-6.hawaii.edu): waiting for synchronization synchro 0xf897500 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 8 (7@host-7.hawaii.edu): waiting for synchronization synchro 0xf89b190 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 9 (8@host-8.hawaii.edu): waiting for synchronization synchro 0xf8a3680 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 10 (9@host-9.hawaii.edu): waiting for synchronization synchro 0xf896280 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 11 (10@host-10.hawaii.edu): waiting for synchronization synchro 0xf8970d0 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 12 (11@host-11.hawaii.edu): waiting for synchronization synchro 0xf89b5c0 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 13 (12@host-12.hawaii.edu): waiting for synchronization synchro 0xf89ce30 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 14 (13@host-13.hawaii.edu): waiting for synchronization synchro 0xf89f530 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 15 (14@host-14.hawaii.edu): waiting for synchronization synchro 0xf89f100 () in state 0 to finish
  [0.884470] [simix_kernel/INFO] Process 16 (15@host-15.hawaii.edu): waiting for synchronization synchro 0xf8a0ca0 () in state 0 to finish
  ==3320== 
  ==3320== Process terminating with default action of signal 6 (SIGABRT)
  ==3320==    at 0x5619428: raise (raise.c:54)
  ==3320==    by 0x561B029: abort (abort.c:89)
  ==3320==    by 0x52347B8: xbt_abort (xbt_main.cpp:167)
  ==3320==    by 0x52F4768: SIMIX_run.part.110 (smx_global.cpp:569)
  ==3320==    by 0x52F6204: SIMIX_run (stl_algobase.h:224)
  ==3320==    by 0x5263E66: smpi_main (smpi_global.cpp:474)
  ==3320==    by 0x560482F: (below main) (libc-start.c:291)
  ==3320== 
  ==3320== HEAP SUMMARY:
  ==3320==     in use at exit: 136,159,788 bytes in 7,560 blocks
  ==3320==   total heap usage: 39,378 allocs, 31,818 frees, 140,230,437 bytes allocated
  ==3320== 
  ==3320== LEAK SUMMARY:
  ==3320==    definitely lost: 321 bytes in 4 blocks
  ==3320==    indirectly lost: 0 bytes in 0 blocks
  ==3320==      possibly lost: 134,294,280 bytes in 96 blocks
  ==3320==    still reachable: 1,865,187 bytes in 7,460 blocks
  ==3320==         suppressed: 0 bytes in 0 blocks
  ==3320== Rerun with --leak-check=full to see details of leaked memory
  ==3320== 
  ==3320== For counts of detected and suppressed errors, rerun with: -v
  ==3320== Use --track-origins=yes to see where uninitialised values come from
  ==3320== ERROR SUMMARY: 1147 errors from 24 contexts (suppressed: 0 from 0)
  valgrind --track-origins:yes ./xhpl --cfg=surf/precision:1e-9 --cfg=network/model:SMPI --cfg=network/TCP-gamma:4194304 --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes --cfg=smpi/shared-malloc:local --cfg=smpi/privatize-global-variables:1 ./cluster_fat_tree_64.xml smpitmp-apprXPdW8
  Execution failed with code 134.
  #+end_example
- Note that this file has been obtained with a nearly-vanilla HPL (see the Github issue). No =smpi_usleep=, and shared
  =malloc= only for the matrix (no partial shared =malloc= for =PANEL->WORK=). Thus, it is quite strange to see such errors.
- The first error (=HPL_pipid.c:144=) happens because =PANEL->ia= is unitialized (checked by modifying the two operands one
  after the other to see if the error persists).
** 2017-04 April
*** 2017-04-03 Monday
**** Keep tracking Simgrid/HPL bug                          :SMPI:BUG:HPL:
- In =HPL_pipid.c=, it is not =PANEL->ia= that is uninitialized, but =PANEL->DPIV[0]=.
- =PANEL->DPIV= is set at the end of =HPL_pdmxswp.c=. But for some ranks, the function =HPL_pipid.c= is called before any call
  to the function =HPL_pdmxswp.c=. For these ranks, it seems taht =DPIV= is initialized by =HPL_bcast= (this function
  broadcasts the panel).
- The implementation of broadcast that is used is =HPL_bcast_1ring=.
- The preprocessor variable =HPL_USE_MPI_DATATYPE= is defined.
- When =PANEL->DPIV[0]= is not defined by =HPL_pipid.c=, it is modified by the only call to =MPI_Recv= in =HPL_1ring.c=. In
  valgrind, when we use shared =malloc=, the new value is always 0, whereas when we use the classical =malloc= the new value
  is different at each time. Same thing if we use the shared =malloc= but we modify the function =smpi_is_shared= to always
  return false: the new value is different at each time.
- Suspect that this bug has something to do with the =MPI_DATATYPE=.
*** 2017-04-04 Tuesday
**** Keep tracking Simgrid/HPL bug                          :SMPI:BUG:HPL:
- Can run =smpirun= with some logs by using option =--log=root.:trace=.
- Even with the classical =malloc=, no copy to the address range of =DPIV= is reported by Simgrid. There
  is a copy to some nearby addresses. At the end, the values of =DPIV= have been modified, so there must have been a copy somehow.
- If we drop the user-defined data-type (by using HPL compilation option =-DHPL_NO_MPI_DATATYPE= in the Makefile), then we
  see a copy to the expected address.
- Also, if we use this option, HPL seems to work fine.
**** So, the bug comes from the datatype                    :SMPI:BUG:HPL:
- Calls to datatype methods are not dumped in the logs. Let’s fix this. Add this in every =Datatype= method:
  #+begin_src c
  fprintf(stderr, "##### %s\n", __PRETTY_FUNCTION__);
  #+end_src
- Then, we see that the =Datatype= methods seem to be called correctly (and quite a lot). However, the methods
  =Datatype::copy=, =Datatype::serialize= and =Datatype::unserialize= are never called (except a few times at the very
  beginning of the execution for the =copy= method).
- Let’s simply drop this =MPI_Datatype= thing in HPL.
**** Effect on the performances and the prediction  :SMPI:PERFORMANCE:HPL:
- First, run a test with N=40000, P=Q=8, with the same Simgrid commit than before but without using =MPI_Datatype= in HPL.
  + Simgrid commit: =c8db21208f3436c35d3fdf5a875a0059719bff43=
  + HPL commit: =20ba5b38bb5ebbf311000fd2d63c34141d4790f5=
  + Script commit: =7a9e467f9446c65a9dbc2a76c4dab7a3d8209148=
  Metrics:
  + Simulation time: 99.0 (was 111.729)
  + Physical memory: 15 MB (was 95 MB)
  + Gflops: 86.9 (was 87.3)
- Then, run a test with N=40000, P=Q=8, with the last version of Simgrid and still without using =MPI_Datatype= in HPL.
  + Simgrid commit: =aaa9acf3fdd6714a158a161cac64c82992d52899=
  + HPL commit: =20ba5b38bb5ebbf311000fd2d63c34141d4790f5=
  + Script commit: =7a9e467f9446c65a9dbc2a76c4dab7a3d8209148=
  Metrics:
  + Simulation time: 100.0 (was 111.729)
  + Physical memory: 17 MB (was 95 MB)
  + Gflops: 86.9 (was 87.3)
- Thus, it seems that the simulation takes a lot less of memory and a bit less of time, while keeping a reasonably good accuracy.
*** 2017-04-05 Wednesday
**** Implementation of =SMPI_PARTIAL_SHARED_MALLOC=             :SMPI:C:CPP:
- This adds a new macro in Simgrid:
  #+begin_src c
  SMPI_PARTIAL_SHARED_MALLOC(size, shared_block_offsets, nb_shared_blocks)
  #+end_src
  + =size= is the size (in bytes) of the contiguous block of memory returned by this macro.
  + =shared_block_offsets= is an array [start_1, stop_1, start_2, stop_2, ..., start_n, stop_n] where each couple
    (start_i, stop_i) denotes a block that may be shared.
    Calling this function with [0, size] is equivalent to calling =SMPI_SHARED_MALLOC=: everything is shared.
  + =nb_shared_blocks= denotes the number of blocks (n in the previous example).
- The code of =SMPI_SHARED_MALLOC= for =global= shared malloc has been factorized with this code, to avoid duplication.
- This does not break the =tesh= test for =SMPI_SHARED_MALLOC=.
- A new =tesh= test has been made for =SMPI_PARTIAL_SHARED_MALLOC=.
- HPL seems to work correctly with this.
**** DONE Future work for shared allocation [3/3]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:35]
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:35]
- State "TODO"       from "TODO"       [2017-04-07 Fri 16:44]
- State "TODO"       from              [2017-04-05 Wed 17:26]
:END:
- [X] Avoid copying data from/to a shared block. This is a generalization of what is currently done for the shared malloc.
- [X] Do a new analysis of HPL once we have this feature.
- [X] Integrate the unit tests in Simgrid and use =boost=, as explained on [[http://simgrid.gforge.inria.fr/simgrid/3.15/doc/inside_tests.html][the website]]. Currently, these are in a single
  file “outside” of Simgrid with some hand-made assertions. The integration tests are already integrated however, using
  =tesh=.
*** 2017-04-06 Thursday
**** LIG keynote                                                 :MEETING:
- The privacy and security challenges raised by precision medicine
- Jean-Pierre Hubaux
- Book: The Circle (Dave Egger)
- The cost of a full genome sequencing decreased a lot: around 100M$ in 2001, 1k$ now. Thus, we can expect that
  everybody will have his/her genome sequenced in the future.
- The number of persons that have had their genome sequenced increases very fast.
- Genome editing: a recent technology to modify the genome. Currently forbidden on human genome.
- Surprisingly, very few researchers work on genome protection in France and UK.
- [[http://genomeprivacy.org][Website]]
**** Begin working on the communication optimization          :SMPI:C:CPP:
- We would like a generalization of what was done for =SMPI_SHARED_MALLOC=, but for =SMPI_PARTIAL_SHARED_MALLOC=.
*** 2017-04-07 Friday
**** Terminate working on the communication optimization      :SMPI:C:CPP:
- Now, for a copy from =src_buff= to =dst_buff=, only the zones which are non-shared in *both* buffers will be copied.
- Quick performance test: we have a gain, but not so impressive. For N=40000 and P=Q=8, the simulation time decreases
  from 100 seconds to 72 seconds.
**** Strange thing with =SMPI_SHARED_FREE=      :SMPI:C:CPP:PERFORMANCE:BUG:
- The performances reported just above have been obtained with a call to =munmap(PANEL->WORK, PANEL->lwork)= to deallocate
  the panel. We should replace it by a call to =SMPI_SHARED_FREE(PANEL->WORK)= which will in turn call
  =munmap(PANEL->WORK, 0)=. But this causes the memory consumption to be multiplied by 10, as well as a (smaller) increase
  of the simulation time.
- Thus, it seems that the size given to =munmap= is important, despite what is said in comment in Simgrid.
- We should still release the meta-data of the partial allocation, so we should think of a way to do so. Maybe give the
  size to =SMPI_SHARED_FREE=?
**** Time and memory efficiency with =partial_shared_malloc= and optimized communications :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Now, we measure the performances of HPL with =partial_shared_malloc= (the one implemented in Simgrid) and the optimized communications.
- Note that we do not use anymore the MPI datatypes in HPL (they were used in all the previous experiments). This
  results in lower memory consumption and simulation time.
- Simgrid commit: =0404a97dcdc03fe9e02c3a36ab9f3def6d297340=
- HPL commit: =21bdc1cf1418d587128140013a5f8085e9a5ff94=
- Script commit: =d54ed5c208621ee9fd84cc6b797fc51bddb1ece4=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl_partial_shared.csv --nb_runs 1 --size 100,5000,10000,15000,20000,25000,30000,35000,40000
  --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  partial_shared_nomemcpy_results <- read.csv('hpl_analysis/hpl_partial_shared_nomemcpy.csv')
  partial_shared_results <- read.csv('hpl_analysis/hpl_partial_shared.csv')
  optimized_results <- read.csv('hpl_analysis/hpl.csv')
  vanilla_results <- read.csv('hpl_analysis/hpl_vanilla.csv')
  partial_shared_nomemcpy_results$hpl = 'partial_shared_nomemcpy_hpl'
  partial_shared_results$hpl = 'partial_shared_hpl'
  optimized_results$hpl = 'optimized_hpl'
  vanilla_results$hpl = 'vanilla_hpl'
  results = rbind(partial_shared_nomemcpy_results, partial_shared_results, optimized_results, vanilla_results)
  head(results)
  #+end_src

  #+RESULTS:
  #+begin_example
  Need help? Try the ggplot2 mailing list:
  http://groups.google.com/group/ggplot2.
         topology nb_roots nb_proc  size    time    Gflops simulation_time
  1 2;8,8;1,8;1,1        8      64  5000   10.62  7.848000       3.8053100
  2 2;8,8;1,8;1,1        8      48 25000  175.83 59.250000      17.9611000
  3 2;8,8;1,8;1,1        8      48 10000   24.09 27.680000       4.3511700
  4 2;8,8;1,8;1,1        8      32 35000  585.01 48.860000      28.1705000
  5 2;8,8;1,8;1,1        8       8 40000 3000.68 14.220000      16.6989000
  6 2;8,8;1,8;1,1        8      16   100    0.08  0.008048       0.0263181
    application_time      uss         rss                         hpl
  1       0.96112900 10743808   295067648 partial_shared_nomemcpy_hpl
  2       9.13868000 12247040  5404385280 partial_shared_nomemcpy_hpl
  3       1.76401000 11235328   971894784 partial_shared_nomemcpy_hpl
  4      16.20590000  9351168 10219630592 partial_shared_nomemcpy_hpl
  5      13.19390000  6610944 13042294784 partial_shared_nomemcpy_hpl
  6       0.00636301        0           0 partial_shared_nomemcpy_hpl
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  plot_results <- function(nb_proc) {
      ggplot(results[results$nb_proc==nb_proc,], aes(x=size, y=Gflops, color=hpl)) +
          geom_point() + geom_line() +
          expand_limits(x=0, y=0) +
          ggtitle(paste("Gflops vs size, nb_proc = ", nb_proc))
  }
  #+end_src

  #+begin_src R :file hpl_analysis/29.png :results value graphics :results output :session *R* :exports both
  plot_results(64)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/29.png]]


  As for the other experiments, the accuracy of the simulation is relatively good.

  #+begin_src R :file hpl_analysis/30.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=simulation_time, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Simulation time vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/30.png]]

  #+begin_src R :file hpl_analysis/31.png :results value graphics :results output :session *R* :exports both
  ggplot(results[results$nb_proc==64,], aes(x=size, y=uss, color=hpl)) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Real memory vs size, P=Q=8")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/31.png]]

  We see here that the latest version of Simgrid/HPL achieves both the lowest simulation time and memory consumption. As
  mentionned, the lower memory consumption of =partial_shared_nomemcpy_hpl= in comparison with =partial_shared_hpl= is due
  to the use of MPI datatypes. Its lower execution time is due to both the use of MPI datatypes and the optimizations in
  the communications.

- Let’s have a look in more details.
  #+begin_src R :results output :session *R* :exports both
  do_plot <- function(my_plot, title) {
      return(my_plot +
          geom_point() + geom_line() +
          ggtitle(title)
      )
  }
  #+end_src

  #+begin_src R :file hpl_analysis/32.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_nomemcpy_results, aes(x=size, y=simulation_time, group=nb_proc, color=nb_proc)),
     "Simulation time vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/32.png]]

  #+begin_src R :file hpl_analysis/33.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_nomemcpy_results, aes(x=nb_proc, y=simulation_time, group=size, color=size)),
      "Simulation time vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/33.png]]

  #+begin_src R :file hpl_analysis/34.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_nomemcpy_results, aes(x=size, y=uss, group=nb_proc, color=nb_proc)),
      "Physical memory consumption vs size")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/34.png]]


  #+begin_src R :file hpl_analysis/35.png :results value graphics :results output :session *R* :exports both
  do_plot(ggplot(partial_shared_nomemcpy_results, aes(x=nb_proc, y=uss, group=size, color=size)),
      "Physical memory consumption vs number of processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/35.png]]

  The simulation time still looks quadratic in the size and roughly linear in the number of processes. The memory
  consumption is much more noisy than before, which is strange.
*** 2017-04-10 Monday
**** Profling the last version of HPL :SMPI:EXPERIMENTS:PERFORMANCE:PROFILING:HPL:
- Simgrid commit: =0404a97dcdc03fe9e02c3a36ab9f3def6d297340=
- HPL commit: =21bdc1cf1418d587128140013a5f8085e9a5ff94=
- Using =libatlas3-base= package for BLAS.
- Run the following command:
  #+begin_src sh
  smpirun -wrapper "valgrind --tool=callgrind" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 16 -hostfile ./hostfile_64.txt -platform
  ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Results:
  + [[file:callgrind/callgrind.out.8458][Output file]]
  + Visualization:
    [[file:callgrind/callgrind.8458.png]]

- Here, we see that there is no clear “winner”. Several different functions take a significant part of the time, but
  none of them is too expensive (relatively to the others). Some of them are HPL functions, others are in Simgrid.
**** Optimizing further?                            :SMPI:PERFORMANCE:HPL:
- In the previous profiling, we see that one of the most expensive functions is =HPL_dlaswp10N=.
- This function performs a sequence of local column interchanges. It is made of a =for= loop.
**** Time and memory efficiency with =partial_shared_malloc= and optimized communications (bis) :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Let’s take back the times of our last analysis and have a look at the application time and the Simgrid time.
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results = read.csv('hpl_analysis/hpl_partial_shared_nomemcpy.csv')
  application_time = results[,c('size', 'nb_proc', 'application_time')]
  simulation_time = results[,c('size', 'nb_proc', 'simulation_time')]
  colnames(application_time) = c('size', 'nb_proc', 'time')
  colnames(simulation_time) = c('size', 'nb_proc', 'time')
  simgrid_time = simulation_time
  simgrid_time$time = simulation_time$time - application_time$time
  application_time$group = 'application'
  simgrid_time$group = 'simgrid'
  simulation_time$group = 'simulation'
  all_times = rbind(application_time, simgrid_time, simulation_time)
  head(all_times)
  #+end_src

  #+RESULTS:
  :    size nb_proc        time       group
  : 1  5000      64  0.96112900 application
  : 2 25000      48  9.13868000 application
  : 3 10000      48  1.76401000 application
  : 4 35000      32 16.20590000 application
  : 5 40000       8 13.19390000 application
  : 6   100      16  0.00636301 application

  #+begin_src R :file hpl_analysis/36.png :results value graphics :results output :session *R* :exports both
  ggplot(all_times[all_times$nb_proc==56,], aes(x=size, y=time, color=factor(group))) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Time vs size, nb_proc=56")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/36.png]]

  #+begin_src R :file hpl_analysis/37.png :results value graphics :results output :session *R* :exports both
  ggplot(all_times[all_times$size==40000,], aes(x=nb_proc, y=time, color=factor(group))) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Time vs nb_proc, size=40000")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/37.png]]

- It seems that both the time spent in the application and the time spent in Simgrid are quadratic in the size. It means
  that there is still room for optimization on HPL side, but also that Simgrid have to be faster if we want to simulate
  Stampede in a reasonable time.
- The time as a function of the number of processes is quite strange. There is the same pattern for the application time
  and the Simgrid time, but at different intensities.
**** Regression of HPL performances       :SMPI:PERFORMANCE:PROFILING:HPL:
- Let’s redo the regression.
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  data <- read.csv('hpl_analysis/hpl_partial_shared_nomemcpy.csv')
  head(data)
  #+end_src

  #+RESULTS:
  #+begin_example
         topology nb_roots nb_proc  size    time    Gflops simulation_time
  1 2;8,8;1,8;1,1        8      64  5000   10.62  7.848000       3.8053100
  2 2;8,8;1,8;1,1        8      48 25000  175.83 59.250000      17.9611000
  3 2;8,8;1,8;1,1        8      48 10000   24.09 27.680000       4.3511700
  4 2;8,8;1,8;1,1        8      32 35000  585.01 48.860000      28.1705000
  5 2;8,8;1,8;1,1        8       8 40000 3000.68 14.220000      16.6989000
  6 2;8,8;1,8;1,1        8      16   100    0.08  0.008048       0.0263181
    application_time      uss         rss
  1       0.96112900 10743808   295067648
  2       9.13868000 12247040  5404385280
  3       1.76401000 11235328   971894784
  4      16.20590000  9351168 10219630592
  5      13.19390000  6610944 13042294784
  6       0.00636301        0           0
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  data = data[!(data$nb_proc%in% c(1,32,48,64)) & data$size > 1000, # get rid of particularly small values
                 c("nb_proc","size","Gflops","simulation_time","uss")]
  head(data)
  #+end_src

  #+RESULTS:
  :    nb_proc  size Gflops simulation_time      uss
  : 5        8 40000 14.220        16.69890  6610944
  : 7        8 25000 13.680         7.23655  6639616
  : 14      24 20000 29.020        12.91630  8564736
  : 16      16 10000 16.870         3.03431  7450624
  : 17      24  5000  6.376         1.89760  8179712
  : 18      56 25000 51.490        47.01380 13443072
  
  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  plot(data)
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-10187Gg0/figure10187SFt.png]]

  #+begin_src R :results output :session *R* :exports both
  reg_rss = lm(data=data,uss ~ nb_proc) # Interactions do not bring much
  summary(reg_rss)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = uss ~ nb_proc, data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -1521073  -626580  -330796  -105779  6108725 

  Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  5625097     470591   11.95 1.92e-14 ***
  nb_proc       148044      14022   10.56 7.38e-13 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1528000 on 38 degrees of freedom
  Multiple R-squared:  0.7458,	Adjusted R-squared:  0.7391 
  F-statistic: 111.5 on 1 and 38 DF,  p-value: 7.382e-13
  #+end_example

  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  par(mfrow=c(2,3)) ; 
    plot(data=data,uss~size); 
    plot(data=data,uss~nb_proc);
    plot(reg_rss); 
  par(mfrow=c(1,1))
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-10187Gg0/figure10187fPz.png]]

  The Stampede HPL output indicates:
  #+BEGIN_EXAMPLE
  The following parameter values will be used:
  
  N        : 3875000 
  NB       :    1024 
  PMAP     : Column-major process mapping
  P        :      77 
  Q        :      78 
  PFACT    :   Right 
  NBMIN    :       4 
  NDIV     :       2 
  RFACT    :   Crout 
  BCAST    :  BlongM 
  DEPTH    :       0 
  SWAP     : Binary-exchange
  L1       : no-transposed form
  U        : no-transposed form
  EQUIL    : no
  ALIGN    :    8 double precision words
  #+END_EXAMPLE

  We aim at ~size=3875000~ and ~nb_proc=77*78~.

  #+begin_src R :results output :session *R* :exports both
  data[data$nb_proc==64 & data$size==40000,]
  data[data$nb_proc==64 & data$size==40000,]$uss/1E6 # in MB
  example=data.frame(size=c(3875000,160000, 40000), nb_proc=c(77*78,56,56));
  predict(reg_rss, example, interval="prediction", level=0.95)/1E6
  #+end_src

  #+RESULTS:
  : [1] nb_proc         size            Gflops          simulation_time
  : [5] uss            
  : <0 lignes> (ou 'row.names' de longueur nulle)
  : numeric(0)
  :         fit       lwr        upr
  : 1 894.77819 725.08505 1064.47133
  : 2  13.91557  10.68973   17.14141
  : 3  13.91557  10.68973   17.14141

  Now, only about 0.5 to 1 GB of memory, even better.

  #+begin_src R :results output :session *R* :exports both
  reg_time = lm(data=data,simulation_time ~ poly(size,3)*poly(nb_proc,2)) # Interactions do not bring much
  summary(reg_time)
  reg_time = lm(data=data,simulation_time ~ poly(size,2)*nb_proc) # Interactions do not bring much
  summary(reg_time)
  #reg_time = lm(data=data,simulation_time ~ poly(size,2)+poly(nb_proc,1)+I(size*nb_proc)) # Interactions do not bring much
  #summary(reg_time)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = simulation_time ~ poly(size, 3) * poly(nb_proc, 
      2), data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -1.14955 -0.17968 -0.01722  0.16422  1.06040 

  Coefficients:
                                    Estimate Std. Error t value Pr(>|t|)    
  (Intercept)                       23.88059    0.08307 287.468  < 2e-16 ***
  poly(size, 3)1                   107.06451    0.52539 203.779  < 2e-16 ***
  poly(size, 3)2                    19.26611    0.52539  36.670  < 2e-16 ***
  poly(size, 3)3                     0.46426    0.52539   0.884 0.384419    
  poly(nb_proc, 2)1                 91.11093    0.52539 173.414  < 2e-16 ***
  poly(nb_proc, 2)2                  3.19501    0.52539   6.081 1.47e-06 ***
  poly(size, 3)1:poly(nb_proc, 2)1 396.02347    3.32288 119.181  < 2e-16 ***
  poly(size, 3)2:poly(nb_proc, 2)1  69.64274    3.32288  20.959  < 2e-16 ***
  poly(size, 3)3:poly(nb_proc, 2)1   4.73441    3.32288   1.425 0.165270    
  poly(size, 3)1:poly(nb_proc, 2)2  13.06017    3.32288   3.930 0.000506 ***
  poly(size, 3)2:poly(nb_proc, 2)2  -0.22607    3.32288  -0.068 0.946241    
  poly(size, 3)3:poly(nb_proc, 2)2  -2.29043    3.32288  -0.689 0.496315    
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.5254 on 28 degrees of freedom
  Multiple R-squared:  0.9997,	Adjusted R-squared:  0.9996 
  F-statistic:  7967 on 11 and 28 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = simulation_time ~ poly(size, 2) * nb_proc, data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -2.13568 -0.27522 -0.04158  0.35106  1.74401 

  Coefficients:
                          Estimate Std. Error t value Pr(>|t|)    
  (Intercept)            -0.195395   0.253878  -0.770    0.447    
  poly(size, 2)1          2.415668   1.605667   1.504    0.142    
  poly(size, 2)2          0.863077   1.605667   0.538    0.594    
  nb_proc                 0.835972   0.007564 110.513  < 2e-16 ***
  poly(size, 2)1:nb_proc  3.633640   0.047842  75.951  < 2e-16 ***
  poly(size, 2)2:nb_proc  0.638994   0.047842  13.356 4.39e-15 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.8244 on 34 degrees of freedom
  Multiple R-squared:  0.999,	Adjusted R-squared:  0.9989 
  F-statistic:  7114 on 5 and 34 DF,  p-value: < 2.2e-16
  #+end_example


  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  par(mfrow=c(2,3)) ; 
    plot(data=data,simulation_time~size); 
    plot(data=data,simulation_time~nb_proc);
    plot(reg_time); 
  par(mfrow=c(1,1))
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-10187Gg0/figure10187RZC.png]]


  #+begin_src R :results output :session *R* :exports both
  data[data$nb_proc==56 & data$size==40000,]
  predict(reg_time, example, interval="prediction", level=0.95)/3600/24 # in days
  #+end_src

  #+RESULTS:
  :    nb_proc  size Gflops simulation_time      uss
  : 72      56 40000  70.28         106.326 14508032
  :            fit          lwr          upr
  : 1 9.235452e+02 7.857690e+02 1.061321e+03
  : 2 1.605254e-02 1.511091e-02 1.699417e-02
  : 3 1.217587e-03 1.193879e-03 1.241294e-03

  It seems that the model of the previous regression was not good. We did not have a factor =size**2*nb_proc=, which is
  strange. The sad news is that with our new model, the prevision is much worse: 700 to 1000 days (we had about 20 days
  with the previous model).
- Did a quick test with 56 processes and a size of 160000.
  + Simulation time of 1437 seconds.
  + Application time of 805 seconds.
- Same thing, but with NB equal to 1024 instead of 120 (this is the size of the blocks).
  + Simulation time of 1305 seconds.
  + Application time of 732 seconds. 
**** Model checking in Simgrid                             :SMPI:BORDEAUX:
- The main idea is to explore the set graph, to see if a set is unreachable (safety property). Several techniques to do so.
- Dynamic partial order reduction (DPOR).
- In Simgrid, a transition in the graph is an exchange of message. Simgrid will try every possible order (which message
  arrives first).
- As always in model checking, there is an explosion of the number of messages.
- To reduce the state space, do partial-order reduction. Two transitions are /independent/ if they neither disable nor
  enable each other and if they commut. In other words, the order in which these transitions are executed does not
  matter. Thus, we have an equivalence class of traces. Also called model checking /representatives/.
- The dependencies are over-approximated: no dependencies are missed, but some transitions are (wrongly) labelled as dependent.
- Thanks to this, there is no need to explore every possible trace: only the equivalent classes.
*** 2017-04-11 Tuesday
**** TODO What we could try with SMPI/HPL [2/4] :SMPI:HPL:MEETING:BORDEAUX:
:LOGBOOK:
- State "TODO"       from "TODO"       [2017-04-11 Tue 17:46]
- State "TODO"       from "TODO"       [2017-04-11 Tue 14:10]
- State "TODO"       from              [2017-04-11 Tue 14:00]
:END:
- [X] Need to talk with Augustin, there is maybe something to do with the =MPI_Iprobe= to speedup the simulation. These are
  busy waitings done by HPL, which takes a lot of time for nothing. What did Christian for HPL was to increase gradually
  the time injected for =iprobe=.
- [ ] We should measure more precisely what happens to have an idea. Measure the time spent in different parts of HPL
  (e.g. column swaps). Also measure the number of Simgrid events. Try to see what is linear and what is quadratic.
- [ ] Do not use a constant for the local max that are then used for the swaps, otherwise no swap will be done (if no swap
  is done, this should have an impact on the estimated time for small enough matrices). Using random values would be
  better. Also, we should try to run vanilla HPL to have an idea of the pattern of the swaps.
- [X] Use the new way to do privatization in SMPI, context switches should be faster with this.
**** Tentative with =MPI_Iprobe=                 :SMPI:HPL:MEETING:BORDEAUX:
- Use the option =--cfg=smpi/iprobe:1= to say that all calls to =MPI_Iprobe= take one second (which is much larger than in
  reality).
- This decreases the simulation time, but not so much: from 58 seconds to 53 seconds for 64 processes and a matrix of
  size 40000. Also, the simulation is completely wrong now (as expected).
- Thus, this is maybe not a good idea.
**** Tentative with the new privatization      :SMPI:HPL:MEETING:BORDEAUX:
- Merge the branches in Simgrid to get the new privatization method with the partial shared malloc.
- Use the option =--cfg=smpi/privatize-global-variables:dlopen=.
- Simulation time decreases from 58 to 48 seconds, without impacting the simulation result. This looks better.
**** Looking at the syscalls                   :SMPI:HPL:MEETING:BORDEAUX:
- First, run Simgrid with =strace= (we have N=40000 and P=Q=8):
  #+begin_src sh
  smpirun -wrapper "strace" --log=root.:critical --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:yes -np 64 -hostfile ./hostfile_64.txt -platform
  ./cluster_fat_tree_64.xml ./xhpl 2> /tmp/output_strace &> /tmp/output_strace
  #+end_src
- Then, process this trace:
  #+begin_src sh
  cat /tmp/output_strace | cut -d '(' -f 1 | sort | uniq -c | sort -rn | head -n 4
  #+end_src

  #+RESULTS:
  | 8562328 | clock_gettime |
  | 6415478 | mmap          |
  |   21653 | munmap        |
  |   14953 | read          |

- We see that there is a lot of =clock_gettime= and =mmap=. But a =mmap= is way more expensive. Thus, it could explain a large
  part of the time consumption. Also, the number of calls to =mmap= should be quadratic in the size of the matrix.
- Running =smpirun= with =-wrapper /usr/bin/time= tells that 36 seconds are spent in user mode and 20 seconds in kernel
  mode. This is very large.
- To decrease the amount of =mmap=, an idea is to play with the option =--cfg=smpi/shared-malloc-blocksize=. Recall that
  the default is a size of 1 MB (i.e. 2^20 bytes).
  | Block size | User time | System time |
  |------------+-----------+-------------|
  |       2^18 |      34.5 |        22.6 |
  |       2^20 |      35.9 |        20.6 |
  |       2^22 |      39.6 |        22.7 |
  |       2^24 |      43.0 |        23.3 |
  |       2^26 |      42.7 |        23.9 |

- So in fact this does not change much the time.
- Tried to run =smpirun= with =strace= and another block size. For a block size of 2^24, the number of =mmap= is 6275035, so
  not a huge difference.
- It seems that all these =mmap= do not come from the (partial) shared malloc. For N=20000 and P=Q=4, we have 849873 =mmap=
  (with a block size of 16 MB). But there is only 2688 initializations of panels, which are all less than 16 MB.
**** Looking at the number of MPI calls        :SMPI:HPL:MEETING:BORDEAUX:
- Run =smpirun= with the option =--log=smpi_mpi.:debug= for various sizes of matrix.
- Size of the trace produced:
  | Matrix size | Trace size |
  |-------------+------------|
  |       10000 |     977573 |
  |       20000 |    1986661 |
  |       40000 |    4083523 |
- So the ammount of communication looks to be linear and not quadratic in the size. So this does not seem to be the
  source of our quadratic time (the size of the communications is supposed not to have a big impact, because we now
  avoid the =memcpy=).
*** 2017-04-12 Wednesday
**** Finally the =mmap= came from the privatization :SMPI:HPL:MEETING:BORDEAUX:
- The above experiments were done with the old privatization. When we use =dlopen=, for N=20000 and P=Q=4, the number of
  =mmap= becomes 22589, which looks more reasonable.
  #+begin_src sh
  cat /tmp/output_strace_dlopen | cut -d '(' -f 1 | sort | uniq -c | sort -rn | head -n 3
  #+end_src

  #+RESULTS:
  | 1257198 | clock_gettime |
  |   25130 | write         |
  |   22589 | mmap          |

- Traced every call to =mmap= done by the shared malloc, by using =XBT_DEBUG=. There are 22350 such calls.
- With =dlopen= privatization, for N=40000 and P=Q=8, the times are now 31 seconds in user mode and 15 seconds in kernel
  mode. Both of them have decreased. The time in kernel mode remains strangely large.
- Use =strace -c= to get the time spent on each syscall. The whole time is 0.1 seconds, so the high time spent in kernel
  mode is not this.
- Use =perf stat= to have some statistics. We see that there are 16 millions page faults, which seems high.
- For a size of 20000, we have 4 millions page faults. So it seems to be quadratic in the matrix size.
- Tried to do a call to =mlock= at the end of each shared malloc. It guarantees that the whole block is resident in
  memory. Need to run the program as root for such large blocks. The times are now 32 seconds for user, 24 seconds for
  system (which is higher).
- It would be good to do the =mmap= on =/dev/null=, so that the kernel does not populate the pages. But it is not possible,
  we would need to patch the kernel...
**** Playing with flame graphs                 :SMPI:HPL:MEETING:BORDEAUX:
- Run =smpirun= with =perf=:
  #+begin_src sh
  smpirun --log=root.:critical -wrapper "perf record -F99 --call-graph dwarf" --cfg=smpi/bcast:mpich
  --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np
  64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Then, process the trace:
  #+begin_src sh
  perf script | ~/Documents/FlameGraph/stackcollapse-perf.pl --kernel | ~/Documents/FlameGraph/flamegraph.pl > /tmp/myapp.svg
  #+end_src
  Where the scripts come from the [[https://github.com/brendangregg/FlameGraph][Flamegraph Github]].
*** 2017-04-13 Thursday
**** DONE Future work [2/2]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-04-13 Thu 11:41]
- State "TODO"       from "TODO"       [2017-04-13 Thu 09:44]
- State "TODO"       from              [2017-04-13 Thu 01:13]
:END:
- [X] Redo some experiments to have new plots (is it still quadratic?).
- [X] Get some MPI traces with Simgrid.
**** Time and memory efficiency with =partial_shared_malloc=, optimized communications and =dlopen= :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Let’s do yet another analysis, to see if the time is still quadratic in the size. This time we use the =dlopen= privatization.
- Simgrid commit: =23a4a825789444c7bc954b0075fdd23715facaf5=
- HPL commit: =d2fa2ddc8e840bcfba89f2953f64c32086f7da04=
- Script commit: =a5ade29a742b23a8d0ea5ab5c9e525c58f530a23=
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results = read.csv('hpl_analysis/hpl_partial_shared_nomemcpy_dlopen.csv')
  application_time = results[,c('size', 'nb_proc', 'application_time')]
  simulation_time = results[,c('size', 'nb_proc', 'simulation_time')]
  colnames(application_time) = c('size', 'nb_proc', 'time')
  colnames(simulation_time) = c('size', 'nb_proc', 'time')
  simgrid_time = simulation_time
  simgrid_time$time = simulation_time$time - application_time$time
  application_time$group = 'application'
  simgrid_time$group = 'simgrid'
  simulation_time$group = 'simulation'
  all_times = rbind(application_time, simgrid_time, simulation_time)
  head(all_times)
  #+end_src

  #+RESULTS:
  :    size nb_proc     time       group
  : 1 25000      40 13.78860 application
  : 2 15000      40  5.42893 application
  : 3 15000      56  6.90723 application
  : 4 30000      40 19.74910 application
  : 5 25000      32  9.05321 application
  : 6 40000      24 24.94500 application

  #+begin_src R :file hpl_analysis/38.png :results value graphics :results output :session *R* :exports both
  ggplot(all_times[all_times$nb_proc==56,], aes(x=size, y=time, color=factor(group))) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Time vs size, nb_proc=56")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/38.png]]

  #+begin_src R :file hpl_analysis/39.png :results value graphics :results output :session *R* :exports both
  ggplot(all_times[all_times$size==40000,], aes(x=nb_proc, y=time, color=factor(group))) +
      geom_point() + geom_line() +
      expand_limits(x=0, y=0) +
      ggtitle("Time vs nb_proc, size=40000")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/39.png]]

- The time is lower when we use =dlopen= (compare the two plots), and the time spent in Simgrid is still linear.
**** Patch the Linux kernel           :PROGRAMMING:TOOLS:MEETING:BORDEAUX:
- Get [[file:kernel_patch/dev_garbage][the patch]] written by Samuel.
- Get and fix the sources.
  #+begin_src sh
  apt-get source linux
  cd linux-*
  cp ../dev_garbage debian/patches/ # copy the patch
  #+end_src
- Add =dev_garbage= at the end of the file =debian/patches/series=.
- Apply the patch.
  #+begin_src sh
  dh_quilt_patch
  cp /boot/config-3.16.0-4-amd64 .config # get the config (the version needs to be the same)
  make -j 16 deb-pkg
  dpkg -i linux-image-*.deb
  #+end_src
- Then reboot and choose the right version.
**** Visualization of HPL communications       :SMPI:TRACING:HPL:BORDEAUX:
- Run Simgrid with the trace. Use N=40000 and P=Q=4.
  #+begin_src sh
  smpirun -trace -trace-file /tmp/trace --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np 16 -hostfile ./hostfile_64.txt
  -platform ./cluster_fat_tree_64.xml ./xhp
  #+end_src
- Visualize the trace.
  #+begin_src sh
  vite /tmp/trace
  #+end_src
  + Unzoomed
    [[file:trace_visualization/1.png]]
  + Zoomed
    [[file:trace_visualization/2.png]]
  + More zoomed
    [[file:trace_visualization/3.png]]

- As expected, most of the time in HPL is spent for computations, there are (relatively) few communications.
- There are clearly synchronization phases happening regularly. They are made of =MPI_Wait= (in yellow), =MPI_Send= (in
  blue) and =MPI_Recv= (in red).
- Thus, the idea of time parallelism may be good. We would run in parallel different simulations, starting at a
  different step of HPL. Then we will join them to get the whole picture.
*** 2017-04-14 Friday
**** Have a look at the sizes of the traces             :SMPI:TRACING:HPL:
- Run the following command, for P=Q=4 and sizes 10000, 20000 and 40000 (do not forget to change the file name for each
  size):
  #+begin_src sh
  smpirun -trace -trace-file /tmp/trace_40000 --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np 16 -hostfile ./hostfile_64.txt
  -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Now, let’s have a look at the sizes of the traces.
  #+begin_src sh
  du -b /tmp/trace_*
  #+end_src
  #+RESULTS:
  | 16189469 | /tmp/trace_10000 |
  | 33795702 | /tmp/trace_20000 |
  | 70259764 | /tmp/trace_40000 |
- Let’s have a look at the log. Run the following command for sizes of 10000, 20000 and 40000:
  #+begin_src sh
  smpirun --log=smpi.:trace --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes
  --cfg=smpi/privatize-global-variables:dlopen -np 64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml
  ./xhpl 2> /tmp/log_40000
  #+end_src
  Then, look at the sizes.
  #+begin_src sh
  du -b /tmp/log_*
  #+end_src
  #+RESULTS:
  |   461115251 | /tmp/log_10000  |
  |   938018122 | /tmp/log_20000  |
  |  1907231506 | /tmp/log_40000  |
  Run the same command but with =–log=root.:trace= instead (to have the full log, not only =smpi=). Size 40000 is not done,
  because this takes a lot of time with the full log.
  #+begin_src sh
  du -b /tmp/log_*
  #+end_src
  #+RESULTS:
  | 18928787862 | /tmp/log_10000 |
  | 38761414633 | /tmp/log_20000 |
- It is clear that the sizes of the traces and the logs are linear in the size of the matrix. However, it is not clear
  why.
**** Getting a Debian 9 image on G5K                                 :G5K:
- There is no Debian 9 environments available on Grid 5000. Let’s build our own.
- Deploy and connect to a Debian 8:
  #+begin_src sh
  oarsub -I -l nodes=1,walltime=7 -t deploy
  kadeploy3 -f $OAR_NODE_FILE -e jessie-x64-big -k
  ssh root@address_of_the_machine
  #+end_src
- Upgrade it (following [[https://linuxconfig.org/how-to-upgrade-debian-8-jessie-to-debian-9-stretch][this tutorial]]).
  #+begin_src sh
  sed -i 's/jessie/stretch/g' /etc/apt/sources.list
  apt update
  apt upgrade # when asked to choose something, always take the default
  apt full-upgrade
  #+end_src
- Then, (from the outside of the machine), grab the OS:
  #+begin_src sh
  ssh root@genepi-8.grenoble.grid5000.fr tgz-g5k > /home/tocornebize/debian_9.tgz
  #+end_src
- Create an environment file like this:
  #+begin_example
  ---
  name: stretch-x64-big
  version: 2017041209
  description: Debian Stretch (big)
  author: tom.cornebize@inria.fr
  visibility: private
  destructive: false
  os: linux
  image:
    file: /home/tocornebize/debian_9.tgz
    kind: tar
    compression: gzip
  boot:
    kernel: "/vmlinuz"
    initrd: "/initrd.img"
  filesystem: ext4
  partition_type: 131
  multipart: false
  #+end_example
- Deploy it:
  #+begin_src sh
  oarsub -I -l nodes=1,walltime=7 -t deploy
  kadeploy3 -f $OAR_NODE_FILE -a mystretch-64-base.env
  #+end_src
  And it fails:
  #+begin_example
  Deployment #D-d905a1f1-9abe-4006-a1b8-aac550dd5657 started
  Grab the tarball file /home/tocornebize/debian_9.tgz
  Launching a deployment on genepi-6.grenoble.grid5000.fr
  Performing a Deploy[SetDeploymentEnvUntrusted] step
    switch_pxe
    reboot
     * Performing a soft reboot on genepi-6.grenoble.grid5000.fr
    wait_reboot
    create_partition_table
    format_deploy_part
    mount_deploy_part
    format_swap_part
  End of step Deploy[SetDeploymentEnvUntrusted] after 95s
  Performing a Deploy[BroadcastEnvKascade] step
    send_environment
     * Broadcast time: 132s
    manage_admin_post_install
    manage_user_post_install
    check_kernel_files
    install_bootloader
    sync
  End of step Deploy[BroadcastEnvKascade] after 158s
  Performing a Deploy[BootNewEnvKexec] step
    switch_pxe
    umount_deploy_part
    mount_deploy_part
    kexec
    wait_reboot
  Performing a Deploy[BootNewEnvHardReboot] step
    switch_pxe
    reboot
     * Performing a hard reboot on genepi-6.grenoble.grid5000.fr
    wait_reboot
  End of deployment on cluster genepi after 620s
  Deployment #D-d905a1f1-9abe-4006-a1b8-aac550dd5657 done
  
  The deployment failed on nodes
  genepi-6.grenoble.grid5000.fr (BootNewEnvHardReboot-wait_reboot: Unreachable after the reboot)
  #+end_example
  Don’t know why this happened.
**** TODO Future work [1/4]
:LOGBOOK:
- State "TODO"       from "TODO"       [2017-04-18 Tue 13:11]
- State "TODO"       from              [2017-04-14 Fri 18:29]
:END:
- [ ] Complete the Perl script from Arnaud and try it.
- [X] Run a Debian 9 on G5K.
- [ ] Apply the patch from Samuel on a G5K machine.
- [ ] Try HPL on the patched kernel.
*** 2017-04-18 Tuesday
**** Getting a Debian 9 image on G5K (bis)                           :G5K:
- Get Christian’s image.
- Modify the =.env= file to replace his username.
- Deploy:
  #+begin_src sh
  oarsub -I -l nodes=1,walltime=7 -t deploy
  kadeploy3 -f $OAR_NODEFILE -a debian_stretch.env -k
  #+end_src
- Note, some useful commands:
  #+begin_src sh
  oarstat # list all jobs
  oarsub -C <job_id> # connect to an existing job, then you can re-run kadeploy3
  #+end_src
- Alternative, without interactive oarsub:
  #+begin_src sh
  oarsub -t deploy -l "{network_address='nova-1.lyon.grid5000.fr'},walltime=1:55" -r "$(date '+%Y-%m-%d %H:%M:%S')"
  #+end_src
  Then connect with =oarsub -C= to the given job ID.
**** Patch the Linux kernel on G5K                 :PROGRAMMING:TOOLS:G5K:
- Get [[file:kernel_patch/dev_garbage][the patch]] written by Samuel.
- Get and fix the sources.
  #+begin_src sh
  cd /tmp # not enough space in /root repository in G5K...
  apt source linux
  cd linux-*
  cp ../dev_garbage debian/patches/ # copy the patch
  echo dev_garbage >> debian/patches/series
  #+end_src
- Apply the patch.
  #+begin_src sh
  apt install quilt
  dh_quilt_patch
  cp /boot/config-4.9.0-2-amd64 .config # get the config (the version needs to be the same)
  make -j 16 deb-pkg
  dpkg -i ../linux-header-*.deb # not sure if this is required
  dpkg -i ../linux-image-*.deb
  #+end_src
- The last command fails.
*** 2017-04-19 Wednesday
**** Fail to launch the Debian image on a node not in Lyon           :G5K:
- Both Christian’s image and my updated image cannot be deployed in Grenoble:
  #+begin_example
  tocornebize@fgrenoble:~$ oarsub -t deploy -l nodes=1,walltime=3 -r "$(date '+%Y-%m-%d %H:%M:%S')"
  [ADMISSION RULE] Reservation starts in less than one hour, not limited
  [ADMISSION RULE] Modify resource description with type constraints
  [ADMISSION RULE] Warning: your reservation starts very soon, you might get less nodes than what you required!
  Generate a job key...
  OAR_JOB_ID=1748552
  Reservation mode : waiting validation...
  Reservation valid --> OK
  tocornebize@fgrenoble:~$ oarsub -C 1748552
  Connect to OAR job 1748552 via the node fgrenoble.grenoble.grid5000.fr
  #+end_example
  #+begin_example
  tocornebize@fgrenoble:~$ kadeploy3 -k -f $OAR_NODEFILE -a debian_stretch.env
  Deployment #D-4f2095fd-7944-4cbf-a62f-405bf70ef5b2 started
  Grab the key file /home/tocornebize/.ssh/authorized_keys
  Grab the tarball file /home/tocornebize/debian_stretch_updated_v2.tgz
  Grab the postinstall file /home/tocornebize/debian_stretch-x64-prod-1.1-post.tgz
  Launching a deployment on genepi-4.grenoble.grid5000.fr
  Performing a Deploy[SetDeploymentEnvUntrusted] step
    switch_pxe
    reboot
     * Performing a soft reboot on genepi-4.grenoble.grid5000.fr
     * Performing a hard reboot on genepi-4.grenoble.grid5000.fr
    wait_reboot
    send_key_in_deploy_env
    create_partition_table
    format_deploy_part
    mount_deploy_part
    format_swap_part
  End of step Deploy[SetDeploymentEnvUntrusted] after 107s
  Performing a Deploy[BroadcastEnvKascade] step
    send_environment
     * Broadcast time: 140s
    manage_admin_post_install
    manage_user_post_install
    check_kernel_files
    send_key
    install_bootloader
    sync
  End of step Deploy[BroadcastEnvKascade] after 169s
  Performing a Deploy[BootNewEnvKexec] step
    switch_pxe
    umount_deploy_part
    mount_deploy_part
    kexec
    wait_reboot
  Performing a Deploy[BootNewEnvHardReboot] step
    switch_pxe
    reboot
     * Performing a hard reboot on genepi-4.grenoble.grid5000.fr
    wait_reboot
  End of deployment on cluster genepi after 644s
  Deployment #D-4f2095fd-7944-4cbf-a62f-405bf70ef5b2 done

  The deployment failed on nodes
  genepi-4.grenoble.grid5000.fr (BootNewEnvHardReboot-wait_reboot: Unreachable after the reboot)
  #+end_example
- Christian’s image can be deployed in Lyon:
  #+begin_example
  tocornebize@flyon:~$ oarsub -t deploy -l nodes=1,walltime=3 -r "$(date '+%Y-%m-%d %H:%M:%S')"
  [ADMISSION RULE] Reservation starts in less than one hour, not limited
  [ADMISSION RULE] Modify resource description with type constraints
  [ADMISSION RULE] Warning: your reservation starts very soon, you might get less nodes than what you required!
  Generate a job key...
  OAR_JOB_ID=858584
  Reservation mode : waiting validation...
  Reservation valid --> OK
  tocornebize@flyon:~$ oarsub -C 858584
  Connect to OAR job 858584 via the node frontend.lyon.grid5000.fr
  #+end_example
  #+begin_example
  tocornebize@flyon:~$ kadeploy3 -k -f $OAR_NODEFILE -a debian_stretch.env
  Deployment #D-c04ab7a9-bcfa-4bed-b036-57b6c14256e0 started
  Grab the key file /home/tocornebize/.ssh/authorized_keys
  Grab the tarball file /home/tocornebize/debian_stretch_updated_v2.tgz
  Grab the postinstall file /home/tocornebize/debian_stretch-x64-prod-1.1-post.tgz
  Launching a deployment on nova-2.lyon.grid5000.fr
  Performing a Deploy[SetDeploymentEnvUntrusted] step
    switch_pxe
    reboot
     * Performing a soft reboot on nova-2.lyon.grid5000.fr
    wait_reboot
    send_key_in_deploy_env
    create_partition_table
    format_deploy_part
    mount_deploy_part
    format_swap_part
  End of step Deploy[SetDeploymentEnvUntrusted] after 176s
  Performing a Deploy[BroadcastEnvKascade] step
    send_environment
     * Broadcast time: 75s
    manage_admin_post_install
    manage_user_post_install
    check_kernel_files
    send_key
    install_bootloader
    sync
  End of step Deploy[BroadcastEnvKascade] after 96s
  Performing a Deploy[BootNewEnvKexec] step
    switch_pxe
    umount_deploy_part
    mount_deploy_part
    kexec
    wait_reboot
  End of step Deploy[BootNewEnvKexec] after 129s
  End of deployment for nova-2.lyon.grid5000.fr after 401s
  End of deployment on cluster nova after 401s
  Deployment #D-c04ab7a9-bcfa-4bed-b036-57b6c14256e0 done

  The deployment is successful on nodes
  nova-2.lyon.grid5000.fr
  #+end_example
- The updated version also:
  #+begin_example
  tocornebize@flyon:~$ oarsub -t deploy -l nodes=1,walltime=3 -r "$(date '+%Y-%m-%d %H:%M:%S')"
  [ADMISSION RULE] Reservation starts in less than one hour, not limited
  [ADMISSION RULE] Modify resource description with type constraints
  [ADMISSION RULE] Warning: your reservation starts very soon, you might get less nodes than what you required!
  Generate a job key...
  OAR_JOB_ID=858594
  Reservation mode : waiting validation...
  Reservation valid --> OK
  tocornebize@flyon:~$ oarsub -C 858594
  Connect to OAR job 858594 via the node frontend.lyon.grid5000.fr
  #+end_example
  #+begin_example
  tocornebize@flyon:~$ kadeploy3 -k -f $OAR_NODEFILE -a debian_stretch_april.env
  Deployment #D-9e37642d-d5ef-4633-aa36-8080c040dd4a started
  Grab the key file /home/tocornebize/.ssh/authorized_keys
  Grab the tarball file /home/tocornebize/debian_stretch_april.tgz
  Grab the postinstall file /home/tocornebize/debian_stretch-x64-prod-1.1-post.tgz
  Launching a deployment on nova-22.lyon.grid5000.fr
  Performing a Deploy[SetDeploymentEnvUntrusted] step
    switch_pxe
    reboot
     * Performing a soft reboot on nova-22.lyon.grid5000.fr
    wait_reboot
    send_key_in_deploy_env
    create_partition_table
    format_deploy_part
    mount_deploy_part
    format_swap_part
  End of step Deploy[SetDeploymentEnvUntrusted] after 181s
  Performing a Deploy[BroadcastEnvKascade] step
    send_environment
     * Broadcast time: 83s
    manage_admin_post_install
    manage_user_post_install
    check_kernel_files
    send_key
    install_bootloader
    sync
  End of step Deploy[BroadcastEnvKascade] after 104s
  Performing a Deploy[BootNewEnvKexec] step
    switch_pxe
    umount_deploy_part
    mount_deploy_part
    kexec
    wait_reboot
  End of step Deploy[BootNewEnvKexec] after 76s
  End of deployment for nova-22.lyon.grid5000.fr after 361s
  End of deployment on cluster nova after 362s
  Deployment #D-9e37642d-d5ef-4633-aa36-8080c040dd4a done

  The deployment is successful on nodes
  nova-22.lyon.grid5000.fr
  #+end_example
  #+begin_example
  tocornebize@flyon:~$ ssh root@nova-22.lyon.grid5000.fr "uname -a"
  Linux nova-22.lyon.grid5000.fr 4.9.0-2-amd64 #1 SMP Debian 4.9.18-1 (2017-03-30) x86_64 GNU/Linux
  #+end_example
**** Retry patching the kernel                     :PROGRAMMING:TOOLS:G5K:
- Run all the commands before the last =dpkg -i=.
- Then, try installing =linux-image=:
  #+begin_example
  % dpkg -i ../linux-image-4.9.18_4.9.18-1_amd64.deb                                                                                                                                        -- INSERT -- 10:57:17
  Selecting previously unselected package linux-image-4.9.18.
  (Reading database ... 168140 files and directories currently installed.)
  Preparing to unpack .../linux-image-4.9.18_4.9.18-1_amd64.deb ...
  Unpacking linux-image-4.9.18 (4.9.18-1) ...
  Setting up linux-image-4.9.18 (4.9.18-1) ...
  Error! Your kernel headers for kernel 4.9.18 cannot be found.
  Please install the linux-headers-4.9.18 package,
  or use the --kernelsourcedir option to tell DKMS where it's located
  update-initramfs: Generating /boot/initrd.img-4.9.18
  W: Possible missing firmware /lib/firmware/tigon/tg3_tso5.bin for module tg3
  W: Possible missing firmware /lib/firmware/tigon/tg3_tso.bin for module tg3
  W: Possible missing firmware /lib/firmware/tigon/tg3.bin for module tg3
  cryptsetup: WARNING: failed to detect canonical device of /dev/sda3
  cryptsetup: WARNING: could not determine root device from /etc/fstab
  W: mdadm: /etc/mdadm/mdadm.conf defines no arrays.
  #+end_example
- Ok, let’s install =linux-headers=:
  #+begin_example
  % dpkg -i ../linux-headers-4.9.18_4.9.18-1_amd64.deb                                                                                                                                      -- INSERT -- 10:57:53
  Selecting previously unselected package linux-headers-4.9.18.
  (Reading database ... 172304 files and directories currently installed.)
  Preparing to unpack .../linux-headers-4.9.18_4.9.18-1_amd64.deb ...
  Unpacking linux-headers-4.9.18 (4.9.18-1) ...
  Setting up linux-headers-4.9.18 (4.9.18-1) ...
  #+end_example
- It seems to work. Let’s retry =linux-image=:
  #+begin_example
  % dpkg -i ../linux-image-4.9.18_4.9.18-1_amd64.deb                                                                                                                                        -- INSERT -- 11:00:10
  (Reading database ... 196297 files and directories currently installed.)
  Preparing to unpack .../linux-image-4.9.18_4.9.18-1_amd64.deb ...
  Unpacking linux-image-4.9.18 (4.9.18-1) over (4.9.18-1) ...
  Setting up linux-image-4.9.18 (4.9.18-1) ...
  Error! Bad return status for module build on kernel: 4.9.18 (x86_64)
  Consult /var/lib/dkms/nvidia/346.22/build/make.log for more information.
  update-initramfs: Generating /boot/initrd.img-4.9.18
  W: Possible missing firmware /lib/firmware/tigon/tg3_tso5.bin for module tg3
  W: Possible missing firmware /lib/firmware/tigon/tg3_tso.bin for module tg3
  W: Possible missing firmware /lib/firmware/tigon/tg3.bin for module tg3
  cryptsetup: WARNING: failed to detect canonical device of /dev/sda3
  cryptsetup: WARNING: could not determine root device from /etc/fstab
  W: mdadm: /etc/mdadm/mdadm.conf defines no arrays.
  #+end_example
- Strange, why is there something related to nvidia?
  See the [[file:kernel_patch/make.log][log file]] =/var/lib/dkms/nvidia/346.22/build/make.log=.
  The first lines of this file are:
  #+begin_example
  DKMS make.log for nvidia-346.22 for kernel 4.9.18 (x86_64)
  Wed Apr 19 11:01:00 CEST 2017
  NVIDIA: calling KBUILD...
  make[1]: Entering directory '/usr/src/linux-headers-4.9.18'
  test -e include/generated/autoconf.h -a -e include/config/auto.conf || (                \
  echo >&2;                                                       \
  echo >&2 "  ERROR: Kernel configuration is invalid.";           \
  echo >&2 "         include/generated/autoconf.h or include/config/auto.conf are missing.";\
  echo >&2 "         Run 'make oldconfig && make prepare' on kernel src to fix it.";      \
  echo >&2 ;                                                      \
  /bin/false)
  #+end_example
**** Found a bug in Simgrid privatization with =dlopen=           :SMPI:BUG:
- Issue reported on [[https://github.com/simgrid/simgrid/issues/157][Github]].
*** 2017-04-20 Thursday
**** What happens if we remove BLAS functions? :SMPI:EXPERIMENTS:PERFORMANCE:HPL:
- Simgrid commit: =23a4a825789444c7bc954b0075fdd23715facaf5=
- HPL commit: =b779f90ce2d0f65a094c11fed1020724f5dee05c= (with the modifications described below)
- The aim of the Perl script is to remove all the BLAS functions, to hopefully avoid any access to the matrix and thus
  all the page faults.
- Let’s see what happens if we just replace the current calls by a no-op.
- For P=Q=8 and N=40000, the simulation time is 50.338 seconds (28.307 spent in the application). HPL predicts 81.8 Gflops.
  Using the command:
  #+begin_src sh
  smpirun -wrapper "perf stat" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np 64 -hostfile ./hostfile_64.txt
  -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
  The command =perf stat= says:
  #+begin_example
     47896,342289      task-clock (msec)         #    0,932 CPUs utilized          
              502      context-switches          #    0,010 K/sec                  
              150      cpu-migrations            #    0,003 K/sec                  
       16 183 906      page-faults               #    0,338 M/sec                  
  151 748 071 102      cycles                    #    3,168 GHz                    
  <not supported>      stalled-cycles-frontend  
  <not supported>      stalled-cycles-backend   
  176 488 478 850      instructions              #    1,16  insns per cycle        
   31 040 694 031      branches                  #  648,081 M/sec                  
      255 287 762      branch-misses             #    0,82% of all branches        

     51,392876085 seconds time elapsed
  #+end_example
- If we remove =cblas_dcopy=, there is a deadlock.
- If we remove all the other BLAS functions, the simulation times decreases to 44.517 seconds (22.853 spent in the
  application). HPL predicts 83.8 Gflops.
  The command =perf stat= says:
  #+begin_example
     41712,302269      task-clock (msec)         #    0,916 CPUs utilized          
              688      context-switches          #    0,016 K/sec                  
              158      cpu-migrations            #    0,004 K/sec                  
       14 730 028      page-faults               #    0,353 M/sec                  
  132 202 044 723      cycles                    #    3,169 GHz                    
  <not supported>      stalled-cycles-frontend  
  <not supported>      stalled-cycles-backend   
  158 208 269 626      instructions              #    1,20  insns per cycle        
   27 938 931 890      branches                  #  669,801 M/sec                  
      236 048 411      branch-misses             #    0,84% of all branches        

     45,517042870 seconds time elapsed
  #+end_example
- Summary:
  | Metric           | With BLAS | Without BLAS | Difference |
  |------------------+-----------+--------------+------------|
  | Cycles           | 151.748e9 |    132.202e9 |    -12.88% |
  | Instructions     | 176.488e9 |    158.208e9 |    -10.36% |
  | Page faults      |  16.184e6 |     14.730e6 |     -8.98% |
  | Time             |    50.338 |       44.517 |    -11.56% |
  | Application time |    28.307 |       22.853 |    -19.26% |
- The difference in the time spent in the application is non-negligible, so it seems to be worth removing the other BLAS
  functions. However, the difference in the number of page faults is not so important: there is still room for
  improvement if we can remove the remaining memory accesses. Maybe a first step would be this =cblas_dcopy=?
- Running the same thing with N=20000 gives a simulation time of 16.881 seconds with 7.833 seconds spent in the
  application with BLAS, and 15.299 seconds with 6.484 seconds spent in the application without BLAS. Note that the time
  spent in the application still looks to be quadratic in the size. The number of page faults in both cases is
  respectively 3.975e6 and 3.731e6, which seems to be quadratic in the size.
  Outputs of =perf stat=:
  + Without the BLAS functions:
    #+begin_example
           15638,734454      task-clock (msec)         #    0,997 CPUs utilized          
              1 235      context-switches          #    0,079 K/sec                  
                164      cpu-migrations            #    0,010 K/sec                  
          3 731 741      page-faults               #    0,239 M/sec                  
     49 508 706 603      cycles                    #    3,166 GHz                    
    <not supported>      stalled-cycles-frontend  
    <not supported>      stalled-cycles-backend   
     58 854 519 915      instructions              #    1,19  insns per cycle        
     11 306 187 930      branches                  #  722,961 M/sec                  
        111 373 194      branch-misses             #    0,99% of all branches        

       15,692032447 seconds time elapsed
    #+end_example
- With the BLAS functions:
  #+begin_example
     17275,202243      task-clock (msec)         #    1,000 CPUs utilized          
              527      context-switches          #    0,031 K/sec                  
              170      cpu-migrations            #    0,010 K/sec                  
        3 975 764      page-faults               #    0,230 M/sec                  
   54 623 876 641      cycles                    #    3,162 GHz                    
  <not supported>      stalled-cycles-frontend  
  <not supported>      stalled-cycles-backend   
   64 072 916 264      instructions              #    1,17  insns per cycle        
   12 201 200 437      branches                  #  706,284 M/sec                  
      117 474 919      branch-misses             #    0,96% of all branches        

     17,282143071 seconds time elapsed
  #+end_example
**** How many page faults for unmodified HPL? :SMPI:EXPERIMENTS:PERFORMANCE:HPL:
- Simgrid commit: =23a4a825789444c7bc954b0075fdd23715facaf5=
- HPL commit: =b779f90ce2d0f65a094c11fed1020724f5dee05c= (with the modifications described below)
- Let’s try the same experiment than above, but with *unmodified* HPL and N=20000, we get a simulation time of 516 seconds
  with 503 seconds spent in the application. The command =perf stat= gives the following:
  #+begin_example
      516367,371201      task-clock (msec)         #    1,000 CPUs utilized          
              4 196      context-switches          #    0,008 K/sec                  
                175      cpu-migrations            #    0,000 K/sec                  
             78 600      page-faults               #    0,152 K/sec                  
  1 636 853 031 635      cycles                    #    3,170 GHz                    
    <not supported>      stalled-cycles-frontend  
    <not supported>      stalled-cycles-backend   
  5 105 655 137 040      instructions              #    3,12  insns per cycle        
     47 868 519 045      branches                  #   92,702 M/sec                  
        694 142 882      branch-misses             #    1,45% of all branches        
 
      516,600732884 seconds time elapsed
  #+end_example
  Note the number of page faults, it is more than 40 times higher with the “optimized” HPL.
- Let’s see what happens with a subset of the optimizations.
  + With the =PARTIAL_SHARED_MALLOC= for the matrix and the panels but without =cblas_dgemm= and =cblas_dtrsm=:
    73 314 page faults.
  + With the =PARTIAL_SHARED_MALLOC= for the panels but not the matrix and without =cblas_dgemm= and =cblas_dtrsm=:
    3 266 513 page faults.
  + With the =PARTIAL_SHARED_MALLOC= for the matrix but not the panels and without =cblas_dgemm= and =cblas_dtrsm=:
    821 425 page faults.
  We can suppose that the panels are more used than the matrix (a part of the matrix is copied in the panel, then all
  the operations are done on the panel). So all these page faults clearly come from “how much” the memory blocks
  obtained with shared malloc are used.
*** 2017-04-21 Friday
**** Number of page faults for different allocations and usages :C:R:EXPERIMENTS:PERFORMANCE:
- We would like to know how much page faults are done with =malloc= and =shared_malloc=, when the memory is actually
  accessed or not. We use a small C program that takes three arguments: =shared=, =size= and =access=. It allocates a memory
  block of given size. When =shared= is =0=, the allocation is done with =malloc=, when it is =1=, the allocation is done with
  the same mechanism than =SMPI_SHARED_MALLOC= (folded memory, using =mmap=). When =access= is =1=, the whole memory block is
  written with =memset=, otherwise it is not touched.
- Script commit: =9f559fcfc90c2d292c65e1d3bdbd7a5238687593=
- Command:
  #+begin_src sh
  gcc -std=gnu11 -O3 -o page_faults page_faults.c -Wall
  ./page_faults.py 100 10000000000 page_faults/results.csv
  #+end_src
  We run 100 experiments. For each of them, the size of the allocation (in bytes) is sampled randomly and uniformly in
  [1, 1e10]. Then, the two modes of allocation are tested, with and without a memory access.
  To count the number of page faults, we use =/usr/bin/time=.
- Files (name are made with the machine name and the kernel version, obtained with =uname -r=):
  + [[file:page_faults/results_laptop_4.4.0-72-generic.csv]] (obtained on my laptop)
  + [[file:page_faults/results_edel-12_3.16.0-4-amd64.csv]] (obtained on edel-12@grenoble node, using =jessie-x64-big= image)
  + [[file:page_faults/results_edel-17_3.16.0-4-amd64.csv]] (obtained on edel-17@grenoble node, using =jessie-x64-big= image)
- The results for =edel-12= and =edel-17= are very similar, so only one of them will be discussed.
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results_44 = read.csv('page_faults/results_laptop_4.4.0-72-generic.csv')
  results_316 = read.csv('page_faults/results_edel-17_3.16.0-4-amd64.csv')
  head(results_44)
  #+end_src

  #+RESULTS:
  :   shared       size mem_access system_time user_time nb_page_faults
  : 1  False 4642096513       True        0.43      0.21           3053
  : 2   True 4642096513       True        1.16      0.38        1208906
  : 3   True 4642096513      False        0.20      0.00          75577
  : 4  False 4642096513      False        0.00      0.00             60
  : 5  False 4804700804       True        0.48      0.21           2894
  : 6   True 4804700804      False        0.23      0.00          78218

  #+begin_src R :file page_faults/1.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=nb_page_faults, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096, alpha=0.3)+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/1.png]]

  #+begin_src R :file page_faults/2.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=user_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("User time for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/2.png]]

  #+begin_src R :file page_faults/3.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=system_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("System time for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/3.png]]

  #+begin_src R :file page_faults/4.png :results value graphics :results output :session *R* :exports both
  ggplot(results_316, aes(x=size, y=nb_page_faults, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096, alpha=0.3)+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different allocation sizes\nKernel 3.16.0-4-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/4.png]]

  #+begin_src R :file page_faults/5.png :results value graphics :results output :session *R* :exports both
  ggplot(results_316, aes(x=size, y=user_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("User time for different allocation sizes\nKernel Kernel 3.16.0-4-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/5.png]]

  #+begin_src R :file page_faults/6.png :results value graphics :results output :session *R* :exports both
  ggplot(results_316, aes(x=size, y=system_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("System time for different allocation sizes\nKernel Kernel 3.16.0-4-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/6.png]]

- There are several interesting things to notice on these plots.
  + For the shared =malloc=, with both kernels, the number of page faults increases linearly with the size, both when the
    memory is accessed or not. However, this number of page faults is higher when the memory is accessed.  This means
    that removing all the BLAS functions (and even other functions accessing the matrix and the panel) may not be enough
    for large matrix sizes. When the memory is not accessed, the number of page faults does not seem to depend on the
    block size of the folding, so I do not understand what causes the page faults here.
  + For the classical =malloc=, the number of page faults remains constant if the memory is not accessed, for both
    kernels. When the memory is accessed, for kernel =3.16=, the number of page faults increases linearly with the size,
    but it remains constant with kernel =4.4=. It seems that Linux developers improved the performances of =malloc= between
    these two kernel versions.
  + The number of page faults is very significantly higher with a shared =malloc= than with a classical =malloc=.
  + For the shared =malloc=, with both kernels, the system time is proportionnal to the size. We could guess that all the
    time is spent on page faults. For the classical =malloc=, as expected the system time is constant when there is no
    memory access. But strangely, the system time is proportional to the size, even for kernel 4.4 (where the number of
    page faults remains constant).
**** DONE Future work for page faults [2/2]      :EXPERIMENTS:PERFORMANCE:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-04-24 Mon 17:00]
- State "TODO"       from "TODO"       [2017-04-24 Mon 13:35]
- State "TODO"       from              [2017-04-21 Fri 18:56]
:END:
- [X] Experiment with several sequential accesses to the memory block. For shared =malloc=, it seems that the number of
  page faults and the system time do not increase. This would be a strong argument in favor of re-using the panels,
  instead of allocating new ones at each iteration.
- [X] Re-do the experiment with a new kernel. Try Debian 9 on G5K.
*** 2017-04-24 Monday
**** Number of page faults for different allocations and usages (bis) :C:R:EXPERIMENTS:PERFORMANCE:
- Let’s redo the experiment.
- Script commit: =9f559fcfc90c2d292c65e1d3bdbd7a5238687593=
- Command:
  #+begin_src sh
  gcc -std=gnu11 -O3 -o page_faults page_faults.c -Wall
  ./page_faults.py 100 10000000000 page_faults/results.csv
  #+end_src
- Files:
  + [[file:page_faults/results_laptop_4.4.0-72-generic_2.csv]] (new one obtained on my laptop)
  + [[file:page_faults/results_orion-4_4.9.0-2-amd64.csv]] (obtained on orion-4@lyon node, using =debian_stretch_april= image
    (Debian 9 from Christian, updated in April))
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results_44 = read.csv('page_faults/results_laptop_4.4.0-72-generic_2.csv')
  results_49 = read.csv('page_faults/results_orion-4_4.9.0-2-amd64.csv')
  head(results_44)
  #+end_src

  #+RESULTS:
  :   shared       size mem_access system_time user_time nb_page_faults
  : 1  False 4601230797       True        0.37      0.18           2785
  : 2   True 4601230797      False        0.16      0.00          74916
  : 3   True 4601230797       True        0.99      0.26        1198265
  : 4  False 4601230797      False        0.00      0.00             60
  : 5  False 9491592478      False        0.00      0.00             60
  : 6   True 9491592478       True        2.04      0.55        2471486

  #+begin_src R :file page_faults/7.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=nb_page_faults, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096, alpha=0.3)+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/7.png]]

  #+begin_src R :file page_faults/8.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=user_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("User time for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/8.png]]

  #+begin_src R :file page_faults/9.png :results value graphics :results output :session *R* :exports both
  ggplot(results_44, aes(x=size, y=system_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("System time for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/9.png]]

  #+begin_src R :file page_faults/10.png :results value graphics :results output :session *R* :exports both
  ggplot(results_49, aes(x=size, y=nb_page_faults, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096, alpha=0.3)+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different allocation sizes\nKernel 4.9.0-2-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/10.png]]

  #+begin_src R :file page_faults/11.png :results value graphics :results output :session *R* :exports both
  ggplot(results_49, aes(x=size, y=user_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("User time for different allocation sizes\nKernel 4.9.0-2-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/11.png]]

  #+begin_src R :file page_faults/12.png :results value graphics :results output :session *R* :exports both
  ggplot(results_49, aes(x=size, y=system_time, linetype=mem_access, color=shared)) +
      geom_line(size=1)+
      geom_abline(intercept=0, slope=1/4096)+
      expand_limits(x=0, y=0)+
      ggtitle("System time for different allocation sizes\nKernel 4.9.0-2-amd64")
  #+end_src

  #+RESULTS:
  [[file:page_faults/12.png]]

- These results are very surprising.
  + The results done on my laptop are very different. I cannot even reproduce the previous results.
  + The results on kernel 4.9 look very similar to the results on kernel 3.16. Thus, it seems that the hypothesis of a
    great performance improvement in =malloc= is wrong.
  + The results on my laptop are so different than the ones on G5K, or even the ones on my laptop from last week. This
    suggests a change in the OS configuration, but I have no idea of what.
  + Tried to play with =sysctl vm.overcommit_memory= and =sysctl vm.max_map_count= to see if this changes something, without success.
  + At least the results for the shared =malloc= look more stable than the ones for the classical =malloc=.
- In all, the results for classical =malloc= are very strange, but maybe we do not care, since we are interested in the
  shared =malloc=.
**** Number of page faults in HPL, on G5K :SMPI:EXPERIMENTS:PERFORMANCE:HPL:
- Trying to reproduce results from [2017-04-20 Thu] on G5K.
- Node: =orion-4.lyon.grid5000.fr=
- Environment: =debian_stretch_april.env= (kernel =4.9.0-2-amd64=)
- Simgrid commit: =23a4a825789444c7bc954b0075fdd23715facaf5=
- HPL commit: =b779f90ce2d0f65a094c11fed1020724f5dee05c= (with the modifications described below)
- P=Q=8 and N=20000
- Using the command:
  #+begin_src sh
  smpirun -wrapper "perf stat" --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969
  --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np 64 -hostfile ./hostfile_64.txt
  -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- With =SMPI_OPTIMIZATION= option, the simulation time is 26.778 seconds (10.079 spent in the application).
  The command =perf stat= says:
  #+begin_example
    26063.474000      task-clock (msec)         #    0.960 CPUs utilized          
             498      context-switches          #    0.019 K/sec                  
              70      cpu-migrations            #    0.003 K/sec                  
       4,206,316      page-faults               #    0.161 M/sec                  
  72,950,402,645      cycles                    #    2.799 GHz                      (83.32%)
  45,431,918,376      stalled-cycles-frontend   #   62.28% frontend cycles idle     (83.34%)
  30,872,187,762      stalled-cycles-backend    #   42.32% backend cycles idle      (66.71%)
  66,491,409,525      instructions              #    0.91  insn per cycle         
                                                #    0.68  stalled cycles per insn  (83.35%)
  12,878,207,578      branches                  #  494.109 M/sec                    (83.34%)
     144,320,765      branch-misses             #    1.12% of all branches          (83.33%)

    27.155448146 seconds time elapsed
  #+end_example
  These numbers are similar to what I got on my laptop (same orders of magnitude).
- Without =SMPI_OPTIMIZATION= option (i.e. vanilla HPL), the simulation time is 652.706 seconds (630.602 spent in the application).
  The command =perf stat= says:
  #+begin_example
      652579.966607      task-clock (msec)         #    1.000 CPUs utilized          
                757      context-switches          #    0.001 K/sec                  
                 75      cpu-migrations            #    0.000 K/sec                  
            868,673      page-faults               #    0.001 M/sec                  
  1,821,806,871,088      cycles                    #    2.792 GHz                      (83.33%)
    431,818,214,600      stalled-cycles-frontend   #   23.70% frontend cycles idle     (83.33%)
    126,761,359,972      stalled-cycles-backend    #    6.96% backend cycles idle      (66.67%)
  5,111,582,144,523      instructions              #    2.81  insn per cycle         
                                                   #    0.08  stalled cycles per insn  (83.33%)
     52,614,144,664      branches                  #   80.625 M/sec                    (83.33%)
        701,800,669      branch-misses             #    1.33% of all branches          (83.33%)
 
      652.737011162 seconds time elapsed
  #+end_example
  Most of these numbers are also similar to what I got on my laptop (same orders of magnitude), *except* the number of
  page faults which is ten times higher (was 78,600).
  Thus, the unstable results observed for the number of page faults with =malloc= in the previous section is confirmed
  here, there is an important variability depending on the kernel.
**** Number of page faults for different number of accesses :C:R:EXPERIMENTS:PERFORMANCE:
- This time, we want to see the impact on the times and the number of page faults of several sequential accesses to the
  allocated memory. We will only test for the shared =malloc=. Also, the size of the block is constant (only the number of
  accesses is a variable).
- Script commit: =d21464d7870c9c182fdb83eb3555605333f93a10=
- Command:
  #+begin_src sh
  gcc -std=gnu11 -O3 -o page_faults page_faults.c -Wall
  ./page_faults_accesses.py 100 10000000000 100 page_faults/results.csv
  #+end_src
  We run 100 experiments. For each of them, the size of the allocation (in bytes) is 1e10. The number of memset calls
  (to the whole block) is sampled randomly and uniformly in [0, 100].  To count the number of page faults, we use
  =/usr/bin/time=.
- File:
  + [[file:page_faults/results_nb_access.csv]] (obtained on my laptop)
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results = read.csv('page_faults/results_nb_access.csv')
  head(results)
  #+end_src

  #+RESULTS:
  :    size nb_access system_time user_time nb_page_faults
  : 1 1e+10        68        4.10     24.30        5045258
  : 2 1e+10        72        3.93     25.69        5045259
  : 3 1e+10        96        3.92     33.74        5035721
  : 4 1e+10        71        2.13     24.67        2603852
  : 5 1e+10        40        2.24     14.04        2603851
  : 6 1e+10        66        2.18     22.90        2603850

  #+begin_src R :file page_faults/13.png :results value graphics :results output :session *R* :exports both
  ggplot(results, aes(x=nb_access, y=nb_page_faults)) +
      geom_point(shape=1)+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different number of accesses\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/13.png]]

  #+begin_src R :file page_faults/14.png :results value graphics :results output :session *R* :exports both
  library(reshape2) # nice tutorial: http://seananderson.ca/2013/10/19/reshape.html
  melted_results = melt(results[c("nb_access", "system_time", "user_time")], id.vars = c("nb_access"), variable.name="time_type", value.name="time")
  ggplot(melted_results, aes(x=nb_access, y=time, color=time_type)) +
      geom_point(shape=1)+
      expand_limits(x=0, y=0)+
      ggtitle("Times for different number of accesses\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/14.png]]

- Two points (one on the other, so we see only one) have a number of accesses equal to 0. Thus, the number of page faults
 and the times are much lower. These are not outliers.
- A great majority of experiments had approximately the same number of page faults and the same time. Thus we can
  conclude that the system time is not impacted by the number of accesses.
- There are several outliers which have an higher number of page faults and a higher system time.
- The user time is (as expected) linear in the number of accesses.
- It is interesting to note that the system time is larger than the user time for a few accesses, but the system time
  quickly becomes (nearly) negligible for larger number of accesses.
- We should try to re-use the panels in HPL. Maybe we should ask the developers why they did not do this in the original
  implementation?
*** 2017-04-25 Tuesday
**** Measuring new metrics in HPL :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Thanks to =/usr/bin/time=, we can have the user and system times, as well as the number of major and minor page faults.
- Let’s redo the experiments with the latest version of HPL/Simgrid (partial shared =malloc=, optimized communications).
- Simgrid commit: =743399f59d6ce57b0772a26cad2daba7e807b9e9=
- HPL commit: =b779f90ce2d0f65a094c11fed1020724f5dee05c= (compiled with =SMPI_OPTIMIZATION= option).
- Script commit: =c83938f77dec089b7d323f36ccd6822ee203e598=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl_partial_shared.csv --nb_runs 1 --size 100,5000,10000,15000,20000,25000,30000,35000,40000
  --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  results <- read.csv('hpl_analysis/hpl_more_metrics.csv')
  head(results)
  max(results$major_page_fault)
  #+end_src

  #+RESULTS:
  #+begin_example
         topology nb_roots nb_proc  size    time    Gflops simulation_time
  1 2;8,8;1,8;1,1        8      48 30000  279.48 64.410000       19.072000
  2 2;8,8;1,8;1,1        8      40 30000  369.90 48.670000       31.797100
  3 2;8,8;1,8;1,1        8      16 40000 1578.37 27.030000       24.827200
  4 2;8,8;1,8;1,1        8      48   100    0.13  0.005225        0.161541
  5 2;8,8;1,8;1,1        8      32  5000    8.57  9.725000        1.371540
  6 2;8,8;1,8;1,1        8       8 15000  176.74 12.730000        2.864450
    application_time user_time system_time major_page_fault minor_page_fault
  1        12.889000     12.08        6.68                0          6601272
  2        18.923600     21.73        9.82                0         11589634
  3        19.437700     15.08        9.50                0         10831122
  4         0.013471      0.04        0.13                0             2053
  5         0.502335      1.01        0.36                0           184305
  6         2.207640      1.82        1.02                0          1113011
         uss         rss
  1 18956288  7689560064
  2 16244736  7627075584
  3  8904704 13122576384
  4  9383936    13430784
  5 14274560   276819968
  6  6152192  1897504768
  [1] 0
  #+end_example

- Let’s first have a look at the number of page faults.
  Firstly, note that the number of major page faults is always 0.

  #+begin_src R :file hpl_analysis/40.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=size, y=minor_page_fault, group=nb_proc, color=nb_proc)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different matrix sizes")
  plot2 = ggplot(results, aes(x=nb_proc, y=minor_page_fault, group=size, color=size)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different number of processes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/40.png]]

  It is clear that the number of page faults is quadratic in the size of the matrix (already observed previously, with
  less data though).  Also, this number of page faults is impacted by the number of processes. It seems to be linear, with
  a lot of variation. This is another argument for the hypothesis that a large part of these page faults do not come from
  the matrix itself (otherwise the number of page faults would be constant in the number of processes), but from the
  panels.

- Now, let’s have a look at the system and user times.

  #+begin_src R :file hpl_analysis/41.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=size, y=user_time, group=nb_proc, color=nb_proc)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("User time for different matrix sizes")
  plot2 = ggplot(results, aes(x=nb_proc, y=user_time, group=size, color=size)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("User time for different number of processes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/41.png]]

  #+begin_src R :file hpl_analysis/42.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=size, y=system_time, group=nb_proc, color=nb_proc)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("System time for different matrix sizes")
  plot2 = ggplot(results, aes(x=nb_proc, y=system_time, group=size, color=size)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("System time for different number of processes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/42.png]]

  #+begin_src R :results output :session *R* :exports both
  library(reshape2)
  melted_results = melt(results[c("size", "nb_proc", "system_time", "user_time")], id.vars = c("size", "nb_proc"), variable.name="time_type", value.name="time")
  head(melted_results)
  #+end_src

  #+RESULTS:
  :    size nb_proc   time_type time
  : 1 30000      48 system_time 6.68
  : 2 30000      40 system_time 9.82
  : 3 40000      16 system_time 9.50
  : 4   100      48 system_time 0.13
  : 5  5000      32 system_time 0.36
  : 6 15000       8 system_time 1.02

  #+begin_src R :file hpl_analysis/43.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(melted_results[melted_results$nb_proc==64,], aes(x=size, y=time, group=time_type, color=time_type)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("System time for different matrix sizes\nNumber of processes: 64")
  plot2 = ggplot(melted_results[melted_results$size==40000,], aes(x=nb_proc, y=time, group=time_type, color=time_type)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("System time for different number of processes\nMatrix size: 40000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/43.png]]

  #+begin_src R :results output :session *R* :exports both
  results_big <- read.csv('hpl_analysis/hpl_more_metrics_big.csv')
  melted_results_big = melt(results_big[c("size", "nb_proc", "system_time", "user_time")], id.vars = c("size", "nb_proc"), variable.name="time_type", value.name="time")
  #+end_src

  #+begin_src R :file hpl_analysis/44.png :results value graphics :results output :session *R* :exports both
  ggplot(melted_results_big[melted_results_big$nb_proc==64,], aes(x=size, y=time, group=time_type, color=time_type)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("System time for different matrix sizes\nNumber of processes: 64")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/44.png]]

  Both times look to be quadratic in the size of the matrix and linear in the number of processes. They have the same
  order of magnitude.
- This shows that reusing the =PANEL->WORK= buffer might help a lot, by reducing very significantly the system time, but
  it will not be enough, since the user time will still be quadratic.
- Also, working on the BLAS functions might help. We saw in [2017-04-13 Thu] that only the application time is quadratic
  now. The best we could do is to remove all useless computations in HPL and get a linear system time. But we also saw
  in [2017-04-24 Mon] that the system time is linear in the size of the allocation (and thus quadratic in the size of
  the matrix), even when the memory block is *not* touched. So, even in the best case, we would still have a quadratic
  system time, although with a lower factor.
- Thus, to run HPL at the scales we want, it seems necessary to *both* reuse the panels and remove entirely the BLAS
  functions (and maybe some other costly functions).
*** 2017-04-26 Wednesday
**** Implementation of =PANEL->WORK= reuse                      :SMPI:C:HPL:
- As discussed previously, the aim is to decrease the time spent by the kernel in handling page faults.
- In HPL, buffers =PANEL->WORK= are allocated and freed at each iteration. Also, their size does not increase throughout
  the iterations, as shown on [2017-03-27 Mon].
- We chan therefore simply reuse the panels. Just keep global variables in =HPL_pdpanel_init.c=:
  #+begin_src c
  static size_t shared_size = 0;
  static size_t shared_start_private=0, shared_stop_private=0;
  static void *shared_ptr = NULL;
  #+end_src
  Then, each time an allocation is done, compare it with these variables. If the size of the allocation and the size of
  its private zone are lower than the size of the previous allocation and private zone, then we can keep the same
  buffer. Otherwise, we have to reallocate it.
  If we keep the old buffer, the pointer we return is not necessarily =shared_ptr=. Indeed, the new private zone must be
  included in the old private zone, so we return a shifted pointer.
- Quick test with N=40000, P=Q=8.
  + The buffer is reallocated 64 times, and reused 21376 times. It means that the second allocation of each process is
    the biggest. On the plot from [2017-03-27 Mon], there are two lines. A guess is that the first allocation is on the
    line below, the second allocation on the line above.
  + The different metrics are:
    | Metric           | Without panel reuse | With panel reuse | Difference |
    |------------------+---------------------+------------------+------------|
    | Simulation time  |               48.23 |            34.32 |    -28.84% |
    | Application time |               29.66 |            15.53 |    -47.64% |
    | System time      |               15.28 |             4.11 |    -73.10% |
    | User time        |               32.42 |            29.81 |     -8.05% |
    | Page faults      |            18121602 |          3507021 |    -80.65% |
    | Gflops           |               81.78 |            82.74 |      1.17% |
    #+TBLFM: $4=(($3-$2)/$2)*100;f2
  + By the way, there is a [[http://orgmode.org/worg/org-tutorials/org-spreadsheet-intro.html][nice tutorial]] for spreadsheets. No idea on how to put the “%” signs automatically, so did it
    by hand.
- This looks nice. We should make new plots.
**** Plots with panel reuse           :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- What is the impact of the last optimization on the times and numbers of page faults?
- Simgrid commit: =743399f59d6ce57b0772a26cad2daba7e807b9e9=
- HPL commit: =473b935f9951032133b8c9e65fd485507fda79d9= (compiled with =SMPI_OPTIMIZATION= option).
- Script commit: =c83938f77dec089b7d323f36ccd6822ee203e598=
- We compare what we had without the panel reuse with what we have now.

  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  library(reshape2)
  results_big <- read.csv('hpl_analysis/hpl_more_metrics_big.csv')
  results_big_reuse <- read.csv('hpl_analysis/hpl_more_metrics_big_panel_reuse.csv')
  results_big$panel_reuse = FALSE
  results_big_reuse$panel_reuse = TRUE
  results = rbind(results_big, results_big_reuse)
  #+end_src

  #+begin_src R :file hpl_analysis/45.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=size, y=simulation_time, group=panel_reuse, linetype=panel_reuse)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Simulation time for different matrix sizes\nNumber of processes: 64")
  plot2 = ggplot(results, aes(x=size, y=minor_page_fault, group=panel_reuse, linetype=panel_reuse)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different matrix sizes\nNumber of processes: 64")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/45.png]]

  #+begin_src R :file hpl_analysis/46.png :results value graphics :results output :session *R* :exports both
  melted_results = melt(results[c("size", "system_time", "user_time", "panel_reuse")], id.vars = c("size", "panel_reuse"), variable.name="time_type", value.name="time")
  ggplot(melted_results, aes(x=size, y=time, color=time_type, linetype=panel_reuse)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Times for different matrix sizes\nNumber of processes: 64")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/46.png]]

  #+begin_src R :file hpl_analysis/47.png :results value graphics :results output :session *R* :exports both
  results$simgrid_time = results$simulation_time - results$application_time
  melted_results = melt(results[c("size", "simgrid_time", "application_time", "panel_reuse")], id.vars = c("size", "panel_reuse"), variable.name="time_type", value.name="time")
  ggplot(melted_results, aes(x=size, y=time, color=time_type, linetype=panel_reuse)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Times for different matrix sizes\nNumber of processes: 64")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/47.png]]

- So the improvement looks pretty good. There is a small reduction of user time and a large reduction of system
  time. The system time was nearly as high as the user time, but now it is much lower (albeit still significantly
  high). Similarly, there is a high decrease of application time and a small one of simgrid time. This is very likely
  due to the large reduction of the number of page faults.
- Plots are closer to a straight line. However, everything still looks quadratic.
- Note that on [2017-04-13 Thu] the simgrid time looked to be linear. But maybe we did not measure for large enough
  sizes (for the sizes we considered, maybe the term in n was dominant on the term in n^2).
**** Removing the remaining BLAS function                     :SMPI:C:HPL:
- Already done on [2017-04-20 Thu], then reverted to look for something bigger. Let’s retry.
- We simply replace these functions by a no-op, with a =#define= macro. Something similar was already done for =HPL_dgemv=.
- As before, only =HPL_dcopy= is kept, because removing it causes a deadlock.
- Quick test for N=40000 and P=Q=8.
  | Metric           | With BLAS functions | Without BLAS functions | Difference |
  |------------------+---------------------+------------------------+------------|
  | Simulation time  |             33.0312 |                27.9788 |    -15.30% |
  | Application time |             16.3666 |                12.1779 |    -25.59% |
  | System time      |                 4.3 |                   3.28 |    -23.72% |
  | User time        |               28.23 |                  24.56 |    -13.00% |
  | Page faults      |             3552244 |                2231626 |    -37.18% |
  | Gflops           |               87.53 |                  89.29 |      2.01% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- Quick test for N=160000 and P=Q=8.
  | Metric           | With BLAS functions | Without BLAS functions | Difference |
  |------------------+---------------------+------------------------+------------|
  | Simulation time  |             339.258 |                278.069 |    -18.04% |
  | Application time |             217.768 |                172.506 |    -20.78% |
  | System time      |               58.67 |                  37.54 |    -36.01% |
  | User time        |              227.86 |                 203.14 |    -10.85% |
  | Page faults      |            76211290 |               38363750 |    -49.66% |
  | Gflops           |               114.3 |                  114.6 |      0.26% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- The improvement is nice, especially for the size of 160000. We divide the number of page faults by two, which
  decreases the time spent in the application and thus the total time. The impact is more important on the system time
  than on the user time.
- Note that it increased the Gflops prediction, but by a small amount (especially for the large matrix). As a
  comparison, reusing the panels caused a Gflops difference of +1.17% for N=40000 and +0.70% for N=160000.
- Thus, it is maybe not worth it to spend time on the “simblas” scripts to inject the times. As Emmanuel said at
  Bordeaux, for large enough sizes, HPL is just a big dgemm.
**** Looking at =HPL_dcopy=                    :SMPI:C:PYTHON:R:TRACING:HPL:
- The function =HPL_dcopy= is the only remaining CBLAS function that we cannot remove without causing a deadlock.
- Instrument the program to get some insights on how it is used. Modify the definition of the function in =HPL_blas.h=:
  #+begin_src c
  #define    HPL_dcopy(N, X, incX, Y, incY) ({\
          int my_rank;\
          MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
          printf("file=%s line=%d rank=%d N=%d incX=%d incY=%d\n", __FILE__, __LINE__, my_rank, N, incX, incY);\
          cblas_dcopy(N, X, incX, Y, incY);\
  })
  #+end_src
- Run HPL by redirecting the output to =/tmp/output=, using N=40000 and P=Q=8.
- Post-processing:
#+begin_src python
import re
import csv
reg = re.compile('file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) N=([0-9]+) incX=([0-9]+) incY=([0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('file', 'line', 'rank', 'N', 'incX', 'incY'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 7))
                    result[0] = result[0][result[0].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/parameters.csv')
#+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  results = read.csv('/tmp/parameters.csv')
  results$idx = 1:length(results$rank)
  head(results)
  #+end_src

  #+RESULTS:
  :                               file line rank   N incX incY idx
  : 1 /hpl-2.2/src/pfact/hpl_pdmxswp.c  168    0 120 5040    1   1
  : 2 /hpl-2.2/src/pfact/hpl_pdmxswp.c  170    0 120 5040    1   2
  : 3 /hpl-2.2/src/pfact/hpl_pdmxswp.c  168    8 120 5040    1   3
  : 4 /hpl-2.2/src/pfact/hpl_pdmxswp.c  168   32 120 5040    1   4
  : 5 /hpl-2.2/src/pfact/hpl_pdmxswp.c  168   16 120 5040    1   5
  : 6 /hpl-2.2/src/pfact/hpl_pdmxswp.c  168   24 120 5040    1   6

  #+begin_src R :results output :session *R* :exports both
  nrow(results)
  library(data.table)
  data = data.table(results)
  x = as.data.frame(data[, list(count=length(N), min_N=min(N), max_N=max(N), min_str=min(incX), max_str=max(incX)), by=c("file", "line")])
  x[with(x, order(-count)),]
  #+end_src

  #+RESULTS:
  : [1] 958854
  :                                file line  count min_N max_N min_str max_str
  : 3  /hpl-2.2/src/pfact/hpl_pdmxswp.c  246 478000    84   244       1       1
  : 1  /hpl-2.2/src/pfact/hpl_pdmxswp.c  168 317200    40   120    4920    5040
  : 5  /hpl-2.2/src/pfact/hpl_pdmxswp.c  249 103320   120   120       1       1
  : 2  /hpl-2.2/src/pfact/hpl_pdmxswp.c  170  40000    40   120    4920    5040
  : 4 /hpl-2.2/src/pfact/hpl_pdpanllt.c  212  20000     1     1      40     120
  : 7   /hpl-2.2/src/pgesv/hpl_pdtrsv.c  258    333   120   120       1       1
  : 6   /hpl-2.2/src/pgesv/hpl_pdtrsv.c  180      1    40    40       1       1
  There are a lot of calls to the function =HPL_dcopy=. All these calls are made in 7 different locations, spread in 3
  files. Function =hpl_pdmxswp= is doing most of the calls.
  The copies are not so large, at most 244 elements. The stride of the second vector is always 1. The stride of the
  first vector is between 1 and 5040.
**** TODO Next work: =HPL_dcopy= and kernel compilation [4/6]        :TOOLS:
:LOGBOOK:
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:35]
- State "TODO"       from "TODO"       [2017-04-28 Fri 17:46]
- State "TODO"       from "TODO"       [2017-04-28 Fri 17:46]
- State "TODO"       from "TODO"       [2017-04-27 Thu 10:18]
- State "TODO"       from              [2017-04-26 Wed 17:55]
:END:
- [X] Try compiling the sources without applying the patch, to see if the problem persists.
- [X] Ask some help to Vincent for the kernel.
- [X] Try to remove some calls =HPL_dcopy=.
- [X] Some compilation checks seem to have disappeared in Simgrid. For instance, it compiles successfully some programs
  with undefined symbols. Seems strange, check if it was the case before.
- [ ] Look at =grub.conf= (in =/boot= maybe?). Maybe the last kernel is not the default one to boot.
- [ ] In gdb, try the command =watch PANEL->WORK[0]= to be notified when this entry of the buffer is accessed. Maybe it
  needs to be done without =SHARED_MALLOC=.
*** 2017-04-27 Thursday
**** Tentative to compile the (unmodified) kernel
- Use these commands:
  #+begin_src sh
  cd /tmp # not enough space in /root repository in G5K...
  apt source linux
  cd linux-*
  cp /boot/config-4.9.0-2-amd64 .config # get the config (the version needs to be the same)
  make -j 16 deb-pkg
  dpkg -i ../linux-header-*.deb # not sure if this is required
  dpkg -i ../linux-image-*.deb
  #+end_src
- The last one fails with the following message (very similar to what we got):
  #+begin_example
  (Reading database ... 196215 files and directories currently installed.)
  Preparing to unpack .../linux-image-4.9.18_4.9.18-1_amd64.deb ...
  Unpacking linux-image-4.9.18 (4.9.18-1) over (4.9.18-1) ...
  Setting up linux-image-4.9.18 (4.9.18-1) ...
  Error! Bad return status for module build on kernel: 4.9.18 (x86_64)
  Consult /var/lib/dkms/nvidia/346.22/build/make.log for more information.
  update-initramfs: Generating /boot/initrd.img-4.9.18
  cryptsetup: WARNING: failed to detect canonical device of /dev/sda3
  cryptsetup: WARNING: could not determine root device from /etc/fstab
  W: mdadm: /etc/mdadm/mdadm.conf defines no arrays.
  #+end_example
- So the issue does not come from the patch.
**** Trying to remove =HPL_dcopy= in =HPL_pdmxswp=                :SMPI:C:HPL:
- Replacing all the calls of this file by a no-op (and not touching any other files) causes a deadlock.
- Removing the two calls from lines 168 and 170 does not cause a deadlock.
- Removing the call from line 246 causes a deadlock.
- Removing the call from line 249 does not cause a deadlock.
- The buffer =WORK= that is copied and sent is *not* the buffer =PANEL->WORK= (the address of =WORK= is not within the range of
  addresses of =PANEL->WORK=).
- Removing the calls did not decrease the time so significantly (about 2 seconds for the total of 30 seconds). Since the
  copies are not on =PANEL->WORK=, maybe we should not focus on that.
**** Trying again to patch the kernel                              :TOOLS:
- Run all the steps of [2017-04-18 Tue]. There is still the error message, but according to Vincent we do not care. The
  compilation still succeeded (except for the nvidia part), and we should be able to run the kernel.
- Use =tgz-g5k= to generate =debian_patched.tgz=.
- Write the file =debian_patched.env=, based on the previous =.env= file.
- Then, deploy the environment. The deployment is successful.
- The kernel version is not right. Command =uname -r= returns =4.9.0-2-amd64= (same version than before), but the compiled
  kernel is =4.9.18-1=.
- On the machine where the kernel was patched, run the following (see [[https://unix.stackexchange.com/questions/41512/booting-a-newly-compiled-linux-kernel][stackexchange]] for the rational).
  #+begin_src sh
  make modules_install install
  #+end_src
  Then redo the =tgz-g5k= and the deployment. Result: =uname -r= still returns the same thing (but the image file is bigger,
  so something must have changed).
- In =/boot=, there are files for kernel =4.9.18= (it was not the case with the original image). So it seems that the
  patched kernel is here somewhere, but we do not boot on it automatically.
- The image obtained after the =make modules_install install= as new files in =/boot=: there are =4.9.18.old= and =4.9.18=
  files. Also, its file =initrd.img-4.9.18= is much bigger than the others (249MB, vs about 20MB).
- Still on the machine where the kernel was patched, run this:
  #+begin_src sh
  update-grub
  #+end_src
  Then redo the =tgz-g5k= and the deployment. Result: =uname -r= still returns =4.9.0-2=.
**** Looking at =HPL_dlacpy= and =HPL_dlatcpy=  :SMPI:C:PYTHON:R:TRACING:HPL:
- HPL has also its own copy functions. Let’s have a look at them.
- Instrument the program to get some insights on how it is used. In file =include/hpl.h=, add these definitions:
  #+begin_src c
  #define HPL_dlacpy( M, N, A, LDA, B, LDB ) ({\
      int my_rank;\
      MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
      printf("function=dlacpy file=%s line=%d rank=%d M=%d N=%d LDA=%d LDB=%d\n", __FILE__, __LINE__, my_rank, M, N, LDA, LDB);\
      HPL_dlacpy_( M, N, A, LDA, B, LDB );\
  })
  #define HPL_dlatcpy( M, N, A, LDA, B, LDB ) ({\
      int my_rank;\
      MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\
      printf("function=dlatcpy file=%s line=%d rank=%d M=%d N=%d LDA=%d LDB=%d\n", __FILE__, __LINE__, my_rank, M, N, LDA, LDB);\
      HPL_dlatcpy_( M, N, A, LDA, B, LDB );\
  })
  #+end_src
- Add an underscore after the original names of the functions.
- Run HPL by redirecting the output to =/tmp/output=, using N=40000 and P=Q=8.
- Post-processing:
#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z0-9/_.-]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) M=([0-9]+) N=([0-9]+) LDA=([0-9]+) LDB=([0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f:
        with open(out_file, 'w') as out_f:
            csv_writer = csv.writer(out_f)
            csv_writer.writerow(('func', 'file', 'line', 'rank', 'M', 'N', 'LDA', 'LDB'))
            for line in in_f:
                match = reg.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 9))
                    result[1] = result[1][result[1].index('/hpl'):].lower()
                    csv_writer.writerow(result)
process('/tmp/output', '/tmp/parameters.csv')
#+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  results = read.csv('/tmp/parameters.csv')
  results$idx = 1:length(results$rank)
  results$size = results$M * results$N
  head(results)
  #+end_src

  #+RESULTS:
  :      func                               file line rank M N LDA  LDB idx size
  : 1 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 0 2 120 5040   1    0
  : 2 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 2 2 120 5040   2    4
  : 3 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 0 4 120 5040   3    0
  : 4 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 4 2 120 5040   4    8
  : 5 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 6 2 120 5040   5   12
  : 6 dlatcpy /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231    0 4 4 120 5040   6   16

  #+begin_src R :results output :session *R* :exports both
  nrow(results)
  library(data.table)
  data = data.table(results)
  x = as.data.frame(data[, list(count=length(N), min_size=min(size), max_size=max(size), mean_size=mean(size)), by=c("func", "file", "line")])
  x[with(x, order(-count)),]
  #+end_src

  #+RESULTS:
  #+begin_example
  [1] 44641
       func                                file line count min_size max_size
  1 dlatcpy  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  231 39332        0     3600
  4 dlatcpy /hpl-2.2/src/pgesv/hpl_pdupdatett.c  386  2637       40   604800
  2  dlacpy       /hpl-2.2/src/comm/hpl_copyl.c  102  2338        0   604800
  3  dlacpy       /hpl-2.2/src/comm/hpl_copyl.c   97   334        0   590400
      mean_size
  1    300.6592
  4 302480.6978
  2 299396.4072
  3 292253.8922
  #+end_example
  There are much less calls to these functions than to =HPL_dcopy=, but the sizes of the copies are much larger. We should
  have a look.
*** 2017-04-28 Friday
**** Flame graphs                                   :SMPI:EXPERIMENTS:HPL:
- New tentative with flame graphs.
- Simgrid commit: =743399f59d6ce57b0772a26cad2daba7e807b9e9=
- HPL commit: =446b7f6d647c1546a79a63a3721eb1949fa32127= (compiled with =SMPI_OPTIMIZATION= option).
- Run the following command (with N=80000 and P=Q=4):
  #+begin_src sh
  sudo smpirun -wrapper "perf record -F1000 --call-graph dwarf" --cfg=smpi/bcast:mpich
  --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen -np
  16 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
  #+begin_src sh
  sudo perf script | ~/Documents/FlameGraph/stackcollapse-perf.pl --kernel | ~/Documents/FlameGraph/flamegraph.pl > /tmp/hpl_flame.svg
  #+end_src
  It is better to run it with =sudo= to also see the name of the kernel functions wich take time (e.g. function
  =page_fault=).
- With the given version of Simgrid, the result is not good. We get warning messages like the following:
  #+begin_example
  Failed to open /home/tom/Dropbox/Documents/Fac/2017_Stage_LIG/hpl-2.2/bin/SMPI/xhpl_10821_4.so, continuing without symbols
  #+end_example
  And the resulting image does not say anything on the functions inside HPL:
  [[file:flame_graphs/flame_broken.svg]]
- The problem is that Simgrid creates a copy of HPL per process (privatization mechanism), but then unlink these
  files. Thus, when the FlameGraph script tries to process the trace, it cannot find the libraries, so it fails to
  output anything useful.
- Raised an issue on [[https://github.com/simgrid/simgrid/issues/160][Github]].
- A simple fix is to remove the call to =unlink= in file =src/smpi/smpi_global.cpp= line 604. Now, the script does not
  output any warning and the graph is much better (but we have these ugly temporary files). Result:
  [[file:flame_graphs/flame_full.svg]]

- The graph is pretty clear. We can see that there is still a lot of time spent in HPL. We also see that a significant
  part of the time is spent in =page_fault=, due to function =HPL_dlatcpy=. Function =HPL_dlacpy=, =HPL_dlaswp10N= and
  =HPL_dlaswp01T= also take a lot of time.
**** Removing =HPL_dlacpy= and =HPL_dlatcpy=                      :SMPI:C:HPL:
- In files =HPL_dlacpy.c= and =HPL_dlatcpy.c=, add this code at the beginning of the functions:
  #+begin_src c
  #ifdef SMPI_OPTIMIZATION
      return;
  #endif
  #+end_src
- Thus, these two copy functions simply become a no-op. Execiton succeeds, there is no deadlock or memory error.
- With N=40000 and P=Q=8, the different metrics are:
  | Metric           | With the copies | Without the copies | Difference |
  |------------------+-----------------+--------------------+------------|
  | Simulation time  |         29.2286 |            26.2468 |    -10.20% |
  | Application time |         11.9311 |            8.93591 |    -25.10% |
  | System time      |            3.23 |               2.26 |    -30.03% |
  | User time        |           25.86 |              23.95 |     -7.39% |
  | Page faults      |         2231251 |             569999 |    -74.45% |
  | Gflops           |           84.14 |              84.22 |      0.10% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- With N=80000 and P=Q=4 (like the flamegraph, but unlike the tests usually done for performances), the different metrics are:
  | Metric           | With the copies | Without the copies | Difference |
  |------------------+-----------------+--------------------+------------|
  | Simulation time  |         38.4268 |            25.6523 |    -33.24% |
  | Application time |         27.1912 |            16.5516 |    -39.13% |
  | System time      |            7.08 |               3.94 |    -44.35% |
  | User time        |           29.44 |              21.70 |    -26.29% |
  | Page faults      |         7646506 |            2617454 |    -65.77% |
  | Gflops           |           28.87 |              28.88 |      0.03% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- As expected, this caused a huge reduction of the number of page faults. Also, as for previous optimizations, the
  impact on the simulation time is larger and the impact on the Gflops is lower when each process as a larger submatrix.
  Something strange is that, for N=80000 and P=Q=4, the simulation time decreased by 33% whereas the two copy functions
  only took about 25% of the time.
**** Removing =HPL_dlaswp10N= and =HPL_dlaswp01T=                :SMPI:C:HPL:
- Same thing than above, we add a =return= statement at the top of the functions.
- With N=40000 and P=Q=8, the different metrics are:
  | Metric           | With the functions | Without the functions | Difference |
  |------------------+--------------------+-----------------------+------------|
  | Simulation time  |            26.2468 |               21.2561 |    -19.01% |
  | Application time |            8.93591 |               3.71109 |    -58.47% |
  | System time      |               2.26 |                  2.30 |      1.77% |
  | User time        |              23.95 |                 18.90 |    -21.09% |
  | Page faults      |             569999 |                496498 |    -12.89% |
  | Gflops           |              84.22 |                 84.33 |      0.13% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- With N=80000 and P=Q=4 (like the flamegraph, but unlike the tests usually done for performances), the different metrics are:
  | Metric           | With the functions | Without the functions | Difference |
  |------------------+--------------------+-----------------------+------------|
  | Simulation time  |            25.6523 |               15.0837 |    -41.20% |
  | Application time |            16.5516 |               5.58209 |    -66.27% |
  | System time      |               3.94 |                  4.00 |      1.52% |
  | User time        |              21.70 |                 10.92 |    -49.68% |
  | Page faults      |            2617454 |               2558058 |     -2.27% |
  | Gflops           |              28.88 |                  28.9 |      0.07% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- Again, we have a nice reduction of the simulation time without high increase of the Gflops. Strangely, this
  modification significantly increased the system time (but this is largely compensated by the loss in the user time).
**** Removing =HPL_dlaswp06T=                                   :SMPI:C:HPL:
- Same thing than above, we add a =return= statement at the top of the function.
- With N=40000 and P=Q=8, the different metrics are:
  | Metric           | With the function | Without the function | Difference |
  |------------------+-------------------+----------------------+------------|
  | Simulation time  |           21.2561 |              20.7063 |     -2.59% |
  | Application time |           3.71109 |              3.47176 |     -6.45% |
  | System time      |              2.30 |                 2.10 |     -8.70% |
  | User time        |             18.90 |                18.60 |     -1.59% |
  | Page faults      |            496498 |               305177 |    -38.53% |
  | Gflops           |             84.33 |                84.29 |     -0.05% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- With N=80000 and P=Q=4 (like the flamegraph, but unlike the tests usually done for performances), the different metrics are:
  | Metric           | With the function | Without the function | Difference |
  |------------------+-------------------+----------------------+------------|
  | Simulation time  |           15.0837 |              13.0237 |    -13.66% |
  | Application time |           5.58209 |              3.70466 |    -33.63% |
  | System time      |              4.00 |                 3.00 |    -25.00% |
  | User time        |             10.92 |                10.02 |     -8.24% |
  | Page faults      |           2558058 |               994459 |    -61.12% |
  | Gflops           |              28.9 |                 28.9 |      0.00% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- Again, we have a nice reduction of the simulation time without high increase of the Gflops (in the first case, the
  Gflops even decrease slightly, which is strange). Also, we have a large reduction of the number of page faults.
**** Flame graphs for the last optimizations        :SMPI:EXPERIMENTS:HPL:
- Simgrid commit: =fa61108e3d288e7e5559992a1feb7c86e255b3e3=
- HPL commit: =ac2f78527c42fb6ea976a715a42b9e2371e1194d= (compiled with =SMPI_OPTIMIZATION= option).
- Flame graph (using N=80000 and P=Q=4):
  [[file:flame_graphs/flame_after_optim.svg]]

- There is (at least with these settings) not much room for improvements in HPL itself. All the large blocks are made of
  small sub-blocks.
- The calls to =smpi_shared_malloc= are starting to take a significant part of the time, due to the =_mm_populate= function
  (certainly caused by the =MAP_POPULATE= option passed to =mmap=). Should test without this flag, and maybe with the kernel
  patch.
**** Time and memory efficiency with the last optimizations, new regression :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Simgrid commit: =fa61108e3d288e7e5559992a1feb7c86e255b3e3=
- HPL commit: =ea70a52c29f177325dc57cc99d96e97537f2ecf2=
- Script commit: =c83938f77dec089b7d323f36ccd6822ee203e598=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl.csv --nb_runs 1 --size
  100,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree
  "2;8,8;1,8;1,1" --experiment HPL
  #+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  data <- read.csv('hpl_analysis/hpl_nopagefaults.csv')
  data$simgrid_time = data$simulation_time - data$application_time
  #+end_src

  #+begin_src R :file hpl_analysis/48.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data[data$nb_proc==56,], aes(x=size, y=simulation_time)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time for different matrix sizes\nNumber of processes: 56")
  plot2 = ggplot(data[data$nb_proc==56,], aes(x=size, y=minor_page_fault)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different matrix sizes\nNumber of processes: 56")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/48.png]]

  #+begin_src R :file hpl_analysis/49.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  melted_results1 = melt(data[c("nb_proc", "size", "system_time", "user_time")], id.vars = c("nb_proc", "size"), variable.name="time_type", value.name="time")
  melted_results2 = melt(data[c("nb_proc", "size", "application_time", "simgrid_time")], id.vars = c("nb_proc", "size"), variable.name="time_type", value.name="time")
  plot1 = ggplot(melted_results1[melted_results1$nb_proc==56,], aes(x=size, y=time, color=time_type)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Times for different matrix sizes\nNumber of processes: 56")
  plot2 = ggplot(melted_results2[melted_results2$nb_proc==56,], aes(x=size, y=time, color=time_type)) +
      geom_line() + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different matrix sizes\nNumber of processes: 56")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/49.png]]

- The simulation time looks linear in the size. But the number of page faults still looks quadratic. We can see the
  consequence in the plots with the detailled time: the user time and the simgrid time looks to be linear, but the
  system time and the application time still looks (slightly) quadratic). Thus, at large scales, the simulation could
  still not be efficient enough.

- Let’s redo the regression.

  #+begin_src R :results output :session *R* :exports both
  data = data[!(data$nb_proc%in% c(1,32,48,64)) & data$size > 1000, # get rid of particularly small values
                 c("nb_proc","size","Gflops","simulation_time","uss")]
  head(data)
  #+end_src

  #+RESULTS:
  :   nb_proc  size Gflops simulation_time      uss
  : 1      16 10000  17.67        1.321010  9891840
  : 2       8 30000  14.08        1.638650 11649024
  : 4       8 20000  13.57        0.975746  7512064
  : 5      24 70000  41.93       17.320500 11030528
  : 7      40 90000  68.82       44.840000 16220160
  : 8      40 70000  66.45       33.992100 16465920

  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  plot(data)
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-16379_94/figure16379n6p.png]]

  #+begin_src R :results output :session *R* :exports both
  reg_rss = lm(data=data,uss ~ nb_proc) # Interactions do not bring much
  summary(reg_rss)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = uss ~ nb_proc, data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -1600671  -937507  -285802   639647  4105940 

  Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  5023214     375584   13.37   <2e-16 ***
  nb_proc       314984      11191   28.15   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1364000 on 48 degrees of freedom
  Multiple R-squared:  0.9429,	Adjusted R-squared:  0.9417 
  F-statistic: 792.2 on 1 and 48 DF,  p-value: < 2.2e-16
  #+end_example

  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  par(mfrow=c(2,3)) ; 
    plot(data=data,uss~size); 
    plot(data=data,uss~nb_proc);
    plot(reg_rss); 
  par(mfrow=c(1,1))
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-16379_94/figure163790Ew.png]]

  The Stampede HPL output indicates:
  #+BEGIN_EXAMPLE
  The following parameter values will be used:
  
  N        : 3875000 
  NB       :    1024 
  PMAP     : Column-major process mapping
  P        :      77 
  Q        :      78 
  PFACT    :   Right 
  NBMIN    :       4 
  NDIV     :       2 
  RFACT    :   Crout 
  BCAST    :  BlongM 
  DEPTH    :       0 
  SWAP     : Binary-exchange
  L1       : no-transposed form
  U        : no-transposed form
  EQUIL    : no
  ALIGN    :    8 double precision words
  #+END_EXAMPLE

  We aim at ~size=3875000~ and ~nb_proc=77*78~.

  #+begin_src R :results output :session *R* :exports both
  data[data$nb_proc==64 & data$size==40000,]
  data[data$nb_proc==64 & data$size==40000,]$uss/1E6 # in MB
  example=data.frame(size=c(3875000,160000, 40000), nb_proc=c(77*78,56,56));
  predict(reg_rss, example, interval="prediction", level=0.95)/1E6
  #+end_src

  #+RESULTS:
  : [1] nb_proc         size            Gflops          simulation_time
  : [5] uss            
  : <0 lignes> (ou 'row.names' de longueur nulle)
  : numeric(0)
  :         fit        lwr        upr
  : 1 1896.8155 1762.29661 2031.33431
  : 2   22.6623   19.82645   25.49816
  : 3   22.6623   19.82645   25.49816

  The prediction is between 1.7 and 2 GB of memory, so this not as good as it used to be, but it is still affordable.

  #+begin_src R :results output :session *R* :exports both
  reg_time = lm(data=data,simulation_time ~ poly(size,3)*poly(nb_proc,2))
  summary(reg_time)
  reg_time = lm(data=data,simulation_time ~ I(size**2) + size*nb_proc)
  summary(reg_time)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = simulation_time ~ poly(size, 3) * poly(nb_proc, 
      2), data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -1.93639 -0.38951 -0.08914  0.33407  2.18166 

  Coefficients:
                                    Estimate Std. Error t value Pr(>|t|)    
  (Intercept)                       18.04686    0.12131 148.761  < 2e-16 ***
  poly(size, 3)1                    72.12749    0.85782  84.082  < 2e-16 ***
  poly(size, 3)2                     4.04034    0.85782   4.710 3.27e-05 ***
  poly(size, 3)3                    -0.07083    0.85782  -0.083   0.9346    
  poly(nb_proc, 2)1                 87.78730    0.85782 102.337  < 2e-16 ***
  poly(nb_proc, 2)2                  2.78981    0.85782   3.252   0.0024 ** 
  poly(size, 3)1:poly(nb_proc, 2)1 338.59322    6.06574  55.821  < 2e-16 ***
  poly(size, 3)2:poly(nb_proc, 2)1  11.43874    6.06574   1.886   0.0670 .  
  poly(size, 3)3:poly(nb_proc, 2)1  -2.07916    6.06574  -0.343   0.7337    
  poly(size, 3)1:poly(nb_proc, 2)2  11.35755    6.06574   1.872   0.0689 .  
  poly(size, 3)2:poly(nb_proc, 2)2  -0.34210    6.06574  -0.056   0.9553    
  poly(size, 3)3:poly(nb_proc, 2)2  -1.74231    6.06574  -0.287   0.7755    
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.8578 on 38 degrees of freedom
  Multiple R-squared:  0.9982,	Adjusted R-squared:  0.9976 
  F-statistic:  1882 on 11 and 38 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = simulation_time ~ I(size^2) + size * nb_proc, data = data)

  Residuals:
       Min       1Q   Median       3Q      Max 
  -2.68720 -0.34522 -0.02409  0.58020  1.62754 

  Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
  (Intercept)   1.167e+00  7.005e-01   1.666 0.102581    
  I(size^2)     7.864e-10  1.860e-10   4.228 0.000114 ***
  size         -1.254e-04  2.242e-05  -5.593 1.25e-06 ***
  nb_proc      -3.204e-02  1.694e-02  -1.891 0.065052 .  
  size:nb_proc  1.368e-05  2.730e-07  50.106  < 2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.9557 on 45 degrees of freedom
  Multiple R-squared:  0.9973,	Adjusted R-squared:  0.9971 
  F-statistic:  4166 on 4 and 45 DF,  p-value: < 2.2e-16
  #+end_example


  #+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R*
  par(mfrow=c(2,3)) ; 
    plot(data=data,simulation_time~size); 
    plot(data=data,simulation_time~nb_proc);
    plot(reg_time); 
  par(mfrow=c(1,1))
  #+end_src

  #+RESULTS:
  [[file:/tmp/babel-16379_94/figure16379zYF.png]]


  #+begin_src R :results output :session *R* :exports both
  data[data$nb_proc==56 & data$size==40000,]
  predict(reg_time, example, interval="prediction", level=0.95)/3600/24 # in days
  #+end_src

  #+RESULTS:
  :    nb_proc  size Gflops simulation_time      uss
  : 40      56 40000  73.38         26.0398 23302144
  :            fit          lwr         upr
  : 1 3.8141471265 3.6556354954 3.972658758
  : 2 0.0014123450 0.0013580322 0.001466658
  : 3 0.0003039585 0.0002805669 0.000327350

- So despite the simulation time still quadratic in the size, the prediction is now nearly 4 days of computation, which
  whould be great. Try with larger sizes and numbers of processes before, to re-do the regression and see if anything changes.
- Quick test with 56 processes and a size of 160000.
  + Simulation time of 128.9 seconds (was 1437 seconds at Bordeaux).
  + Application time of 23.3 seconds (was 805 seconds).
*** 2017-04-29 Saturday
**** Bad CPU utilization when the size increases :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- It seems that the CPU utilization decreases when the size of the matrix increases, and it is maybe worse when =smemstat=
  is used. Modified the script to measure the CPU utilization and decreases the sampling rate for the calls to =smemstat=
  (from 10Hz to 0.25Hz). An experiment was also done without calling =smemstat= at all (but it means there is no data for
  =uss= and =rss=).
- Simgrid commit: =fa61108e3d288e7e5559992a1feb7c86e255b3e3=
- HPL commit: =ea70a52c29f177325dc57cc99d96e97537f2ecf2=
- Script commit: =f9442d471d2d0591cd9ffa5ceb18a46fe81ef5bc= (modified for the experiment without memstat)
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl.csv --nb_runs 1 --size 50000,100000,200000,250000,300000 --nb_proc 56 --fat_tree
  "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  data_with_smemstat <- read.csv('hpl_analysis/hpl_big.csv')
  data_with_smemstat$smemstat = TRUE
  data_without_smemstat <- read.csv('hpl_analysis/hpl_big_nosmemstat.csv')
  data_without_smemstat$smemstat = FALSE
  data = rbind(data_with_smemstat, data_without_smemstat)
  data$ideal_time = data$simulation_time * data$cpu_utilization
  melted_data = melt(data[c("size", "simulation_time", "ideal_time", "smemstat")], id.vars = c("size", "smemstat"), variable.name="time_type", value.name="time")
  #+end_src

  #+begin_src R :file hpl_analysis/51.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(melted_data, aes(x=size, y=time, color=smemstat, linetype=time_type)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time for different matrix sizes\nNumber of processes: 56")
  plot2 = ggplot(data, aes(x=size, y=cpu_utilization, color=smemstat)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("CPU utilization for different matrix sizes\nNumber of processes: 56")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/51.png]]

- It is very clear that the CPU utilization is dropping when the size increases. It is still pretty high for the two
  first points, which are for sizes 50000 and 100000. This explain why this was unnoticed before.
- Using =semstat= seems to have no significant impact (at least with the small number of points we have).
- The “ideal time” in the left plots is the time we would have with a 100% CPU utilization. This looks linear. The
  “simulation time” is the real time, which looks quadratic.
- Rough estimation: the CPU utilization decreases of 0.5 for a size raise of 3e5. Thus, we should not be able to do
  a simulation with a size above 1e6 (recall that the target is 3e6).
*** 2017-04-30 Sunday
**** Bad CPU utilization when the number of processes increases :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- Let’s see what is the effect of the number of processes on the CPU utilization.
- Simgrid commit: =fa61108e3d288e7e5559992a1feb7c86e255b3e3=
- HPL commit: =ea70a52c29f177325dc57cc99d96e97537f2ecf2=
- Script commit: =846af7959f08d3fdb3457244bdddf97d19cc0b06=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl.csv --nb_runs 1 --size 300000 --nb_proc 16,32,64,128,256,512 --fat_tree "2;16,32;1,16;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  data <- read.csv('hpl_analysis/hpl_big2.csv')
  data$ideal_time = data$simulation_time * data$cpu_utilization
  melted_data = melt(data[c("nb_proc", "simulation_time", "ideal_time")], id.vars = c("nb_proc"), variable.name="time_type", value.name="time")
  #+end_src

  #+begin_src R :file hpl_analysis/52.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(melted_data, aes(x=nb_proc, y=time, linetype=time_type)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time for different number of processes\nMatrix size: 300000")
  plot2 = ggplot(data, aes(x=nb_proc, y=cpu_utilization)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      expand_limits(y=1)+
      ggtitle("CPU utilization for different number of processes\nMatrix size: 300000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/52.png]]

- The CPU utilization is also dropping when the number of processes increases, but it is less brutal than what we
  observed for the size.
- Still no idea of what is happening.
** 2017-05 May
*** 2017-05-02 Tuesday
**** Investigating the low CPU usage    :SMPI:EXPERIMENTS:PERFORMANCE:HPL:
- Run HPL with a large enough size and have a look at =htop=.
  + Sometimes, the OS “freezes”: mouse and keyboard are inactive. After a few seconds, everything returns to
    normal. This is *not* a result of swaping, enough RAM is still available.
  + The memory consumption is very high, with N=600000 and P=Q=8 there is a delta of about 6 GB (from 5.9 to 11.8
    GB). This memory consumption does not come directly from HPL, as shown by previous experiments. A guess is that it
    comes from the page table.
  + There seems to be three “states”:
    - Process =smpimain= has a CPU load above 95%, most of it in user mode (green in =htop=).
    - Process =smpimain= has a CPU load above 95%, most of it in kernel mode (red in =htop=). Mostly at the beginning of the
      execution.
    - Process =smpimain= has a CPU load of 0%, but the CPU is loaded at 100% by the kernel. When this happens, then there
      is a “freeze” of the OS.
- We can get the size of the page table with the following command:
  + For N=300000 and P=Q=8:
    #+begin_src sh
    cat /proc/`pgrep smpimain`/status | grep VmPTE
    #+end_src
    #+RESULTS:
    | VmPTE: | 1382808 kB |
  + For N=600000 and P=Q=8:
    #+begin_src sh
    cat /proc/`pgrep smpimain`/status | grep VmPTE
    #+end_src
    #+RESULTS:
    | VmPTE: | 5511536 kB |
  So the page table itself takes 1.4 GB of memory for N=3e5 and 5.5 GB for N=6e5. This is huge. And indeed, the size of
  the page table is expected to be proportional to the size of the allocation (and thus quadratic in the size of the
  matrix).
**** CPU utilization for different allocation sizes :C:R:EXPERIMENTS:PERFORMANCE:
- A guess is that the low CPU usage comes from the memory. Let’s reuse the work done previously with page faults to see
  if it is true: allocate one buffer (of various size), write or not on the whole buffer, and measure the CPU utilization.
- Script commit: =a2d1ae9f2a34b8d91c1ba48a7ac3f2dffec2975a=
- Command:
  #+begin_src sh
  gcc -std=gnu11 -O3 -o page_faults page_faults.c -Wall
  ./cpu_utilization.py 10 720000000000 cpu_exp.csv
  #+end_src
  Note that the given size is 3e5**2*8, i.e. the size we use to allocate matrices of size 300000.
  We run 10 experiments. For each experiment, the size is sampled randomly and uniformly in [1, size].
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results = read.csv('page_faults/cpu_exp.csv')
  head(results)
  #+end_src

  #+begin_src R :file page_faults/15.png :results value graphics :results output :session *R* :exports both
  ggplot(results, aes(x=size, y=cpu_utilization, color=mem_access)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Number of page faults for different allocation sizes\nKernel 4.4.0-72-generic")
  #+end_src

  #+RESULTS:
  [[file:page_faults/15.png]]

- We can also query the size of the page table when running the program =page_faults= with a size of 7.2e11.
  #+begin_src sh
  cat /proc/`pgrep page_faults`/status | grep VmPTE
  #+end_src

  #+RESULTS:
  | VmPTE: | 1373316 kB |

  We find approximately the same size than for HPL with N=3e5.
**** DONE Issue of the page table size [2/2]                    :SMPI:HPL:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:31]
- State "TODO"       from "TODO"       [2017-05-02 Tue 16:54]
- State "TODO"       from              [2017-05-02 Tue 14:40]
:END:
- [X] Output the page table size in the script, and draw some plots (memory consumption of HPL vs memory consumption of
  its page table might be funny).
- [X] Use [[https://wiki.debian.org/Hugepages][huge pages]]. If we use larger pages, then the page table will be smaller, so this should help.
- Note that the size of the page table will still be quadratic in the size of the matrix.
**** Run huge HPL with Simgrid                      :SMPI:EXPERIMENTS:HPL:
- Use a node in G5K, nova@lyon, which has about 62.7GB of RAM available.
- Run HPL with P=Q=8 and N=3875000 (the N from Stampede).
- The initialization phase lasts for very long (the process loads the CPU at 100% in kernel mode), and the size of the
  page table grows.
  + At about 10 minutes, the page table has a size of about 43 GB.
  + At about 13 minutes, the page table has a size of about 55  GB (total memory consumption in the node: 59 GB).
  + At 13:41, the CPU load of the process drops to 0%. The kernel loads the CPU at 100%. The page table has a size of
    about 58.591 GB and the total memory consumption of the node is 61.9 GB.
  + At 40 minutes, nothing changed, the process is still at 0% of CPU load, with the same amount of memory for its page
    table, and the kernel still loads the CPU at 100%. Note that there is nearly no swap (only 328 kB, it was already
    the case at 13:41).
  + Shortly after, =smpimain= became loaded at 100% again, in kernel mode. Three other cores were also very loaded in
    kernel mode: =/usr/sbin/ntpd= at 105%, =/sbin/iscsid= at 105%, and =watch= I used to monitor the page table size. Then,
    the swap size raises to 49 MB (page table now takes 59.178 GB), and everything freezes.
  + After a few minutes, it unfreezes for a few seconds and freezes again. Now, =smpimain= is back at 0%. The process
    =/usr/sbin/cpufreqd= is at 100% in kernel mode, and the kernel itself is at 69.5%.
  + At some point, the process is killed and everything returns to normal.
**** Plots of the page table size                   :SMPI:EXPERIMENTS:HPL:
- The script now outputs the page table size. Let’s have a look.
- Simgrid commit: =fa61108e3d288e7e5559992a1feb7c86e255b3e3=
- HPL commit: =ea70a52c29f177325dc57cc99d96e97537f2ecf2=
- Script commit: =217585966d6aacd49c1081dcb4ac07ed0634c2c9=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl.csv --nb_runs 1 --size
  100,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000 --nb_proc 1,8,16,24,32,40,48,56,64 --fat_tree
  "2;8,8;1,8;1,1" --experiment HPL
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  data <- read.csv('hpl_analysis/hpl_page_table.csv')
  melted_data = melt(data[c("size", "nb_proc","uss", "page_table_size")], id.vars = c("size", "nb_proc"), variable.name="memory_usage", value.name="memory")
  #+end_src

  #+begin_src R :file hpl_analysis/53.png :results value graphics :results output :session *R* :exports both :height 400
  ggplot(melted_data[melted_data$nb_proc==64 & melted_data$size > 1000,], aes(x=size, y=memory, color=memory_usage)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Memory consumption for different matrix sizes\nNumber of processes: 64")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/53.png]]

- The problem is pretty clear: the size of the page table grows quadratically with the size of the matrix, as expected.
- Quick test: allocate a (small) constant size for the matrix, and run HPL with valgrind. There are invalid
  reads/writes. When they are removed, there are new invalid reads/writes somewhere else, etc. Removing all the memory
  accesses and doing a small allocation is certainly the perfect solution in terms of performance, but it might be
  tedious.
*** 2017-05-03 Wednesday
**** Thoughts on the page table issue                           :SMPI:HPL:
- Recall an observation made with =htop=: when running HPL with a large matrix size, =smpimain= regularly drops at 0% of CPU
  load. When this happens, the CPU remains loaded at 100% in kernel mode, without any process showing at high load in
  =htop= (thus the hypothesis that this is the kernel itself that makes the heavy load).
- I do not see an explanation with the large page table. The table is initialized once when the allocation is done (here
  we observe =smpimain= loading the CPU at 100% in kernel mode, this is expected). But then, the translation of an address
  (in case of TLB miss) is done in a constant number of memory accesses, so the size of the page table should be
  irrelevant. Also, the page table itself does not seem to be swapped out (the swap size remains small, according to
  =htop=).
**** Possibilities to fix the page table issue                  :SMPI:HPL:
- Using huge pages. Not as easy as it looks, the OS has a pool of huge pages which (obviously) fit in the memory, so
  not possible to use them for our very large allocations. Also did not manage to do a single small =mmap= with
  =MAP_HUGETLB=.
  Usefull links:
  + https://wiki.debian.org/Hugepages
  + http://stackoverflow.com/questions/27634109/why-mmap-cannot-allocate-memory
  + https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt
  + http://www.mcs.anl.gov/~kazutomo/hugepage-old/huge-mmap.c
- Removing all the accesses to the matrix and stop allocaitng it.
  + Very tedious. The basic process is to remove the allocation, then run HPL with Valgrind and look for all the
    invalid read/write. Modified 11 files so far, not working yet.
- JIT address translation.
  + The idea here is, at execution time, to rewrite all the addresses that are in the range of the shared malloc (and
    stop allocating anything in the shared malloc). This is very similar to what is done in OS virtualization. Maybe
    too difficult?
*** 2017-05-04 Thursday
**** Debugging huge pages                                :C:TOOLS:MEETING:
- Debugging of the use of huge pages in the small program =page_faults.c=.
- The =blocksize= has to be exactly the size of a huge page.
- Add =MMAP_HUGETLB= only for the =mmap= of the folding, not the first one. These =mmap= have to be done on an address that is
  aligned on the size of a huge page (so the first =mmap= has to be done with a larger size).
- The file given to the =mmap= of the folding has to be in a file system supporting huge pages:
  #+begin_src sh
  sudo mkdir /home/huge
  sudo mount none /home/huge -t hugetlbfs -o rw,mode=0777
  #+end_src
  It cannot be written to.
- Script to have a look at the page table of a process:
  #+begin_src sh
  sudo pagemap <pid>
  #+end_src
  To get it, add the following in file =/etc/apt/sources.list.d/vincent.list=:
  #+begin_example
  deb http://people.debian.org/~vdanjean/debian unstable main
  #+end_example
  Then, install the package =vdanjean-archive-keyring=.
- Running =pagemap= shows that huge pages are used and that they are all mapped at the same location. Also, the page table
  size seems to have decreased (from 9568 kB to 28 kB).
**** DONE Next steps for huge pages [2/2]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:36]
- State "TODO"       from "TODO"       [2017-05-04 Thu 17:20]
- State "TODO"       from              [2017-05-04 Thu 11:52]
:END:
- [X] Look for the bug. Try the program on a more recent kernel (it is maybe a kernel bug, in this case it should be reported).
- [X] Implement it in Simgrid, for the shared malloc.
**** Keep debugging huge pages                                   :C:TOOLS:
- Same thing than this morning, on a G5K node with kernel 4.9.0-2. The same bug happens:
  #+begin_example
  ./page_faults: relocation error: ./page_faults: symbol memset, version GLIBC_2.2.5 not defined in file libc.so.6 with link time reference
  #+end_example
  This bug does not always happen, sometimes there is no bug, sometimes there is a segmentation fault.
- The same bug still happens if the call to =memset= is replaced by a loop writting on the whole buffer, so it seems that
  it is another call to =memset= that causes the problem.
  → Forget that, the compiler figured out by itself what I wanted to do and replaced it by a real call to =memset=
  (checked with =objdump=).
- If the write on the buffer is removed (we just allocate then deallocate) or if we write on *some* parts of the buffer,
  then sometimes this error happens:
  #+begin_example
  ./page_faults: relocation error: ./page_faults: symbol munmap, version GLIBC_2.2.5 not defined in file libc.so.6 with link time reference
  #+end_example
- If we also remove the deallocation, then no (visible) error happens.
- If we replace the big =mmap= done at the beginning by a =malloc= (and the =munmap= by a =free=), then the same bug still happens.
- The error happens only when we call =mmap= for the last huge page. If we replace the =size/blocksize= of the for loop by a
  =size/blocksize-1=, then everything is fine. Do not understand why this happens (this is not a mistake with the indices,
  this block is within the range to allocate). Another fix is to do the first =mmap= with a length of =size+2*blocksize=
  instead of =size+blocksize=. This is better, since then all the pages accessed are mapped on the file (thus the
  consumption in physical memory remains constant).
- The value reported in =/proc/<pid>/status= (field =VmPTE=) is always 28kB, no matter the size of the allocation. So it
  seems that =VmPTE= only accounts for the classical pages. The value reported by the field =VmPMD= seems ok, it grows
  linearly with the size of the allocation, even with huge pages. But I don't think it accounts for the whole page table
  (the man says "Size of second-level page tables", and when experimenting with classical pages, this value is
  approximately 500 times lower than VmPTE). The total size of the page tables of the system (in =/proc/meminfo=, field
  =PageTables=) does not seem to account for huge pages either. See [[http://stackoverflow.com/questions/853736/how-to-find-or-calculate-a-linux-processs-page-table-size-and-other-kernel-acco/13868623][this stackoverflow thread]] for more information. Thus,
  it seems the only way we have to estimate the memory consumption of the simulation (including page tables) is to look
  at the whole memory consumption of the system before the simulation and during the simulation (but it is obviously
  much more noisy).
- Quick test of the program:
  + Command used:
    #+begin_src sh
    /usr/bin/time ./page_faults 1 500000000000 1
    #+end_src
  + Commit =217585966d6aacd49c1081dcb4ac07ed0634c2c9= (without huge pages)
    #+begin_example
    27.66user 106.08system 3:10.30elapsed 70%CPU (0avgtext+0avgdata 488282588maxresident)k
    0inputs+10240outputs (0major+130176864minor)pagefaults 0swaps
    #+end_example
  + Commit =23a2dd3da34601f699e3615f9d48f635d331e579= (with huge pages)
    #+begin_example
    17.11user 0.48system 0:17.60elapsed 99%CPU (0avgtext+0avgdata 1332maxresident)k
    0inputs+0outputs (0major+238480minor)pagefaults 0swaps
    #+end_example
  + Summary:
    | Metric          | With the copies | Without the copies |   Ratio |
    |-----------------+-----------------+--------------------+---------|
    | Wall-clock time |          190.30 |              17.60 |   10.81 |
    | System time     |          106.08 |               0.48 |  221.00 |
    | User time       |           27.66 |              17.11 |    1.62 |
    | Page faults     |       130176864 |             238480 |  545.86 |
    #+TBLFM: $4=$2/$3;f2
  + The size of the huge pages on my system is 2048 kB. Therefore we expected to do about 512 times less page
    faults. The reality is slightly better, with a ratio of 545. The improvement in terms of system time (ratio of 221)
    or CPU utilization (form 70%) is also nice.
*** 2017-05-09 Tuesday
**** DONE Experiments for energy paper [3/3]
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:36]
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:36]
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:36]
- State "TODO"       from              [2017-05-09 Tue 15:27]
:END:
- [X] Replace =smpi_usleep= to be able to have an energy estimation.
- [X] Run the same experiment than the paper with optimized HPL (N=20000, nbproc=12*12).
- [X] Run experiments at larger scale. Fix N=65536 and test until nbproc=12*128 (use a 2-level fat tree topology).
**** Add support for huge pages in Simgrid                        :SMPI:C:
- Now, to use huge pages with shared malloc, the following option can be passed to =smpirun=:
  #+begin_example
  --cfg=smpi/shared-malloc-hugepage:/path/to/huge/mount/point
  #+end_example
**** Comparing HPL with and without huge pages :SMPI:R:EXPERIMENTS:PERFORMANCE:HPL:
- The script has been modified to use (or not) huge pages. It also outputs a rough estimation of the memory consumption,
  since the metric we had for the page table size is apparently broken. The estimation is simply the difference of total
  memory consumption of the system, so it is highly subject to noise.
- Simgrid commit: =5322c1a2292172a7f8709f4fdcf42e0e1e5e3f68=
- HPL commit: =1b9c7d4e24e685908000e8c874211b0d4ff90a0e=
- Script commit: =4cf832702fe49f85dd41aa1b3826c38584b2edab=
- Command line to run the experiment:
  #+begin_src sh
  ./run_measures.py --global_csv hpl_no_hugepage.csv --nb_runs 1 --size 50000,100000,150000,200000,250000,300000
  --nb_proc 56 --fat_tree "2;8,8;1,8;1,1" --experiment HPL

  ./run_measures.py --global_csv hpl_hugepage.csv --hugepage /home/huge --nb_runs 1 --size
  50000,100000,150000,200000,250000,300000 --nb_proc 56 --fat_tree "2;8,8;1,8;1,1" --experiment HPL && date
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  data_with_hugepage <- read.csv('hpl_analysis/hpl_hugepage.csv')
  data_with_hugepage$hugepage = TRUE
  data_without_hugepage <- read.csv('hpl_analysis/hpl_no_hugepage.csv')
  data_without_hugepage$hugepage = FALSE
  data = rbind(data_with_hugepage, data_without_hugepage)
  data[data$size == 300000,]
  #+end_src

  #+RESULTS:
  :        topology nb_roots nb_proc   size     time Gflops simulation_time
  : 5 2;8,8;1,8;1,1        8      56 300000 175768.5  102.4         182.478
  : 9 2;8,8;1,8;1,1        8      56 300000 175772.5  102.4         345.459
  :   application_time user_time system_time major_page_fault minor_page_fault
  : 5          24.2901    168.08       14.34                0            20619
  : 9          55.4340    187.40       42.02                0         12056395
  :   cpu_utilization      uss          rss page_table_size memory_size hugepage
  : 5            0.99 21372928    245661696         1260000    82120704     TRUE
  : 9            0.66 22872064 725257936896      1383856000  1548226560    FALSE

  #+begin_src R :file hpl_analysis/54.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data, aes(x=size, y=simulation_time, color=hugepage)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time for different matrix sizes\nNumber of processes: 56")
  plot2 = ggplot(data, aes(x=size, y=memory_size, color=hugepage)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Total memory consumption for different matrix sizes\nNumber of processes: 56")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/54.png]]

  #+begin_src R :file hpl_analysis/55.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data, aes(x=size, y=minor_page_fault, color=hugepage)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Page faults for different matrix sizes\nNumber of processes: 56")
  plot2 = ggplot(data, aes(x=size, y=cpu_utilization, color=hugepage)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("CPU utilization for different matrix sizes\nNumber of processes: 56")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/55.png]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(data_without_hugepage)
  aggr_new = aggregate_results(data_with_hugepage)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_new$Gflops
  #+end_src

  #+begin_src R :file hpl_analysis/56.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  ggplot(aggr_new[aggr_new$nb_proc==56,], aes(x=size, y=Gflops_error)) +
      stat_summary(fun.y = mean, geom="line")+
      stat_summary(fun.y = mean, geom="point")+
      expand_limits(x=0, y=0)+
      ggtitle("Gflops estimation error for different matrix sizes\nUsing 56 MPI processes")
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/56.png]]

- The problem of low CPU utilization is fixed, at least for the considered sizes, and the simulation time is linear.
- The effect of using huge tables is very visible: the number of page faults dropped, as well as the memory consumption.
- Run a large test: N=1e6 and P*Q=512.
  #+begin_src sh
  ./run_measures.py --global_csv hpl_1M.csv --nb_runs 1 --size 1000000 --nb_proc 512 --fat_tree "2;16,32;1,16;1,1"
  --experiment HPL --hugepage /home/huge
  #+end_src
  Performances:
  #+begin_src R :results output :session *R* :exports both
  read.csv('hpl_analysis/hpl_1M.csv')
  #+end_src

  #+RESULTS:
  :           topology nb_roots nb_proc    size     time Gflops simulation_time
  : 1 2;16,32;1,16;1,1       16     512 1000000 707829.7  941.8         2788.48
  :   application_time user_time system_time major_page_fault minor_page_fault
  : 1          441.101   2560.64      223.63                0          1412996
  :   cpu_utilization       uss        rss page_table_size memory_size
  : 1            0.99 162611200 2297610240        10628000   947236864

  So, the simulation lasted a bit more than 46 minutes and took slightly less than 1GB of memory.
  The CPU utilization is high (99%) and only a small amount of time is spent in the application (15% of the total time)
  or in the kernel (7% of the total time). Also, the number of page faults is reasonably low (1.4e6).
- Simulating Stampede now seems to be feasible, except if a new “bad surprise” pops out. As a reminder, with Stampede,
  N=3875000 (3.875 times larger) and P*Q = 77*78 (11 times larger).
**** DONE Integrate the huge page optimization in Simgrid
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-31 Wed 15:20]
- State "TODO"       from              [2017-05-09 Tue 18:50]
:END:
*** 2017-05-10 Wednesday
**** Setting up SSH                                          :SHELL:TOOLS:
- Create this file:
  #+begin_src sh :results output
  cat ~/.ssh/config
  #+end_src

  #+RESULTS:
  #+begin_example
  # See https://www.grid5000.fr/mediawiki/index.php/SSH#Using_SSH_with_ssh_proxycommand_setup_to_access_hosts_inside_Grid.275000

  ### First jumphost. Directly reachable
  Host g5k
    User tocornebize
    HostName access.grid5000.fr

  # Automatic connection to hosts within Grid'5000, and not reachable direction on SSH tcp port 22
  Host *.g5k
    User tocornebize
    ProxyCommand ssh g5k -W "`basename %h .g5k`:%p"
    ForwardAgent no
  #+end_example
- It is now possible to connect directly to =lyon= with the command:
  #+begin_src sh
  ssh lyon.g5k
  #+end_src
**** Replacing =smpi_usleep=                                      :SMPI:HPL:
- Using =smpi_usleep= instead of =HPL_dgemm= is fine to have a good estimation of the time, but not the energy. It should be
  replaced by something like =smpi_execute=.
- Just replacing one by the other does not work, the Gflops estimation is broken (approximately 5 times lower).
- The issue is that =smpi_usleep= expect a virtual time but =smpi_execute= expect a real time. Currently, the time has been
  estimated by doing a linear regression with times measured *in* HPL with =gettimeofday=. This function is overloaded by
  Simgrid, so these are virtual times.
- A dirty fix is to multiply the time passed to =smpi_execute= by the ratio =simulated_gflops/host_gflops=.
- Another solution is to do the linear regression with real times. For this, we need to measure these times outside of
  HPL.
- Let’s reuse what was done on [2017-03-07 Tue] for =dgemm=. A quick test shows that if we use the coefficient obtained
  with this linear regression (without multiplying by the ratio), we get similar Gflops.
- We can do a similar thing for =dtrsm=. But then, the Gflops get broken (e.g. 69 Gflops instead of 84 Gflops for N=40000
  and P=Q=8). Something strange is that the coefficients for real and virtual times did not evolve the same way for
  =dgemm= and =dtrsm=:
  + For =dgemm=:
    - Virtual time: 1.064e-09*M*N*K
    - Real    time: 1.664855e-10*M*N*K
    - The ratio between the two is 6.39
  + For =dtrsm=:
    - Virtual time: 9.246e-08*M*N
    - Real    time: 4.330120e-07*M*N
    - The ratio between the two is 0.21
- Found out that the time complexity of =dtrsm= is cubic and not quadratic. It seems that in HPL, M and N are never
  “large” at the same time. For instance, in one execution, for a high number of calls, M is approximately 5000 and N
  is 120. In the measures made outside of HPL, M and N are always equal. This may be the reason for the different
  coefficients we found.

  #+begin_src R :results output :session *R* :exports both
  data = read.csv('dtrsm_times.csv')
  reg = lm(time ~ polym(m, n, degree=3, raw=TRUE), data)
  summary(reg)
  reg = lm(time ~ I(m*n**2), data)
  summary(reg)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = time ~ polym(m, n, degree = 3, raw = TRUE), data = data)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.094439 -0.019057 -0.001875  0.016962  0.150670 

  Coefficients:
                                           Estimate Std. Error t value Pr(>|t|)
  (Intercept)                            -1.027e-01  1.552e-01  -0.662   0.5157
  polym(m, n, degree = 3, raw = TRUE)1.0  2.073e-04  1.034e-04   2.005   0.0587
  polym(m, n, degree = 3, raw = TRUE)2.0 -8.602e-08  3.747e-08  -2.296   0.0326
  polym(m, n, degree = 3, raw = TRUE)3.0  8.602e-12  4.837e-12   1.778   0.0906
  polym(m, n, degree = 3, raw = TRUE)0.1  1.194e-05  1.398e-04   0.085   0.9328
  polym(m, n, degree = 3, raw = TRUE)1.1 -5.616e-08  5.679e-08  -0.989   0.3345
  polym(m, n, degree = 3, raw = TRUE)2.1  1.619e-11  7.878e-12   2.055   0.0531
  polym(m, n, degree = 3, raw = TRUE)0.2  3.497e-08  4.706e-08   0.743   0.4660
  polym(m, n, degree = 3, raw = TRUE)1.2  8.316e-11  6.396e-12  13.001 3.26e-11
  polym(m, n, degree = 3, raw = TRUE)0.3 -6.396e-12  6.186e-12  -1.034   0.3135

  (Intercept)                               
  polym(m, n, degree = 3, raw = TRUE)1.0 .  
  polym(m, n, degree = 3, raw = TRUE)2.0 *  
  polym(m, n, degree = 3, raw = TRUE)3.0 .  
  polym(m, n, degree = 3, raw = TRUE)0.1    
  polym(m, n, degree = 3, raw = TRUE)1.1    
  polym(m, n, degree = 3, raw = TRUE)2.1 .  
  polym(m, n, degree = 3, raw = TRUE)0.2    
  polym(m, n, degree = 3, raw = TRUE)1.2 ***
  polym(m, n, degree = 3, raw = TRUE)0.3    
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.05122 on 20 degrees of freedom
  Multiple R-squared:  0.9997,	Adjusted R-squared:  0.9995 
  F-statistic:  6733 on 9 and 20 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = time ~ I(m * n^2), data = data)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.095222 -0.020818 -0.004185  0.007353  0.289151 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 1.752e-02  1.454e-02   1.205    0.238    
  I(m * n^2)  8.690e-11  4.305e-13 201.835   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.06245 on 28 degrees of freedom
  Multiple R-squared:  0.9993,	Adjusted R-squared:  0.9993 
  F-statistic: 4.074e+04 on 1 and 28 DF,  p-value: < 2.2e-16
  #+end_example

- With a time of 8.690e-11*M*N*N, the Gflops seem to be ok. We should draw some plots to check it of course.
- It messes up the “time spent in the application”. For instance, for N=40000 and P=Q=8, this value becomes 147.7
  seconds, but the simulation time is still 19.5 seconds.
**** Stop hard-coding the blas coefficients in HPL            :SMPI:C:HPL:
- The coefficients used to compute the time to sleep were hard-coded in =include/hpl_blas.h=.
- Now, they are two constants that can be passed as follows:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=1.658198e-10 -DSMPI_DTRSM_COEFFICIENT=8.624970e-11" -j 4
  arch=SMPI
  #+end_src
- If the compilation is done with =SMPI_OPTIMIZATION= defined but one of the coefficients is not defined, then the
  compilation fails with an error message.
*** 2017-05-11 Thursday
**** New results will not be obtained with the *same* HPL than the energy paper :SMPI:HPL:
- In the experiments made for the paper, the only option passed to HPL at compile time is =HPL_CALL_CBLAS= (see the
  [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/src/hpl-2.2/Make.SMPI#L160][Makefile]]).
- This means that =MPI_Datatype= is used in the experiments, since it is the default.
- For optimized HPL, this “feature” is not used anymore (option =-DHPL_NO_MPI_DATATYPE= is passed). It is required to
  avoid an unsolved bug (not sure if it is in HPL or Simgrid). See [2017-04-04 Tue] for the discussion.
- Maybe this is not a big deal. But we should keep this in mind.
**** Quick tests for replication of the paper’s results         :SMPI:HPL:
- Looking at [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/tree/master/experiments/simgrid/hpl/taurus_2017-01-30][taurus_2017-01-30]] experiment.
- Command:
  #+begin_src sh
  unlink HPL.dat
  ln -s HPL.dat.${cores} HPL.dat
  smpirun -np $cores -platform platform_taurus_hpl.xml -hostfile ../Debian/hostnames-144
  --log=smpi_bench.thres:critical --cfg=tracing:yes --cfg=tracing/filename:/tmp/tracing --cfg=tracing/smpi:1
  --cfg=tracing/smpi/computing:yes --cfg=smpi/privatize-global-variables:yes --cfg=plugin:Energy ./xhpl
  #+end_src
- Test for cores=1, see [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/experiments/simgrid/hpl/taurus_2017-01-30/original-data/taurus-8.lyon.grid5000.fr_hpl_20000_1_no_turbo_numactl.out][original results]].
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120     1     1             668.77              7.976e+00
  HPL_pdgesv() start time Thu May 11 10:22:37 2017
  
  HPL_pdgesv() end time   Thu May 11 10:22:38 2017
  
  [669.009942] [surf_energy/INFO] Total energy consumption: 1219939.253384 Joules (used hosts: 79444.555556 Joules; unused/idle hosts: 1140494.697827)
  [669.009942] [smpi_kernel/INFO] Simulated time: 669.01 seconds. 
  
  The simulation took 0.7707 seconds (after parsing and platform setup)
  0.734606 seconds were actual computation of the application
  [669.009942] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 63388.691960 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 76936.143276 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 79444.555556 Joules
  [669.009942] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 76936.143276 Joules
  #+end_example
- Test for cores=12, see [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/experiments/simgrid/hpl/taurus_2017-01-30/original-data/taurus-8.lyon.grid5000.fr_hpl_20000_12_no_turbo_numactl.out][original results]].
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              47.10              1.132e+02
  HPL_pdgesv() start time Thu May 11 10:16:03 2017
  
  HPL_pdgesv() end time   Thu May 11 10:16:53 2017
  
  [47.218012] [surf_energy/INFO] Total energy consumption: 89722.952042 Joules (used hosts: 9228.045507 Joules; unused/idle hosts: 80494.906535)
  [47.218012] [smpi_kernel/INFO] Simulated time: 47.218 seconds. 
  
  The simulation took 49.6711 seconds (after parsing and platform setup)
  11.3934 seconds were actual computation of the application
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 4473.906669 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 5430.071419 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 9228.045507 Joules
  [47.218012] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 5430.071419 Joules
  #+end_example
- Test for core=48, see [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/experiments/simgrid/hpl/taurus_2017-01-30/original-data/taurus-8.lyon.grid5000.fr_hpl_20000_48_no_turbo_numactl.out][original results]].
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     4              22.16              2.407e+02
  HPL_pdgesv() start time Thu May 11 10:18:59 2017
  
  HPL_pdgesv() end time   Thu May 11 10:20:07 2017
  
  [22.271603] [surf_energy/INFO] Total energy consumption: 47130.713380 Joules (used hosts: 16846.901341 Joules; unused/idle hosts: 30283.812039)
  [22.271603] [smpi_kernel/INFO] Simulated time: 22.2716 seconds. 
  
  The simulation took 68.6603 seconds (after parsing and platform setup)
  55.6854 seconds were actual computation of the application
  [22.271603] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 2110.234374 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 2561.234333 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 4187.844290 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 4184.906795 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 4305.822733 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 4168.327523 Joules
  [22.271603] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 2561.234333 Joules
  #+end_example
- Test for cores=96, see [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/experiments/simgrid/hpl/taurus_2017-01-30/original-data/taurus-8.lyon.grid5000.fr_hpl_20000_96_no_turbo_numactl.out][original results]].
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     8              21.74              2.453e+02
  HPL_pdgesv() start time Thu May 11 09:58:47 2017
  
  HPL_pdgesv() end time   Thu May 11 10:00:24 2017
  
  [21.848108] [surf_energy/INFO] Total energy consumption: 54199.676127 Joules (used hosts: 34541.840706 Joules; unused/idle hosts: 19657.835421)
  [21.848108] [smpi_kernel/INFO] Simulated time: 21.8481 seconds. 
  
  The simulation took 97.4425 seconds (after parsing and platform setup)
  300.45 seconds were actual computation of the application
  [21.848108] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 4617.363031 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 2512.532452 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 4394.181028 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 2070.108259 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 4243.630249 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 4278.366463 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 4231.118445 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 4214.370839 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 4347.066781 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 4215.743870 Joules
  [21.848108] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 2512.532452 Joules
  #+end_example
- Summary of the evolution of the Gflops:
  | Number of processes | Energy paper | Optimized HPL | Difference |
  |---------------------+--------------+---------------+------------|
  |                   1 |    7.798e+00 |     7.976e+00 |     +2.28% |
  |                  12 |    7.343e+01 |     1.132e+02 |    +54.16% |
  |                  48 |    1.843e+02 |     2.407e+02 |    +30.60% |
  |                  96 |    2.730e+02 |     2.453e+02 |    -10.15% |
  #+TBLFM: $4=(($3-$2)/$2)*100;f2
- Well, there is a problem...
**** Tracking down the problem in HPL                       :SMPI:BUG:HPL:
- Let’s look at the output of the simulation for 12 processes and different commits of HPL. 
- Commit =1b9c7d4e24e685908000e8c874211b0d4ff90a0e= (before using =smpi_execute=):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             271.01              1.968e+01
  #+end_example
- Commit =473b935f9951032133b8c9e65fd485507fda79d9= (before removing the last time-consumming functions):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             272.32              1.959e+01
  #+end_example
- Commit =3b68236d432ad380265ea0a9120489f2075b7bed= (before reusing =PANEL->Work=):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             273.18              1.953e+01
  #+end_example
- Commit =6cc643a5c2a123fa549d02a764bea408b5ad6114= (before the partial shared malloc and the =-DHPL_NO_MPI_DATATYPE=
  option):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             272.45              1.958e+01
  #+end_example
- Commit =cbcf309dd67d6075c2983c71acda73a4c3d834b3= (before removing =HPL_dgemv= and the initialization and checks):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             273.25              1.952e+01
  #+end_example
- Commit =6e8531575b9ee3ff344605d191f76f45b2b4dff6= (before using the shared malloc for the matrix and setting =WORK[0]= to
  a constant):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1             272.52              1.957e+01
  #+end_example
- Commit =331ae1c8b12669153d35ca5ee3e4741252cc6d7d= (latest commit), compiled without option =-DSMPI_OPTIMIZATION= (vanilla
  HPL, the final tests on the matrix are passing): 
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              51.96              1.027e+02
  #+end_example
- After some thoughts, it is expected to observe wrong results for the versions where =smpi_usleep= was used. Since the
  linear regression was done on virtual times, the coefficients are valid only for a given speed. And of course, the
  speed used in =platform_taurus_hpl.xml= is different.
  So forget all these intermediate results.
- The Gflops obtained with vanilla HPL is quite close to the one obtained with the latest version of optimized HPL,
  albeit not perfect (10% difference).
**** Quick tests for replication of the paper’s results, ran on taurus-3 :SMPI:HPL:
- Maybe the simulation needs to be done on the target platform (despite the simulation of the big functions like =dgemm=).
- Done in in =taurus-3=, with coefficients obtained yesterday on =taurus-7= (checked afterward, indeed the coefficients of
  =taurus-3= are very similar).
- Compilation:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.006005e-10 -DSMPI_DTRSM_COEFFICIENT=1.027705e-10" -j 4 arch=SMPI
  #+end_src
- Test for cores=12:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              56.87              9.379e+01
  #+end_example
- Test for cores=48:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     4              27.51              1.939e+02
  #+end_example
- Test for cores=12, with vanilla HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              60.88              8.762e+01
  #+end_example
- The results are closer to the ones of the energy paper, but still too far. Even the results with vanilla HPL are far.
  Two possible reasons:
  + The option =-DHPL_NO_MPI_DATATYPE= was passed for the compilation.
  + Did not do exactly the same thing than for the energy paper. The first three lines of the [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/blob/master/experiments/simgrid/hpl/taurus_2017-01-30/execution/simgrid_experiments_hpl.zsh][script]] were not called:
    #+begin_example
    ~/experiments/setup_cores.zsh 12
    ~/experiments/disable_turbomode.zsh
    sudo cpupower -c all frequency-set -g userspace && sudo cpupower -c all frequency-set -d 2300MHz -u 2300MHz
    #+end_example
    And the option =-wrapper "numactl --cpunodebind=1"= was not passed to =smpirun=.
- Note that the difference between optimized HPL and vanilla HPL for 12 processes is a bit large (7%).
- Test for cores=12 with vanilla HPL, with the two commands =cpupower= called and the wrapping with =numactl= but not the
  two scripts =setup_cores.zsh= and =disable_turbomode.zsh= (did not have them):
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              72.74              7.333e+01
  HPL_pdgesv() start time Thu May 11 13:12:57 2017
  
  HPL_pdgesv() end time   Thu May 11 13:27:30 2017
  
  --------------------------------------------------------------------------------
  ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0008515 ...... PASSED
  ||Ax-b||_oo  . . . . . . . . . . . . . . . . . =           0.000000
  ||A||_oo . . . . . . . . . . . . . . . . . . . =        5077.970709
  ||A||_1  . . . . . . . . . . . . . . . . . . . =        5073.290150
  ||x||_oo . . . . . . . . . . . . . . . . . . . =          14.024664
  ||x||_1  . . . . . . . . . . . . . . . . . . . =       50326.296352
  ||b||_oo . . . . . . . . . . . . . . . . . . . =           0.499989
  [75.738071] [surf_energy/INFO] Total energy consumption: 144148.886646 Joules (used hosts: 15034.410090 Joules; unused/idle hosts: 129114.476556)
  [75.738071] [smpi_kernel/INFO] Simulated time: 75.7381 seconds. 
  
  The simulation took 907.842 seconds (after parsing and platform setup)
  794.039 seconds were actual computation of the application
  [75.738071] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 7176.182228 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 8709.878166 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 15034.410090 Joules
  [75.738071] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 8709.878166 Joules
  #+end_example
- Finally get good results with vanilla HPL!
- It is strange that what is done with the commands =cpupower= and =numactl= decreases the (estimated) performances.
- Let’s retry with optimized HPL.
- Redo the linear regression, to take into account the =cpupower= commands (but not the =numactl=, this is more
  complicated). And indeed, the coefficients changed (performances are also decreased). Now, compile with:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.445036e-10 -DSMPI_DTRSM_COEFFICIENT=1.259681e-10" -j 4 arch=SMPI
  #+end_src
- Test for cores=12 with optimized HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     1              68.48              7.790e+01
  HPL_pdgesv() start time Thu May 11 13:47:57 2017
  
  HPL_pdgesv() end time   Thu May 11 13:49:53 2017
  
  [68.592617] [surf_energy/INFO] Total energy consumption: 130511.427887 Joules (used hosts: 13578.164907 Joules; unused/idle hosts: 116933.262980)
  [68.592617] [smpi_kernel/INFO] Simulated time: 68.5926 seconds. 
  
  The simulation took 116.28 seconds (after parsing and platform setup)
  28.3679 seconds were actual computation of the application
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 6499.150413 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 7888.150898 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 13578.164907 Joules
  [68.592617] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 7888.150898 Joules
  #+end_example
- The time and the Gflops are quite close to vanilla HPL (Gflops increased by 6%). The energy consumption is a bit less
  accurate (decreased by 9%), but it seems to be due to the fact that it also accounts for the initialization and the
  tests, since the total time for each node also decreased by 9%. To fix this, we could re-enable the initialization and
  the checks.
- Test for cores=48 with optimized HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12     4              32.36              1.648e+02
  HPL_pdgesv() start time Thu May 11 13:54:03 2017
  
  HPL_pdgesv() end time   Thu May 11 13:56:50 2017
  
  [32.466707] [surf_energy/INFO] Total energy consumption: 69392.610702 Joules (used hosts: 25246.006513 Joules; unused/idle hosts: 44146.604190)
  [32.466707] [smpi_kernel/INFO] Simulated time: 32.4667 seconds. 
  
  The simulation took 167.219 seconds (after parsing and platform setup)
  155.337 seconds were actual computation of the application
  [32.466707] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 3076.220443 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 3733.671250 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 6282.601677 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 6285.912571 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 6451.442977 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 6226.049288 Joules
  [32.466707] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 3733.671250 Joules
  #+end_example
  The Gflops decreased by 10%. The energy consumption of =taurus-1= increased by 9%.
- Test for cores=144 with optimized HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              47.63              1.120e+02
  HPL_pdgesv() start time Thu May 11 14:10:34 2017
  
  HPL_pdgesv() end time   Thu May 11 14:16:32 2017
  
  [47.745189] [surf_energy/INFO] Total energy consumption: 139957.906486 Joules (used hosts: 118961.959636 Joules; unused/idle hosts: 20995.946850)
  [47.745189] [smpi_kernel/INFO] Simulated time: 47.7452 seconds. 
  
  The simulation took 358.285 seconds (after parsing and platform setup)
  1695.38 seconds were actual computation of the application
  [47.745189] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 9833.209185 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 10395.870005 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 9917.905846 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 10572.284195 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 9921.864610 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 5490.696732 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 5490.696732 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 10045.186571 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 4523.856655 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 9676.450213 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 9760.484174 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 9653.907156 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 9615.261748 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 9931.091611 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 9638.444320 Joules
  [47.745189] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 5490.696732 Joules
  #+end_example
  The Gflops decreased by 65%. It seems that the prediction becomes very unaccurate when the number of processes
  increase. Maybe if the local matrices become too small, then some operations are not negligible anymore (e.g. the
  copies or the other BLAS functions).
- Test for cores=144 with vanilla HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              16.56              3.221e+02
  HPL_pdgesv() start time Thu May 11 14:21:41 2017
  
  HPL_pdgesv() end time   Thu May 11 14:39:39 2017
  
  --------------------------------------------------------------------------------
  ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)= 2394248404.8693275 ...... FAILED
  ||Ax-b||_oo  . . . . . . . . . . . . . . . . . =          61.313920
  ||A||_oo . . . . . . . . . . . . . . . . . . . =        5077.970709
  ||A||_1  . . . . . . . . . . . . . . . . . . . =        5073.290150
  ||x||_oo . . . . . . . . . . . . . . . . . . . =           2.271123
  ||x||_1  . . . . . . . . . . . . . . . . . . . =        8563.901030
  ||b||_oo . . . . . . . . . . . . . . . . . . . =           0.499989
  [16.918235] [surf_energy/INFO] Total energy consumption: 47289.777830 Joules (used hosts: 39849.983792 Joules; unused/idle hosts: 7439.794038)
  [16.918235] [smpi_kernel/INFO] Simulated time: 16.9182 seconds. 
  
  The simulation took 1112.83 seconds (after parsing and platform setup)
  803.275 seconds were actual computation of the application
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 3294.134621 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 3482.679266 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 3318.958204 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 3539.088201 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 3319.812144 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 1945.597076 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 1945.597076 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 3359.424506 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 1603.002809 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 3243.553112 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 3270.738159 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 3235.614523 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 3225.269605 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 3327.203930 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 3233.507520 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 1945.597076 Joules
  #+end_example
- Just noticed that the old way to do the privatization was used for the previous tests, not =dlopen=. This does not
  impact the results, but the experiments could be faster.
- Test for cores=144, with vanilla HPL *except* for =HPL_dgemm= and =HPL_dtrsm= that are still replaced by =smpi_execute=:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              55.85              9.551e+01
  HPL_pdgesv() start time Thu May 11 15:12:58 2017
  
  HPL_pdgesv() end time   Thu May 11 15:19:32 2017
  
  --------------------------------------------------------------------------------
  ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=             -nan ...... FAILED
  ||Ax-b||_oo  . . . . . . . . . . . . . . . . . =               -nan
  ||A||_oo . . . . . . . . . . . . . . . . . . . =        5077.970709
  ||A||_1  . . . . . . . . . . . . . . . . . . . =        5073.290150
  ||x||_oo . . . . . . . . . . . . . . . . . . . =               -nan
  ||x||_1  . . . . . . . . . . . . . . . . . . . =               -nan
  ||b||_oo . . . . . . . . . . . . . . . . . . . =           0.499989
  [56.202279] [surf_energy/INFO] Total energy consumption: 165778.917841 Joules (used hosts: 141063.965556 Joules; unused/idle hosts: 24714.952285)
  [56.202279] [smpi_kernel/INFO] Simulated time: 56.2023 seconds. 
  
  The simulation took 429.128 seconds (after parsing and platform setup)
  2361.36 seconds were actual computation of the application
  [56.202279] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 11664.208514 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 12317.280677 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 11751.027002 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 12526.259406 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 11761.333393 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 6463.262110 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 6463.262110 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 11906.251512 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 5325.165956 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 11477.542760 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 11581.368553 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 11446.161421 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 11412.087285 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 11785.925720 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 11434.519313 Joules
  [56.202279] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 6463.262110 Joules
  #+end_example
- Test for cores=144, with vanilla HPL *except* for =HPL_dgemm= that is still replaced by =smpi_execute=:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              99.89              5.340e+01
  HPL_pdgesv() start time Thu May 11 15:25:10 2017
  
  HPL_pdgesv() end time   Thu May 11 15:33:05 2017
  
  --------------------------------------------------------------------------------
  ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=             -nan ...... FAILED
  ||Ax-b||_oo  . . . . . . . . . . . . . . . . . =               -nan
  ||A||_oo . . . . . . . . . . . . . . . . . . . =        5077.970709
  ||A||_1  . . . . . . . . . . . . . . . . . . . =        5073.290150
  ||x||_oo . . . . . . . . . . . . . . . . . . . =               -nan
  ||x||_1  . . . . . . . . . . . . . . . . . . . =               -nan
  ||b||_oo . . . . . . . . . . . . . . . . . . . =           0.499989
  [100.242310] [surf_energy/INFO] Total energy consumption: 298711.491383 Joules (used hosts: 254629.935402 Joules; unused/idle hosts: 44081.555980)
  [100.242310] [smpi_kernel/INFO] Simulated time: 100.242 seconds. 
  
  The simulation took 509.256 seconds (after parsing and platform setup)
  5827.05 seconds were actual computation of the application
  [100.242310] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 21058.306334 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 22226.647446 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 21215.183212 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 22613.195593 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 21218.306339 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 11527.865691 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 11527.865691 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 21490.275166 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 9497.958907 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 20715.241315 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 20905.090684 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 20658.177928 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 20604.557130 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 21283.276263 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 20641.677992 Joules
  [100.242310] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 11527.865691 Joules
  #+end_example
- Test for cores=144 with optimized HPL *except* for =HPL_dgemm= that still calls =cblas_dgemm=:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12             209.19              2.550e+01
  HPL_pdgesv() start time Thu May 11 15:36:45 2017
  
  HPL_pdgesv() end time   Thu May 11 16:07:48 2017
  
  [209.301460] [surf_energy/INFO] Total energy consumption: 530059.819522 Joules (used hosts: 438019.502541 Joules; unused/idle hosts: 92040.316981)
  [209.301460] [smpi_kernel/INFO] Simulated time: 209.301 seconds. 
  
  The simulation took 1863.78 seconds (after parsing and platform setup)
  9002.05 seconds were actual computation of the application
  [209.301460] [smpi_kernel/INFO] More than 75% of the time was spent inside the application code.
  You may want to use sampling functions or trace replay to reduce this.
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 36531.836413 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 38637.126679 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 36497.368703 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 39099.444657 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 36234.444929 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 24069.667886 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 24069.667886 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 36838.650141 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 19831.313323 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 35404.207347 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 35698.221614 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 35148.998425 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 35299.581957 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 36531.189596 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 36098.432082 Joules
  [209.301460] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 24069.667886 Joules
  #+end_example
- Trying to use another coefficient for =dgemm=, to artificially accelerate the function. Recall that we used
  2.445036e-10) in previous experiments.
  With a coefficient of 8.507603478260869e-11 (2.875 times smaller): 133.7 Gflops.
  Note that the Gflops did not change much (1.1875 times larger). This suggests that =dgemm= is maybe not the function
  that takes most of the time.
- To sum up:
  + Using =SMPI_OPTIMIZATION= or not for functions other than =dgemm= and =dtrsm= has a very limited impact on the Gflops.
  + Using =SMPI_OPTIMIZATION= or not for =dgemm= and =dtrsm= has a large impact.
  + Changing the coefficient of =dgemm= has an impact, but not so high.
**** Execution time of optimized HPL with(out) energy plugin and trace :SMPI:PERFORMANCE:HPL:
- The execution of optimized HPL took nearly six minutes. This seems to be high, in comparison with the times we used to have.
- An hypothesis is that it is due to the energy plugin. Another hypothesis is the trace. Recall that the simulation took
  359 seconds and estimated 112 Gflops.
- Without the energy plugin but with the trace: 343.5 seconds, estimated 1.164e+02 Gflops.
- With the energy plugin but without the trace: 191.6 seconds, estimated 1.534e+02 Gflops.
- So the tracing is taking a lot of time. Strangely, it also has a significant impact on the Gflops (but the value is
  still wrong).
**** Trace visualization of the energy paper’s simulations :SMPI:TRACING:HPL:
- Visualize the traces obtained with N=20000 and P=Q=12 (still the file =HPL.dat.144=).
- Be carefull with =vite=. It consumes a lot of memory and has an important memory leak (the memory consumption increases
  when we zoom/unzoom and quickly grows larger than 16GB, freezing the computer and forcing to reboot).
- The full traces:
  + Vanilla HPL:
    [[file:trace_visualization/taurus/vanilla_full.png]]
  + Optimized HPL:
    [[file:trace_visualization/taurus/optimized_full.png]]
- Traces between 6 and 7 seconds (these times are arbitrary):
  + Vanilla HPL:
    [[file:trace_visualization/taurus/vanilla_6-7.png]]
  + Optimized HPL:
    [[file:trace_visualization/taurus/optimized_6-7.png]]
- In the full traces, we can see two phases where there are abnormally long receive, at 22 and 28 seconds. But this is
  not enough to cause the high differences observed.
- In the zoomed traces, we see that between each synchronization, there are two computation phases. The first one is
  made of a single large computation, the second is made of a lot of small computations. In vanilla HPL, these two
  phases take roughly the same time, but in optimized HPL the second phase is roughly four times longer.
- The estimated time for optimized HPL is 2.88 times larger than the estimated time for vanilla HPL, and there seems to
  be more time spent in computation than in communication, so this long “second phase” may be the issue.
- We should check if these second phase is longer because the function calls are longer, or because there are more
  function calls. Just by looking, the function calls seem longer in optimized HPL, but this is hard to see.
- The processes are not all equal in optimized HPL. Some of them have another long function call in their second phase (thus two long
  function calls in total), some others have only short function calls. This does not happen in vanilla HPL. But this
  does not seem to cause idle time (except if idle time is counted as computations).
*** 2017-05-12 Friday
**** Keep looking at the trace                          :SMPI:TRACING:HPL:
- Run =pj_dump=:
  #+begin_src sh
  pj_dump --user-defined ~/tmp/15/trace_vanilla   > ~/tmp/15/dump_vanilla
  pj_dump --user-defined ~/tmp/15/trace_optimized > ~/tmp/15/dump_optimized
  #+end_src
- Quick counting of the states:
  #+begin_src sh :results output
  grep State ~/tmp/15/dump_vanilla | cut -d"," -f8 | sed 's| ||g' | sort | uniq -c
  #+end_src

  #+RESULTS:
  : 5424300 computing
  :     143 PMPI_Finalize
  :     144 PMPI_Init
  :  774232 PMPI_Irecv
  :  275054 PMPI_Recv
  : 1049286 PMPI_Send
  :  774232 PMPI_Wait

  #+begin_src sh :results output
  grep State ~/tmp/15/dump_optimized | cut -d"," -f8 | sed 's| ||g' | sort | uniq -c
  #+end_src

  #+RESULTS:
  : 6565169 computing
  :     143 PMPI_Finalize
  :     144 PMPI_Init
  :  774232 PMPI_Irecv
  :  253870 PMPI_Recv
  : 1028102 PMPI_Send
  :  774232 PMPI_Wait

- This is a bit strange. The numbers of =PMPI_Irecv= and =PMPI_Wait= are exactly the same in the two files. But the numbers
  of =computing=, =PMPI_Recv= and =PMPI_Send= are different. There are more =computing= in the optimized version, but more
  =PMPI_Recv= and =PMPI_Send= in the vanilla version.
**** Trace the calls to =dgemm= and =dtrsm=                 :SMPI:TRACING:HPL:
- Run optimized HPL on =taurus-8= and vanilla HPL on =taurus-11=.
- Optimized HPL compiled with:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_MEASURE -DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.445036e-10
  -DSMPI_DTRSM_COEFFICIENT=1.259681e-10" -j 4 arch=SMPI
  #+end_src
- Vanilla HPL compiled with:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_MEASURE" arch=SMPI 
  #+end_src
- On both machines, run this:
  #+begin_src sh
  cpupower -c all frequency-set -g userspace
  cpupower -c all frequency-set -d 2300MHz -u 2300MHz
  unlink HPL.dat
  cores=144
  ln -s HPL.dat.${cores} HPL.dat
  #+end_src
  #+begin_src sh
  smpirun -wrapper "numactl --cpunodebind=1" -np $cores -platform platform_taurus_hpl.xml -hostfile
  ../Debian/hostnames-144 --log=smpi_bench.thres:critical --cfg=tracing:yes --cfg=tracing/filename:/tmp/tracing
  --cfg=tracing/smpi:1 --cfg=tracing/smpi/computing:yes --cfg=smpi/privatize-global-variables:yes --cfg=plugin:Energy
  ./xhpl >| output
  #+end_src
- Time and Gflops:
  #+begin_src sh :results output
  tail /home/tom/tmp/16/taurus_144/output_vanilla   -n 16 | head -n 4
  tail /home/tom/tmp/16/taurus_144/output_optimized -n  8 | head -n 4
  #+end_src

  #+RESULTS:
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12    12              16.98              3.141e+02
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12    12              40.11              1.330e+02

- Processing:
#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:
        csv_writer = csv.writer(out_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'time'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 12))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
process('/home/tom/tmp/16/taurus_144/output_optimized', '/home/tom/tmp/16/taurus_144/calls_optimized.csv')
process('/home/tom/tmp/16/taurus_144/output_vanilla',   '/home/tom/tmp/16/taurus_144/calls_vanilla.csv')
#+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  calls_vanilla <- read.csv("/home/tom/tmp/16/taurus_144/calls_vanilla.csv");
  vanilla_dgemm = calls_vanilla[calls_vanilla$func == "dgemm",]
  vanilla_dtrsm = calls_vanilla[calls_vanilla$func == "dtrsm",]
  calls_optimized <- read.csv("/home/tom/tmp/16/taurus_144/calls_optimized.csv");
  optimized_dgemm = calls_optimized[calls_optimized$func == "dgemm",]
  optimized_dtrsm = calls_optimized[calls_optimized$func == "dtrsm",]
  #+end_src

  #+RESULTS:

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  get_stats <- function(results) {
      data = data.table(results)
      x = as.data.frame(data[, list(count=length(time), min_time=min(time), max_time=max(time), mean_time=mean(time), total_time=sum(time)), by=c("func", "file", "line")])
      return(x[with(x, order(-count)),])
  }
  get_stats_rank <- function(results, rank) {
      return(get_stats(results[results$rank == rank,]))
  }
  #+end_src

  #+begin_src R :results output :session *R* :exports both
  get_stats_rank(vanilla_dgemm, 0)
  get_stats_rank(optimized_dgemm, 0)
  #+end_src

  #+RESULTS:
  #+begin_example
     func                                file line count min_time max_time
  1 dgemm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222  1652 0.000000 0.001810
  3 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  407   143 0.000496 0.075240
  2 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  384    13 0.000511 0.076601
       mean_time total_time
  1 2.677663e-05   0.044235
  3 2.814373e-02   4.024553
  2 2.828262e-02   0.367674
     func                                file line count min_time max_time
  1 dgemm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222  1652 0.000000 0.005900
  3 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  407   143 0.000552 0.224263
  2 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  384    13 0.002495 0.153763
       mean_time total_time
  1 0.0001082125   0.178767
  3 0.0732114825  10.469242
  2 0.0806426923   1.048355
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  get_stats_rank(vanilla_dtrsm, 0)
  get_stats_rank(optimized_dtrsm, 0)
  #+end_src

  #+RESULTS:
  #+begin_example
     func                                file line count min_time max_time
  1 dtrsm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171  1652 0.000000 0.000080
  2 dtrsm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363   156 0.000342 0.004531
       mean_time total_time
  1 3.729419e-06   0.006161
  2 2.359615e-03   0.368100
     func                                file line count min_time max_time
  1 dtrsm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171  1652 0.000000 0.000677
  2 dtrsm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363   156 0.000914 0.066385
       mean_time total_time
  1 5.848668e-06   0.009662
  2 3.366344e-02   5.251497
  #+end_example
- So there are *exactly* the same number of calls to =dgemm= and =dtrsm= at the same locations for optimized and vanilla
  HPL. However, the calls in optimized HPL have a longer time. There is a diference of roughly 7 seconds for process 0.
- Let’s look at the sizes.
  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  get_stats_sizes <- function(results) {
      data = data.table(results)
      x = as.data.frame(data[, list(count=length(time), min_m=min(m), max_m=max(m), mean_m=mean(m), min_n=min(n),
                                    max_n=max(n), mean_n=mean(n), min_k=min(k), max_k=max(k), mean_k=mean(k)), by=c("func", "file", "line")])
      return(x[with(x, order(-count)),])
  }
  get_stats_sizes_rank <- function(results, rank) {
      return(get_stats_sizes(results[results$rank == rank,]))
  }
  #+end_src

  #+RESULTS:

  #+begin_src R :results output :session *R* :exports both
  get_stats_sizes_rank(vanilla_dgemm, 0)
  get_stats_sizes_rank(optimized_dgemm, 0)
  #+end_src

  #+RESULTS:
  #+begin_example
     func                                file line count min_m max_m     mean_m
  1 dgemm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222  1652     2    60   6.033898
  3 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  407   143   120  1560 840.000000
  2 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  384    13   120  1560 840.000000
    min_n max_n   mean_n min_k max_k     mean_k
  1     2  1680 843.1186     0    60   3.118644
  3   120  1560 840.0000   120   120 120.000000
  2   120  1560 840.0000   120   120 120.000000
     func                                file line count min_m max_m     mean_m
  1 dgemm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  222  1652     2    60   6.033898
  3 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  407   143   120  1560 840.000000
  2 dgemm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  384    13   120  1560 840.000000
    min_n max_n   mean_n min_k max_k     mean_k
  1     2  1680 843.1186     0    60   3.118644
  3   120  1560 840.0000   120   120 120.000000
  2   120  1560 840.0000   120   120 120.000000
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  get_stats_sizes_rank(vanilla_dtrsm, 0)
  get_stats_sizes_rank(optimized_dtrsm, 0)
  #+end_src

  #+RESULTS:
  #+begin_example
     func                                file line count min_m max_m     mean_m
  1 dtrsm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171  1652     0    60   3.118644
  2 dtrsm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363   156   120   120 120.000000
    min_n max_n     mean_n min_k max_k mean_k
  1     2    60   6.033898    -1    -1     -1
  2   120  1560 840.000000    -1    -1     -1
     func                                file line count min_m max_m     mean_m
  1 dtrsm  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171  1652     0    60   3.118644
  2 dtrsm /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363   156   120   120 120.000000
    min_n max_n     mean_n min_k max_k mean_k
  1     2    60   6.033898    -1    -1     -1
  2   120  1560 840.000000    -1    -1     -1
  #+end_example

- The parameters of the calls seems to be exactly the same. So maybe the regression was wrong?
- New regression for =dgemm=:
  #+begin_src R :results output :session *R* :exports both
  reg = lm(time~polym(m, n, k, degree=3, raw=TRUE), data=vanilla_dgemm)
  summary(reg)
  reg = lm(time~I(m*n*k), data=vanilla_dgemm)
  summary(reg)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = time ~ polym(m, n, k, degree = 3, raw = TRUE), data = vanilla_dgemm)

  Residuals:
         Min         1Q     Median         3Q        Max 
  -1.691e-03 -7.140e-06  1.750e-06  7.630e-06  2.090e-03 

  Coefficients:
                                                Estimate Std. Error  t value
  (Intercept)                                  2.070e-05  6.528e-07   31.713
  polym(m, n, k, degree = 3, raw = TRUE)1.0.0 -3.540e-06  6.830e-08  -51.830
  polym(m, n, k, degree = 3, raw = TRUE)2.0.0 -3.763e-08  1.037e-09  -36.276
  polym(m, n, k, degree = 3, raw = TRUE)3.0.0 -9.254e-12  6.981e-14 -132.555
  polym(m, n, k, degree = 3, raw = TRUE)0.1.0 -8.120e-08  2.929e-09  -27.718
  polym(m, n, k, degree = 3, raw = TRUE)1.1.0  1.777e-08  1.102e-10  161.292
  polym(m, n, k, degree = 3, raw = TRUE)2.1.0  1.998e-11  1.295e-13  154.272
  polym(m, n, k, degree = 3, raw = TRUE)0.2.0  4.787e-11  3.994e-12   11.986
  polym(m, n, k, degree = 3, raw = TRUE)1.2.0 -1.075e-11  6.373e-14 -168.715
  polym(m, n, k, degree = 3, raw = TRUE)0.3.0 -3.121e-15  1.565e-15   -1.994
  polym(m, n, k, degree = 3, raw = TRUE)0.0.1  1.669e-06  1.174e-07   14.213
  polym(m, n, k, degree = 3, raw = TRUE)1.0.1  2.179e-07  9.397e-09   23.194
  polym(m, n, k, degree = 3, raw = TRUE)2.0.1  3.260e-10  8.685e-12   37.537
  polym(m, n, k, degree = 3, raw = TRUE)0.1.1 -4.653e-09  2.060e-10  -22.591
  polym(m, n, k, degree = 3, raw = TRUE)1.1.1  8.628e-11  1.091e-12   79.104
  polym(m, n, k, degree = 3, raw = TRUE)0.2.1  7.884e-12  1.017e-13   77.512
  polym(m, n, k, degree = 3, raw = TRUE)0.0.2 -1.573e-07  9.895e-09  -15.898
  polym(m, n, k, degree = 3, raw = TRUE)1.0.2 -1.560e-09  7.799e-11  -19.999
  polym(m, n, k, degree = 3, raw = TRUE)0.1.2  7.245e-11  2.038e-12   35.557
  polym(m, n, k, degree = 3, raw = TRUE)0.0.3  1.191e-09  8.004e-11   14.884
                                              Pr(>|t|)    
  (Intercept)                                   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.0.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)2.0.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)3.0.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.1.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.1.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)2.1.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.2.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.2.0   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.3.0   0.0462 *  
  polym(m, n, k, degree = 3, raw = TRUE)0.0.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.0.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)2.0.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.1.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.1.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.2.1   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.0.2   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)1.0.2   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.1.2   <2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.0.3   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 7.057e-05 on 259096 degrees of freedom
  Multiple R-squared:      1,	Adjusted R-squared:      1 
  F-statistic: 3.515e+08 on 19 and 259096 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = time ~ I(m * n * k), data = vanilla_dgemm)

  Residuals:
         Min         1Q     Median         3Q        Max 
  -1.694e-03 -1.893e-05 -1.718e-05 -9.670e-06  1.996e-03 

  Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  2.093e-05  2.039e-07   102.6   <2e-16 ***
  I(m * n * k) 2.580e-10  4.522e-15 57055.6   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.0001011 on 259114 degrees of freedom
  Multiple R-squared:  0.9999,	Adjusted R-squared:  0.9999 
  F-statistic: 3.255e+09 on 1 and 259114 DF,  p-value: < 2.2e-16
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  reg = lm(time~polym(m, n, k, degree=3, raw=TRUE), data=optimized_dgemm)
  summary(reg)
  reg = lm(time~I(m*n*k), data=optimized_dgemm)
  summary(reg)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = time ~ polym(m, n, k, degree = 3, raw = TRUE), data = optimized_dgemm)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.135685 -0.000032  0.000004  0.000036  0.134001 

  Coefficients:
                                                Estimate Std. Error t value
  (Intercept)                                  3.960e-05  9.504e-05   0.417
  polym(m, n, k, degree = 3, raw = TRUE)1.0.0 -1.406e-05  9.944e-06  -1.414
  polym(m, n, k, degree = 3, raw = TRUE)2.0.0  1.444e-07  1.510e-07   0.956
  polym(m, n, k, degree = 3, raw = TRUE)3.0.0  3.257e-10  1.016e-11  32.047
  polym(m, n, k, degree = 3, raw = TRUE)0.1.0 -2.256e-07  4.265e-07  -0.529
  polym(m, n, k, degree = 3, raw = TRUE)1.1.0  4.525e-08  1.604e-08   2.821
  polym(m, n, k, degree = 3, raw = TRUE)2.1.0 -3.381e-10  1.886e-11 -17.933
  polym(m, n, k, degree = 3, raw = TRUE)0.2.0  1.254e-10  5.814e-10   0.216
  polym(m, n, k, degree = 3, raw = TRUE)1.2.0 -1.766e-11  9.279e-12  -1.903
  polym(m, n, k, degree = 3, raw = TRUE)0.3.0 -1.253e-14  2.278e-13  -0.055
  polym(m, n, k, degree = 3, raw = TRUE)0.0.1  1.562e-05  1.709e-05   0.914
  polym(m, n, k, degree = 3, raw = TRUE)1.0.1  7.611e-08  1.368e-06   0.056
  polym(m, n, k, degree = 3, raw = TRUE)2.0.1 -3.808e-09  1.264e-09  -3.012
  polym(m, n, k, degree = 3, raw = TRUE)0.1.1  2.756e-08  2.999e-08   0.919
  polym(m, n, k, degree = 3, raw = TRUE)1.1.1  3.134e-09  1.588e-10  19.734
  polym(m, n, k, degree = 3, raw = TRUE)0.2.1 -4.899e-12  1.481e-11  -0.331
  polym(m, n, k, degree = 3, raw = TRUE)0.0.2 -3.488e-07  1.441e-06  -0.242
  polym(m, n, k, degree = 3, raw = TRUE)1.0.2  3.215e-09  1.135e-08   0.283
  polym(m, n, k, degree = 3, raw = TRUE)0.1.2 -2.688e-09  2.967e-10  -9.060
  polym(m, n, k, degree = 3, raw = TRUE)0.0.3  2.472e-09  1.165e-08   0.212
                                              Pr(>|t|)    
  (Intercept)                                  0.67694    
  polym(m, n, k, degree = 3, raw = TRUE)1.0.0  0.15742    
  polym(m, n, k, degree = 3, raw = TRUE)2.0.0  0.33886    
  polym(m, n, k, degree = 3, raw = TRUE)3.0.0  < 2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.1.0  0.59691    
  polym(m, n, k, degree = 3, raw = TRUE)1.1.0  0.00479 ** 
  polym(m, n, k, degree = 3, raw = TRUE)2.1.0  < 2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.2.0  0.82918    
  polym(m, n, k, degree = 3, raw = TRUE)1.2.0  0.05706 .  
  polym(m, n, k, degree = 3, raw = TRUE)0.3.0  0.95616    
  polym(m, n, k, degree = 3, raw = TRUE)0.0.1  0.36083    
  polym(m, n, k, degree = 3, raw = TRUE)1.0.1  0.95563    
  polym(m, n, k, degree = 3, raw = TRUE)2.0.1  0.00260 ** 
  polym(m, n, k, degree = 3, raw = TRUE)0.1.1  0.35807    
  polym(m, n, k, degree = 3, raw = TRUE)1.1.1  < 2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.2.1  0.74076    
  polym(m, n, k, degree = 3, raw = TRUE)0.0.2  0.80866    
  polym(m, n, k, degree = 3, raw = TRUE)1.0.2  0.77707    
  polym(m, n, k, degree = 3, raw = TRUE)0.1.2  < 2e-16 ***
  polym(m, n, k, degree = 3, raw = TRUE)0.0.3  0.83197    
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.01027 on 259096 degrees of freedom
  Multiple R-squared:  0.871,	Adjusted R-squared:  0.8709 
  F-statistic: 9.204e+04 on 19 and 259096 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = time ~ I(m * n * k), data = optimized_dgemm)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.108990 -0.000555 -0.000554 -0.000503  0.143079 

  Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
  (Intercept)  5.557e-04  2.261e-05   24.57   <2e-16 ***
  I(m * n * k) 5.992e-10  5.015e-13 1194.67   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.01121 on 259114 degrees of freedom
  Multiple R-squared:  0.8463,	Adjusted R-squared:  0.8463 
  F-statistic: 1.427e+06 on 1 and 259114 DF,  p-value: < 2.2e-16
  #+end_example

- New regression for =dtrsm=:
  #+begin_src R :results output :session *R* :exports both
  reg = lm(time~polym(m, n, degree=3, raw=TRUE), data=vanilla_dtrsm)
  summary(reg)
  reg = lm(time~I(m*n**2), data=vanilla_dtrsm)
  summary(reg)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = time ~ polym(m, n, degree = 3, raw = TRUE), data = vanilla_dtrsm)

  Residuals:
         Min         1Q     Median         3Q        Max 
  -3.535e-04 -5.700e-07 -3.000e-08  2.300e-07  1.301e-03 

  Coefficients:
                                           Estimate Std. Error t value Pr(>|t|)
  (Intercept)                             1.980e-06  6.114e-08  32.378  < 2e-16
  polym(m, n, degree = 3, raw = TRUE)1.0  3.773e-07  1.562e-08  24.159  < 2e-16
  polym(m, n, degree = 3, raw = TRUE)2.0 -8.561e-09  2.470e-09  -3.465  0.00053
  polym(m, n, degree = 3, raw = TRUE)3.0  5.763e-11  2.026e-11   2.844  0.00446
  polym(m, n, degree = 3, raw = TRUE)0.1  1.375e-08  1.325e-08   1.037  0.29963
  polym(m, n, degree = 3, raw = TRUE)1.1  9.785e-09  2.373e-09   4.124 3.73e-05
  polym(m, n, degree = 3, raw = TRUE)2.1  9.833e-11  1.979e-11   4.969 6.75e-07
  polym(m, n, degree = 3, raw = TRUE)0.2 -8.965e-11  2.631e-10  -0.341  0.73332
  polym(m, n, degree = 3, raw = TRUE)1.2  3.180e-12  2.193e-12   1.450  0.14704
  polym(m, n, degree = 3, raw = TRUE)0.3 -1.011e-13  1.488e-15 -67.911  < 2e-16

  (Intercept)                            ***
  polym(m, n, degree = 3, raw = TRUE)1.0 ***
  polym(m, n, degree = 3, raw = TRUE)2.0 ***
  polym(m, n, degree = 3, raw = TRUE)3.0 ** 
  polym(m, n, degree = 3, raw = TRUE)0.1    
  polym(m, n, degree = 3, raw = TRUE)1.1 ***
  polym(m, n, degree = 3, raw = TRUE)2.1 ***
  polym(m, n, degree = 3, raw = TRUE)0.2    
  polym(m, n, degree = 3, raw = TRUE)1.2    
  polym(m, n, degree = 3, raw = TRUE)0.3 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 1.802e-05 on 259106 degrees of freedom
  Multiple R-squared:  0.9995,	Adjusted R-squared:  0.9995 
  F-statistic: 5.586e+07 on 9 and 259106 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = time ~ I(m * n^2), data = vanilla_dtrsm)

  Residuals:
         Min         1Q     Median         3Q        Max 
  -1.457e-03 -3.895e-05 -3.796e-05 -3.795e-05  1.061e-03 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 4.095e-05  4.045e-07   101.2   <2e-16 ***
  I(m * n^2)  1.745e-11  8.948e-15  1949.8   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.0002006 on 259114 degrees of freedom
  Multiple R-squared:  0.9362,	Adjusted R-squared:  0.9362 
  F-statistic: 3.802e+06 on 1 and 259114 DF,  p-value: < 2.2e-16
  #+end_example

  #+begin_src R :results output :session *R* :exports both
  reg = lm(time~polym(m, n, degree=3, raw=TRUE), data=optimized_dtrsm)
  summary(reg)
  reg = lm(time~I(m*n**2), data=optimized_dtrsm)
  summary(reg)
  #+end_src

  #+RESULTS:
  #+begin_example

  Call:
  lm(formula = time ~ polym(m, n, degree = 3, raw = TRUE), data = optimized_dtrsm)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.038798 -0.000027 -0.000004  0.000025  0.052064 

  Coefficients:
                                           Estimate Std. Error t value Pr(>|t|)
  (Intercept)                            -3.505e-05  2.026e-05  -1.730 0.083563
  polym(m, n, degree = 3, raw = TRUE)1.0  1.880e-05  5.175e-06   3.634 0.000279
  polym(m, n, degree = 3, raw = TRUE)2.0 -2.232e-06  8.185e-07  -2.727 0.006384
  polym(m, n, degree = 3, raw = TRUE)3.0  2.438e-08  6.714e-09   3.631 0.000283
  polym(m, n, degree = 3, raw = TRUE)0.1  5.467e-06  4.391e-06   1.245 0.213109
  polym(m, n, degree = 3, raw = TRUE)1.1  7.007e-07  7.862e-07   0.891 0.372788
  polym(m, n, degree = 3, raw = TRUE)2.1 -1.791e-09  6.557e-09  -0.273 0.784684
  polym(m, n, degree = 3, raw = TRUE)0.2 -9.080e-08  8.718e-08  -1.042 0.297617
  polym(m, n, degree = 3, raw = TRUE)1.2  1.215e-10  7.266e-10   0.167 0.867194
  polym(m, n, degree = 3, raw = TRUE)0.3  2.939e-11  4.932e-13  59.602  < 2e-16

  (Intercept)                            .  
  polym(m, n, degree = 3, raw = TRUE)1.0 ***
  polym(m, n, degree = 3, raw = TRUE)2.0 ** 
  polym(m, n, degree = 3, raw = TRUE)3.0 ***
  polym(m, n, degree = 3, raw = TRUE)0.1    
  polym(m, n, degree = 3, raw = TRUE)1.1    
  polym(m, n, degree = 3, raw = TRUE)2.1    
  polym(m, n, degree = 3, raw = TRUE)0.2    
  polym(m, n, degree = 3, raw = TRUE)1.2    
  polym(m, n, degree = 3, raw = TRUE)0.3 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.005971 on 259106 degrees of freedom
  Multiple R-squared:  0.6685,	Adjusted R-squared:  0.6685 
  F-statistic: 5.806e+04 on 9 and 259106 DF,  p-value: < 2.2e-16

  Call:
  lm(formula = time ~ I(m * n^2), data = optimized_dtrsm)

  Residuals:
        Min        1Q    Median        3Q       Max 
  -0.050369 -0.000984 -0.000983 -0.000983  0.073534 

  Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
  (Intercept) 9.849e-04  1.536e-05   64.12   <2e-16 ***
  I(m * n^2)  1.598e-10  3.398e-13  470.42   <2e-16 ***
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

  Residual standard error: 0.007617 on 259114 degrees of freedom
  Multiple R-squared:  0.4606,	Adjusted R-squared:  0.4606 
  F-statistic: 2.213e+05 on 1 and 259114 DF,  p-value: < 2.2e-16
  #+end_example

- There are a lot of strange things.
  + For both functions and both versions of HPL, the regression with a polynomial of degree 3 does not find what “it
    should” find (i.e. a single term =m*n*k= for =dgemm= and a single term =m*n*n= for =dtrsm=).
  + The regressions are different for the two versions of HPL, even when we restrict to this “single term we should find”.
  + For both functions, the R-squared is much better for vanilla than for optimized. It should be the contrary, since
    the noise for optimized should be much smaller.

  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  optimized_dtrsm$optimized = TRUE
  optimized_dgemm$optimized = TRUE
  vanilla_dtrsm$optimized = FALSE
  vanilla_dgemm$optimized = FALSE
  data_dtrsm = rbind(optimized_dtrsm, vanilla_dtrsm)
  data_dgemm = rbind(optimized_dgemm, vanilla_dgemm)
  data_dtrsm$size_product = data_dtrsm$m * data_dtrsm$n**2
  data_dgemm$size_product = data_dgemm$m * data_dgemm$n* data_dgemm$k
  #+end_src

  #+RESULTS:

  #+begin_src R :file trace_visualization/taurus/1.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data_dtrsm, aes(x=size_product, y=time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dtrsm for different sizes")
  plot2 = ggplot(data_dgemm, aes(x=size_product, y=time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dgemm for different sizes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/1.png]]

- Ouch. Terrible plots, there is obviously something very wrong.
- Let’s try the same thing with 12 processes (which works well).
- Time and Gflops:
  #+begin_src sh :results output
  tail /home/tom/tmp/16/taurus_12/output_vanilla   -n 16 | head -n 4
  tail /home/tom/tmp/16/taurus_12/output_optimized -n  8 | head -n 4
  #+end_src

  #+RESULTS:
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12     1              73.79              7.229e+01
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12     1              69.82              7.639e+01

#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=(-?[0-9]+.[0-9]+)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:
        csv_writer = csv.writer(out_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'time'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 12))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
process('/home/tom/tmp/16/taurus_12/output_optimized', '/home/tom/tmp/16/taurus_12/calls_optimized.csv')
process('/home/tom/tmp/16/taurus_12/output_vanilla',   '/home/tom/tmp/16/taurus_12/calls_vanilla.csv')
#+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  calls_vanilla <- read.csv("/home/tom/tmp/16/taurus_12/calls_vanilla.csv");
  vanilla_dgemm = calls_vanilla[calls_vanilla$func == "dgemm",]
  vanilla_dtrsm = calls_vanilla[calls_vanilla$func == "dtrsm",]
  calls_optimized <- read.csv("/home/tom/tmp/16/taurus_12/calls_optimized.csv");
  optimized_dgemm = calls_optimized[calls_optimized$func == "dgemm",]
  optimized_dtrsm = calls_optimized[calls_optimized$func == "dtrsm",]
  #+end_src
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  optimized_dtrsm$optimized = TRUE
  optimized_dgemm$optimized = TRUE
  vanilla_dtrsm$optimized = FALSE
  vanilla_dgemm$optimized = FALSE
  data_dtrsm = rbind(optimized_dtrsm, vanilla_dtrsm)
  data_dgemm = rbind(optimized_dgemm, vanilla_dgemm)
  data_dtrsm$size_product = data_dtrsm$m * data_dtrsm$n**2
  data_dgemm$size_product = data_dgemm$m * data_dgemm$n* data_dgemm$k
  #+end_src

  #+begin_src R :file trace_visualization/taurus/2.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data_dtrsm, aes(x=size_product, y=time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dtrsm for different sizes")
  plot2 = ggplot(data_dgemm, aes(x=size_product, y=time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dgemm for different sizes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/2.png]]

- The plots look ok here. The estimation of =dtrsm= is wrong, but at least it is not an horrible mess.

**** Automatic modeling                                           :ARNAUD:
Using code from "[[http://www-bcf.usc.edu/~gareth/ISL/ISLR%2520First%2520Printing.pdf][Introduction to Statistical Learning: Applications
with R]]".
***** Loading data
#+begin_src R :results output :session *R* :exports both
data = read.csv('dtrsm_times.csv')
options(width=200)
dim(data)
head(data)
#+end_src

#+RESULTS:
: [1] 30  8
:       time size_product lead_product     ratio    m    n lead_A lead_B
: 1 0.939842      3380016     10342656  3.059943 1051 3216   3216   3216
: 2 0.078053      1131603      3059001  2.703246 1749  647   1749   1749
: 3 0.044048       967694      5597956  5.784841 2366  409   2366   2366
: 4 3.982264     14205729     19918369  1.402136 4463 3183   4463   4463
: 5 1.267480      5933670      6200100  1.044901 2383 2490   2490   2490
: 6 0.443926      1060864     22429696 21.142857  224 4736   4736   4736


#+begin_src R :results output :session *R* :exports both
reg = lm(time ~ I(m*n**2), data) # Model carefully selected by Tom
summary(reg)
reg = lm(time ~ I(n*m**2), data) # Alternate model ?... Clearly not as good
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = time ~ I(m * n^2), data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.095222 -0.020818 -0.004185  0.007353  0.289151 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.752e-02  1.454e-02   1.205    0.238    
I(m * n^2)  8.690e-11  4.305e-13 201.835   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.06245 on 28 degrees of freedom
Multiple R-squared:  0.9993,	Adjusted R-squared:  0.9993 
F-statistic: 4.074e+04 on 1 and 28 DF,  p-value: < 2.2e-16

Call:
lm(formula = time ~ I(n * m^2), data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.00096 -0.36423 -0.09053  0.42366  1.39948 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 2.315e-01  1.656e-01   1.398    0.173    
I(n * m^2)  7.817e-11  4.764e-12  16.407 6.79e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7314 on 28 degrees of freedom
Multiple R-squared:  0.9058,	Adjusted R-squared:  0.9024 
F-statistic: 269.2 on 1 and 28 DF,  p-value: 6.794e-16
#+end_example
***** Best subset selection
Using code from "Introduction to Statistical Learning: Applications
with R", chapter 6.5.1 (Best Subset Selection)

The regsubsets function performs best subset selection by identifying
the best model that contains a given number of predictors, where best
is quantified using RSS.

#+begin_src R :results output :session *R* :exports both
library(leaps)
mreg = regsubsets(time ~ poly(m, n, degree=3, raw=TRUE), data ) # the raw=TRUE seems important here but I do not really understand why
mreg.summary = summary(mreg)
mreg.summary
#+end_src

#+RESULTS:
#+begin_example
Subset selection object
Call: regsubsets.formula(time ~ poly(m, n, degree = 3, raw = TRUE), 
    data)
9 Variables  (and intercept)
                                      Forced in Forced out
poly(m, n, degree = 3, raw = TRUE)1.0     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)2.0     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)3.0     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)0.1     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)1.1     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)2.1     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)0.2     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)1.2     FALSE      FALSE
poly(m, n, degree = 3, raw = TRUE)0.3     FALSE      FALSE
1 subsets of each size up to 8
Selection Algorithm: exhaustive
         poly(m, n, degree = 3, raw = TRUE)1.0 poly(m, n, degree = 3, raw = TRUE)2.0 poly(m, n, degree = 3, raw = TRUE)3.0 poly(m, n, degree = 3, raw = TRUE)0.1 poly(m, n, degree = 3, raw = TRUE)1.1
1  ( 1 ) " "                                   " "                                   " "                                   " "                                   " "                                  
2  ( 1 ) " "                                   " "                                   " "                                   " "                                   " "                                  
3  ( 1 ) " "                                   "*"                                   " "                                   " "                                   " "                                  
4  ( 1 ) "*"                                   "*"                                   "*"                                   " "                                   " "                                  
5  ( 1 ) "*"                                   "*"                                   "*"                                   " "                                   " "                                  
6  ( 1 ) "*"                                   "*"                                   "*"                                   "*"                                   " "                                  
7  ( 1 ) "*"                                   "*"                                   "*"                                   " "                                   " "                                  
8  ( 1 ) "*"                                   "*"                                   "*"                                   " "                                   "*"                                  
         poly(m, n, degree = 3, raw = TRUE)2.1 poly(m, n, degree = 3, raw = TRUE)0.2 poly(m, n, degree = 3, raw = TRUE)1.2 poly(m, n, degree = 3, raw = TRUE)0.3
1  ( 1 ) " "                                   " "                                   "*"                                   " "                                  
2  ( 1 ) "*"                                   " "                                   "*"                                   " "                                  
3  ( 1 ) "*"                                   " "                                   "*"                                   " "                                  
4  ( 1 ) " "                                   " "                                   "*"                                   " "                                  
5  ( 1 ) "*"                                   " "                                   "*"                                   " "                                  
6  ( 1 ) "*"                                   " "                                   "*"                                   " "                                  
7  ( 1 ) "*"                                   "*"                                   "*"                                   "*"                                  
8  ( 1 ) "*"                                   "*"                                   "*"                                   "*"
#+end_example

#+begin_src R :results output :session *R* :exports both
mreg.summary$rsq
mreg.summary$bic
#+end_src

#+RESULTS:
: [1] 0.9993131 0.9995344 0.9995606 0.9995851 0.9996288 0.9996460 0.9996486 0.9996699
: [1] -211.6992 -219.9649 -218.3013 -216.6163 -216.5535 -214.5764 -211.4020 -209.8751

Let's look at other criterias like the adjusted R^2 or the BIC
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
par(mfrow=c(2,2))
plot(mreg.summary$rss, type="l");
plot(mreg.summary$adjr2, type="l");
plot(mreg.summary$bic, type="l");
par(mfrow=c(1,1))
#+end_src

#+RESULTS:
[[file:/tmp/babel-26211LnU/figure26211SNm.png]]


#+begin_src R :results output :session *R* :exports both
which.max(mreg.summary$adjr2)
which.min(mreg.summary$bic)
#+end_src

#+RESULTS:
: [1] 6
: [1] 2

It seems there exists specific plotting functions but I do not really
understand how to read their output at the moment...
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
plot(mreg)
#+end_src

#+RESULTS:
[[file:/tmp/babel-26211LnU/figure262114GC.png]]

***** Trying to extract a model I like
Let's focus on the one minimizing the BIC. Why ? I don't know. It
looks like it is the only "non-trivial" one here. :)

#+begin_src R :results output :session *R* :exports both
v = mreg.summary$outmat[which.min(mreg.summary$bic),]=="*"
v = names(v)[v]
v
#+end_src

#+RESULTS:
: [1] "poly(m, n, degree = 3, raw = TRUE)2.1" "poly(m, n, degree = 3, raw = TRUE)1.2"

From this, I should be able to rebuild the following regression.
#+begin_src R :results output :session *R* :exports both
reg = lm(as.formula("time ~ I(m^2*n^1) + I(m^1*n^2)"), data )
summary(reg)
#+end_src

#+RESULTS:
#+begin_example

Call:
lm(formula = as.formula("time ~ I(m^2*n^1) + I(m^1*n^2)"), data = data)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.100817 -0.016086 -0.004193  0.013402  0.205040 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.925e-02  1.220e-02   1.578  0.12622    
I(m^2 * n^1) 3.814e-12  1.065e-12   3.583  0.00132 ** 
I(m^1 * n^2) 8.307e-11  1.127e-12  73.736  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.05236 on 27 degrees of freedom
Multiple R-squared:  0.9995,	Adjusted R-squared:  0.9995 
F-statistic: 2.898e+04 on 2 and 27 DF,  p-value: < 2.2e-16
#+end_example

***** Making predictions
The coefficients from the regression can easily be extracted as
follows but I don't know how to extract directly the model itself to
recompute all the previous information (p-value, confidence intervals,
etc.).
#+begin_src R :results output :session *R* :exports both
v = coef(mreg, id=which.min(mreg.summary$bic))
v
#+end_src

#+RESULTS:
:                           (Intercept) poly(m, n, degree = 3, raw = TRUE)2.1 poly(m, n, degree = 3, raw = TRUE)1.2 
:                          1.924518e-02                          3.813688e-12                          8.307229e-11

Here is a function (still from the book) to use the model to make
predictions that we can apply for example to the original data
#+begin_src R :results output :session *R* :exports both
predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form,newdata)
    coefi = coef(object, id=id)
    xvars = names (coefi)
    mat[,xvars] %*% coefi
}
data$predict = predict.regsubsets(mreg, data,id=2)
data
#+end_src

#+RESULTS:
#+begin_example
       time size_product lead_product     ratio    m    n lead_A lead_B    predict
1  0.939842      3380016     10342656  3.059943 1051 3216   3216   3216 0.93579965
2  0.078053      1131603      3059001  2.703246 1749  647   1749   1749 0.08761427
3  0.044048       967694      5597956  5.784841 2366  409   2366   2366 0.06085578
4  3.982264     14205729     19918369  1.402136 4463 3183   4463   4463 4.01729982
5  1.267480      5933670      6200100  1.044901 2383 2490   2490   2490 1.30055017
6  0.443926      1060864     22429696 21.142857  224 4736   4736   4736 0.43752756
7  4.010293     12776269     13256881  1.037618 3509 3641   3641   3641 4.05460992
8  0.265316      2522156      4826809  1.913763 2197 1148   2197   2197 0.28090793
9  1.675509      5460642     12159169  2.226692 1566 3487   3487   3487 1.63365842
10 1.837690      9837027     22005481  2.237005 4691 2097   4691   4691 1.90886535
11 4.657941     13733885     15249025  1.110321 3517 3905   3905   3905 4.65868946
12 0.212877      1362375      2424249  1.779429  875 1557   1557   1557 0.20000582
13 0.673950      1582054     23990404 15.164087  323 4898   4898   4898 0.66491291
14 1.878663      5013955     19175641  3.824454 1145 4379   4379   4379 1.86508378
15 0.250965      2339104      4596736  1.965170 2144 1091   2144   2144 0.25036834
16 0.096826       881475      1199025  1.360248  805 1095   1095   1095 0.10213395
17 4.319228     14636045     21520321  1.470365 4639 3155   4639   4639 4.11418783
18 0.285244      2016175      2059225  1.021352 1405 1435   1435   1435 0.27039399
19 1.522144      5998176      8020224  1.337110 2118 2832   2832   2832 1.47883007
20 9.901155     22909898     24611521  1.074275 4618 4961   4961   4961 9.86439021
21 1.534956      4865575     12780625  2.626745 1361 3575   3575   3575 1.48949483
22 0.244505      2749880      8934121  3.248913 2989  920   2989   2989 0.26075510
23 0.113355      1513422      4104676  2.712182 2026  747   2026   2026 0.12485408
24 0.387667      3114894      5230369  1.679148 2287 1362   2287   2287 0.39884599
25 0.022760        84188      2621161 31.134615   52 1619   1619   1619 0.03058465
26 0.228347      1947946      2337841  1.200157 1529 1274   1529   1529 0.23676301
27 1.319131      5352408      8088336  1.511158 1882 2844   2844   2844 1.32220841
28 1.205215      3100720     20430400  6.588921  686 4520   4520   4520 1.19163656
29 5.741450     16201107     16916769  1.044174 3939 4113   4113   4113 5.79815503
30 5.973683     17022651     17313921  1.017111 4161 4091   4161   4161 6.07450011
#+end_example

It seems to work perfectly:
#+begin_src R :results output graphics :file (org-babel-temp-file "figure" ".png") :exports both :width 600 :height 400 :session *R* 
plot(data$predict,data$time)
#+end_src

#+RESULTS:
[[file:/tmp/babel-26211LnU/figure26211sva.png]]

Unfortunately using it to make predictions on new data seems
complicated. :( 

#+begin_src R :results output :session *R* :exports both
newdata = head(data,n=1)
newdata$m = 1000
newdata$n = 250
predict.regsubsets(mreg, newdata,id=2)
#+end_src

#+RESULTS:
: Error in mat[, xvars] (from #6) : indice hors limites

Grumpf. :( Let's try differently:
#+begin_src R :results output :session *R* :exports both
newdata=data
newdata$m = 1000
newdata$n = 250
head(predict.regsubsets(mreg, newdata,id=2),n=1)
#+end_src

#+RESULTS:
:         [,1]
: 1 0.02539062

This seem to break when building the =model.frame=. There seems to be
something wrong with the polynom from the formula but I can't figure
out what.
#+begin_src R :results output :session *R* :exports both
newdata = head(data,n=1)
newdata$m = 1000
newdata$n = 250
my_general_model = as.formula(mreg$call[[2]])
model.frame(my_general_model, newdata, raw=T)
#+end_src

#+RESULTS:
#+begin_example
      time poly(m, n, degree = 3, raw = TRUE).1 poly(m, n, degree = 3, raw = TRUE).2 poly(m, n, degree = 3, raw = TRUE).3 poly(m, n, degree = 3, raw = TRUE).4 poly(m, n, degree = 3, raw = TRUE).5
1 0.939842                                1e+03                                1e+06                                1e+09                                1e+12                                1e+15
  poly(m, n, degree = 3, raw = TRUE).6 poly(m, n, degree = 3, raw = TRUE).7 poly(m, n, degree = 3, raw = TRUE).8 poly(m, n, degree = 3, raw = TRUE).9 poly(m, n, degree = 3, raw = TRUE).10
1                                1e+18                                1e+21                                1e+24                                1e+27                                 1e+30
  poly(m, n, degree = 3, raw = TRUE).11 poly(m, n, degree = 3, raw = TRUE).12 poly(m, n, degree = 3, raw = TRUE).13 poly(m, n, degree = 3, raw = TRUE).14 poly(m, n, degree = 3, raw = TRUE).15
1                                 1e+33                                 1e+36                                 1e+39                                 1e+42                                 1e+45
  poly(m, n, degree = 3, raw = TRUE).16 poly(m, n, degree = 3, raw = TRUE).17 poly(m, n, degree = 3, raw = TRUE).18 poly(m, n, degree = 3, raw = TRUE).19 poly(m, n, degree = 3, raw = TRUE).20
1                                 1e+48                                 1e+51                                 1e+54                                 1e+57                                 1e+60
  poly(m, n, degree = 3, raw = TRUE).21 poly(m, n, degree = 3, raw = TRUE).22 poly(m, n, degree = 3, raw = TRUE).23 poly(m, n, degree = 3, raw = TRUE).24 poly(m, n, degree = 3, raw = TRUE).25
1                                 1e+63                                 1e+66                                 1e+69                                 1e+72                                 1e+75
  poly(m, n, degree = 3, raw = TRUE).26 poly(m, n, degree = 3, raw = TRUE).27 poly(m, n, degree = 3, raw = TRUE).28 poly(m, n, degree = 3, raw = TRUE).29 poly(m, n, degree = 3, raw = TRUE).30
1                                 1e+78                                 1e+81                                 1e+84                                 1e+87                                 1e+90
  poly(m, n, degree = 3, raw = TRUE).31 poly(m, n, degree = 3, raw = TRUE).32 poly(m, n, degree = 3, raw = TRUE).33 poly(m, n, degree = 3, raw = TRUE).34 poly(m, n, degree = 3, raw = TRUE).35
1                                 1e+93                                 1e+96                                 1e+99                                1e+102                                1e+105
  poly(m, n, degree = 3, raw = TRUE).36 poly(m, n, degree = 3, raw = TRUE).37 poly(m, n, degree = 3, raw = TRUE).38 poly(m, n, degree = 3, raw = TRUE).39 poly(m, n, degree = 3, raw = TRUE).40
1                                1e+108                                1e+111                                1e+114                                1e+117                                1e+120
  poly(m, n, degree = 3, raw = TRUE).41 poly(m, n, degree = 3, raw = TRUE).42 poly(m, n, degree = 3, raw = TRUE).43 poly(m, n, degree = 3, raw = TRUE).44 poly(m, n, degree = 3, raw = TRUE).45
1                                1e+123                                1e+126                                1e+129                                1e+132                                1e+135
  poly(m, n, degree = 3, raw = TRUE).46 poly(m, n, degree = 3, raw = TRUE).47 poly(m, n, degree = 3, raw = TRUE).48 poly(m, n, degree = 3, raw = TRUE).49 poly(m, n, degree = 3, raw = TRUE).50
1                                1e+138                                1e+141                                1e+144                                1e+147                                1e+150
  poly(m, n, degree = 3, raw = TRUE).51 poly(m, n, degree = 3, raw = TRUE).52 poly(m, n, degree = 3, raw = TRUE).53 poly(m, n, degree = 3, raw = TRUE).54 poly(m, n, degree = 3, raw = TRUE).55
1                                1e+153                                1e+156                                1e+159                                1e+162                                1e+165
  poly(m, n, degree = 3, raw = TRUE).56 poly(m, n, degree = 3, raw = TRUE).57 poly(m, n, degree = 3, raw = TRUE).58 poly(m, n, degree = 3, raw = TRUE).59 poly(m, n, degree = 3, raw = TRUE).60
1                                1e+168                                1e+171                                1e+174                                1e+177                                1e+180
  poly(m, n, degree = 3, raw = TRUE).61 poly(m, n, degree = 3, raw = TRUE).62 poly(m, n, degree = 3, raw = TRUE).63 poly(m, n, degree = 3, raw = TRUE).64 poly(m, n, degree = 3, raw = TRUE).65
1                                1e+183                                1e+186                                1e+189                                1e+192                                1e+195
  poly(m, n, degree = 3, raw = TRUE).66 poly(m, n, degree = 3, raw = TRUE).67 poly(m, n, degree = 3, raw = TRUE).68 poly(m, n, degree = 3, raw = TRUE).69 poly(m, n, degree = 3, raw = TRUE).70
1                                1e+198                                1e+201                                1e+204                                1e+207                                1e+210
  poly(m, n, degree = 3, raw = TRUE).71 poly(m, n, degree = 3, raw = TRUE).72 poly(m, n, degree = 3, raw = TRUE).73 poly(m, n, degree = 3, raw = TRUE).74 poly(m, n, degree = 3, raw = TRUE).75
1                                1e+213                                1e+216                                1e+219                                1e+222                                1e+225
  poly(m, n, degree = 3, raw = TRUE).76 poly(m, n, degree = 3, raw = TRUE).77 poly(m, n, degree = 3, raw = TRUE).78 poly(m, n, degree = 3, raw = TRUE).79 poly(m, n, degree = 3, raw = TRUE).80
1                                1e+228                                1e+231                                1e+234                                1e+237                                1e+240
  poly(m, n, degree = 3, raw = TRUE).81 poly(m, n, degree = 3, raw = TRUE).82 poly(m, n, degree = 3, raw = TRUE).83 poly(m, n, degree = 3, raw = TRUE).84 poly(m, n, degree = 3, raw = TRUE).85
1                                1e+243                                1e+246                                1e+249                                1e+252                                1e+255
  poly(m, n, degree = 3, raw = TRUE).86 poly(m, n, degree = 3, raw = TRUE).87 poly(m, n, degree = 3, raw = TRUE).88 poly(m, n, degree = 3, raw = TRUE).89 poly(m, n, degree = 3, raw = TRUE).90
1                                1e+258                                1e+261                                1e+264                                1e+267                                1e+270
  poly(m, n, degree = 3, raw = TRUE).91 poly(m, n, degree = 3, raw = TRUE).92 poly(m, n, degree = 3, raw = TRUE).93 poly(m, n, degree = 3, raw = TRUE).94 poly(m, n, degree = 3, raw = TRUE).95
1                                1e+273                                1e+276                                1e+279                                1e+282                                1e+285
  poly(m, n, degree = 3, raw = TRUE).96 poly(m, n, degree = 3, raw = TRUE).97 poly(m, n, degree = 3, raw = TRUE).98 poly(m, n, degree = 3, raw = TRUE).99 poly(m, n, degree = 3, raw = TRUE).100
1                                1e+288                                1e+291                                1e+294                                1e+297                                 1e+300
  poly(m, n, degree = 3, raw = TRUE).101 poly(m, n, degree = 3, raw = TRUE).102 poly(m, n, degree = 3, raw = TRUE).103 poly(m, n, degree = 3, raw = TRUE).104 poly(m, n, degree = 3, raw = TRUE).105
1                                 1e+303                                 1e+306                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).106 poly(m, n, degree = 3, raw = TRUE).107 poly(m, n, degree = 3, raw = TRUE).108 poly(m, n, degree = 3, raw = TRUE).109 poly(m, n, degree = 3, raw = TRUE).110
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).111 poly(m, n, degree = 3, raw = TRUE).112 poly(m, n, degree = 3, raw = TRUE).113 poly(m, n, degree = 3, raw = TRUE).114 poly(m, n, degree = 3, raw = TRUE).115
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).116 poly(m, n, degree = 3, raw = TRUE).117 poly(m, n, degree = 3, raw = TRUE).118 poly(m, n, degree = 3, raw = TRUE).119 poly(m, n, degree = 3, raw = TRUE).120
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).121 poly(m, n, degree = 3, raw = TRUE).122 poly(m, n, degree = 3, raw = TRUE).123 poly(m, n, degree = 3, raw = TRUE).124 poly(m, n, degree = 3, raw = TRUE).125
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).126 poly(m, n, degree = 3, raw = TRUE).127 poly(m, n, degree = 3, raw = TRUE).128 poly(m, n, degree = 3, raw = TRUE).129 poly(m, n, degree = 3, raw = TRUE).130
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).131 poly(m, n, degree = 3, raw = TRUE).132 poly(m, n, degree = 3, raw = TRUE).133 poly(m, n, degree = 3, raw = TRUE).134 poly(m, n, degree = 3, raw = TRUE).135
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).136 poly(m, n, degree = 3, raw = TRUE).137 poly(m, n, degree = 3, raw = TRUE).138 poly(m, n, degree = 3, raw = TRUE).139 poly(m, n, degree = 3, raw = TRUE).140
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).141 poly(m, n, degree = 3, raw = TRUE).142 poly(m, n, degree = 3, raw = TRUE).143 poly(m, n, degree = 3, raw = TRUE).144 poly(m, n, degree = 3, raw = TRUE).145
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).146 poly(m, n, degree = 3, raw = TRUE).147 poly(m, n, degree = 3, raw = TRUE).148 poly(m, n, degree = 3, raw = TRUE).149 poly(m, n, degree = 3, raw = TRUE).150
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).151 poly(m, n, degree = 3, raw = TRUE).152 poly(m, n, degree = 3, raw = TRUE).153 poly(m, n, degree = 3, raw = TRUE).154 poly(m, n, degree = 3, raw = TRUE).155
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).156 poly(m, n, degree = 3, raw = TRUE).157 poly(m, n, degree = 3, raw = TRUE).158 poly(m, n, degree = 3, raw = TRUE).159 poly(m, n, degree = 3, raw = TRUE).160
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).161 poly(m, n, degree = 3, raw = TRUE).162 poly(m, n, degree = 3, raw = TRUE).163 poly(m, n, degree = 3, raw = TRUE).164 poly(m, n, degree = 3, raw = TRUE).165
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).166 poly(m, n, degree = 3, raw = TRUE).167 poly(m, n, degree = 3, raw = TRUE).168 poly(m, n, degree = 3, raw = TRUE).169 poly(m, n, degree = 3, raw = TRUE).170
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).171 poly(m, n, degree = 3, raw = TRUE).172 poly(m, n, degree = 3, raw = TRUE).173 poly(m, n, degree = 3, raw = TRUE).174 poly(m, n, degree = 3, raw = TRUE).175
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).176 poly(m, n, degree = 3, raw = TRUE).177 poly(m, n, degree = 3, raw = TRUE).178 poly(m, n, degree = 3, raw = TRUE).179 poly(m, n, degree = 3, raw = TRUE).180
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).181 poly(m, n, degree = 3, raw = TRUE).182 poly(m, n, degree = 3, raw = TRUE).183 poly(m, n, degree = 3, raw = TRUE).184 poly(m, n, degree = 3, raw = TRUE).185
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).186 poly(m, n, degree = 3, raw = TRUE).187 poly(m, n, degree = 3, raw = TRUE).188 poly(m, n, degree = 3, raw = TRUE).189 poly(m, n, degree = 3, raw = TRUE).190
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).191 poly(m, n, degree = 3, raw = TRUE).192 poly(m, n, degree = 3, raw = TRUE).193 poly(m, n, degree = 3, raw = TRUE).194 poly(m, n, degree = 3, raw = TRUE).195
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).196 poly(m, n, degree = 3, raw = TRUE).197 poly(m, n, degree = 3, raw = TRUE).198 poly(m, n, degree = 3, raw = TRUE).199 poly(m, n, degree = 3, raw = TRUE).200
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).201 poly(m, n, degree = 3, raw = TRUE).202 poly(m, n, degree = 3, raw = TRUE).203 poly(m, n, degree = 3, raw = TRUE).204 poly(m, n, degree = 3, raw = TRUE).205
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).206 poly(m, n, degree = 3, raw = TRUE).207 poly(m, n, degree = 3, raw = TRUE).208 poly(m, n, degree = 3, raw = TRUE).209 poly(m, n, degree = 3, raw = TRUE).210
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).211 poly(m, n, degree = 3, raw = TRUE).212 poly(m, n, degree = 3, raw = TRUE).213 poly(m, n, degree = 3, raw = TRUE).214 poly(m, n, degree = 3, raw = TRUE).215
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).216 poly(m, n, degree = 3, raw = TRUE).217 poly(m, n, degree = 3, raw = TRUE).218 poly(m, n, degree = 3, raw = TRUE).219 poly(m, n, degree = 3, raw = TRUE).220
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).221 poly(m, n, degree = 3, raw = TRUE).222 poly(m, n, degree = 3, raw = TRUE).223 poly(m, n, degree = 3, raw = TRUE).224 poly(m, n, degree = 3, raw = TRUE).225
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).226 poly(m, n, degree = 3, raw = TRUE).227 poly(m, n, degree = 3, raw = TRUE).228 poly(m, n, degree = 3, raw = TRUE).229 poly(m, n, degree = 3, raw = TRUE).230
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).231 poly(m, n, degree = 3, raw = TRUE).232 poly(m, n, degree = 3, raw = TRUE).233 poly(m, n, degree = 3, raw = TRUE).234 poly(m, n, degree = 3, raw = TRUE).235
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).236 poly(m, n, degree = 3, raw = TRUE).237 poly(m, n, degree = 3, raw = TRUE).238 poly(m, n, degree = 3, raw = TRUE).239 poly(m, n, degree = 3, raw = TRUE).240
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).241 poly(m, n, degree = 3, raw = TRUE).242 poly(m, n, degree = 3, raw = TRUE).243 poly(m, n, degree = 3, raw = TRUE).244 poly(m, n, degree = 3, raw = TRUE).245
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  poly(m, n, degree = 3, raw = TRUE).246 poly(m, n, degree = 3, raw = TRUE).247 poly(m, n, degree = 3, raw = TRUE).248 poly(m, n, degree = 3, raw = TRUE).249 poly(m, n, degree = 3, raw = TRUE).250
1                                    Inf                                    Inf                                    Inf                                    Inf                                    Inf
  (raw)
1  TRUE
#+end_example
***** Giving a try to Ridge regression and Lasso
#+begin_src R :results output :session *R* :exports both
library(glmnet)
data = read.csv('dtrsm_times.csv')
#+end_src

#+RESULTS:
: Le chargement a nécessité le package : Matrix
: Le chargement a nécessité le package : foreach
: foreach: simple, scalable parallel programming from Revolution Analytics
: Use Revolution R for scalability, fault tolerance and more.
: http://www.revolutionanalytics.com
: Loaded glmnet 2.0-10

#+begin_src R :results output :session *R* :exports both
x = model.matrix(time ~ poly(m, n, degree=3, raw=TRUE), data )
y = data$time
ridge.mod = glmnet(x,y)
#+end_src

#+RESULTS:


OK, let's try to see what's inside... Yuck, this looks ugly.
#+begin_src R :results output :session *R* :exports both
ridge.mod
summary(ridge.mod)
#+end_src

#+RESULTS:
#+begin_example

Call:  glmnet(x = x, y = y) 

      Df   %Dev  Lambda
 [1,]  0 0.0000 2.30100
 [2,]  1 0.1697 2.09700
 [3,]  1 0.3105 1.91100
 [4,]  1 0.4275 1.74100
 [5,]  1 0.5246 1.58600
 [6,]  1 0.6052 1.44500
 [7,]  1 0.6721 1.31700
 [8,]  1 0.7276 1.20000
 [9,]  1 0.7738 1.09300
[10,]  1 0.8121 0.99620
[11,]  1 0.8439 0.90770
[12,]  1 0.8702 0.82710
[13,]  1 0.8922 0.75360
[14,]  1 0.9104 0.68660
[15,]  1 0.9255 0.62560
[16,]  1 0.9380 0.57010
[17,]  1 0.9484 0.51940
[18,]  1 0.9570 0.47330
[19,]  1 0.9642 0.43120
[20,]  1 0.9702 0.39290
[21,]  1 0.9751 0.35800
[22,]  1 0.9792 0.32620
[23,]  1 0.9826 0.29720
[24,]  1 0.9855 0.27080
[25,]  1 0.9878 0.24680
[26,]  2 0.9898 0.22480
[27,]  2 0.9914 0.20490
[28,]  3 0.9928 0.18670
[29,]  3 0.9939 0.17010
[30,]  3 0.9949 0.15500
[31,]  3 0.9957 0.14120
[32,]  3 0.9963 0.12870
[33,]  3 0.9969 0.11720
[34,]  3 0.9973 0.10680
[35,]  3 0.9977 0.09733
[36,]  3 0.9980 0.08868
[37,]  3 0.9983 0.08080
[38,]  3 0.9985 0.07363
[39,]  3 0.9987 0.06709
[40,]  3 0.9988 0.06113
[41,]  3 0.9989 0.05570
[42,]  3 0.9990 0.05075
          Length Class     Mode   
a0         42    -none-    numeric
beta      420    dgCMatrix S4     
df         42    -none-    numeric
dim         2    -none-    numeric
lambda     42    -none-    numeric
dev.ratio  42    -none-    numeric
nulldev     1    -none-    numeric
npasses     1    -none-    numeric
jerr        1    -none-    numeric
offset      1    -none-    logical
call        3    -none-    call   
nobs        1    -none-    numeric
#+end_example

What about coefficients ? OK, it looks a bit better. I guess each
columns corresponds to a given lambda (tradeoff between RSS and
parameter size).
#+begin_src R :results output :session *R* :exports both
coef(ridge.mod)
#+end_src

#+RESULTS:
#+begin_example
11 x 42 sparse Matrix of class "dgCMatrix"
   [[ suppressing 42 column names ‘s0’, ‘s1’, ‘s2’ ... ]]
                                                                                                                                                                                             
(Intercept)                           1.837149 1.675499e+00 1.528208e+00 1.394003e+00 1.271720e+00 1.160300e+00 1.058779e+00 9.662761e-01 8.819912e-01 8.051939e-01 7.352191e-01 6.714606e-01
(Intercept)                           .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.0 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.0 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)3.0 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.1 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.1 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.1 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.2 .        .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.2 .        7.719571e-12 1.475336e-11 2.116228e-11 2.700185e-11 3.232266e-11 3.717077e-11 4.158819e-11 4.561318e-11 4.928061e-11 5.262222e-11 5.566698e-11
poly(m, n, degree = 3, raw = TRUE)0.3 .        .            .            .            .            .            .            .            .            .            .            .           
                                                                                                                                                                                                
(Intercept)                           6.133663e-01 5.604329e-01 5.122020e-01 4.682558e-01 4.282136e-01 3.917287e-01 3.58485e-01 3.281945e-01 3.005950e-01 2.754474e-01 2.525338e-01 2.316558e-01
(Intercept)                           .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.0 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.0 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)3.0 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.1 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.1 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.1 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.2 .            .            .            .            .            .            .           .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.2 5.844125e-11 6.096906e-11 6.327231e-11 6.537095e-11 6.728314e-11 6.902547e-11 7.06130e-11 7.205951e-11 7.337751e-11 7.457843e-11 7.567266e-11 7.666968e-11
poly(m, n, degree = 3, raw = TRUE)0.3 .            .            .            .            .            .            .           .            .            .            .            .           
                                                                                                                                                                                                 
(Intercept)                           2.126325e-01 1.944718e-01 1.760072e-01 1.593252e-01 1.453801e-01 1.347481e-01 1.243893e-01 1.147846e-01 1.060850e-01 9.799656e-02 9.085799e-02 8.415107e-02
(Intercept)                           .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.0 .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.0 .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)3.0 .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.1 .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.1 .            5.327219e-10 2.252483e-09 3.731841e-09 4.287865e-09 3.533223e-09 3.277304e-09 3.147826e-09 2.997486e-09 2.961163e-09 2.783876e-09 2.748938e-09
poly(m, n, degree = 3, raw = TRUE)2.1 .            .            .            1.366468e-14 8.107164e-14 3.971581e-13 6.834701e-13 9.332186e-13 1.164130e-12 1.362630e-12 1.559849e-12 1.726540e-12
poly(m, n, degree = 3, raw = TRUE)0.2 .            .            .            .            .            .            .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.2 7.757813e-11 7.828961e-11 7.866848e-11 7.901913e-11 7.945636e-11 7.987470e-11 8.016336e-11 8.041490e-11 8.064780e-11 8.084997e-11 8.104925e-11 8.121624e-11
poly(m, n, degree = 3, raw = TRUE)0.3 .            .            .            .            .            .            .            .            .            .            .            .           
                                                                                                                   
(Intercept)                           7.823185e-02 7.261557e-02 6.766487e-02 6.310389e-02 5.889144e-02 5.498933e-02
(Intercept)                           .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.0 .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)2.0 .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)3.0 .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)0.1 .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.1 2.597321e-09 2.598535e-09 2.495350e-09 2.432951e-09 2.411540e-09 2.431915e-09
poly(m, n, degree = 3, raw = TRUE)2.1 1.891307e-12 2.027176e-12 2.161265e-12 2.281295e-12 2.387113e-12 2.479269e-12
poly(m, n, degree = 3, raw = TRUE)0.2 .            .            .            .            .            .           
poly(m, n, degree = 3, raw = TRUE)1.2 8.138162e-11 8.151619e-11 8.165125e-11 8.176957e-11 8.187319e-11 8.196318e-11
poly(m, n, degree = 3, raw = TRUE)0.3 .            .            .            .            .            .
#+end_example

Let's pick one...
#+begin_src R :results output :session *R* :exports both
coef(ridge.mod)[,5]
#+end_src

#+RESULTS:
:                           (Intercept)                           (Intercept) poly(m, n, degree = 3, raw = TRUE)1.0 poly(m, n, degree = 3, raw = TRUE)2.0 poly(m, n, degree = 3, raw = TRUE)3.0 
:                          1.271720e+00                          0.000000e+00                          0.000000e+00                          0.000000e+00                          0.000000e+00 
: poly(m, n, degree = 3, raw = TRUE)0.1 poly(m, n, degree = 3, raw = TRUE)1.1 poly(m, n, degree = 3, raw = TRUE)2.1 poly(m, n, degree = 3, raw = TRUE)0.2 poly(m, n, degree = 3, raw = TRUE)1.2 
:                          0.000000e+00                          0.000000e+00                          0.000000e+00                          0.000000e+00                          2.700185e-11 
: poly(m, n, degree = 3, raw = TRUE)0.3 
:                          0.000000e+00

OK, I'll stop here for today and give it a try with other datasets.
**** More information in the traces of =dgemm= and =dtrsm=  :SMPI:TRACING:HPL:
- Did new experiments. Now, the real time *and* the expected time are printed.
  #+begin_src sh :results output
  tail /home/tom/tmp/16/taurus_144_2/output_vanilla   -n 16 | head -n 4
  tail /home/tom/tmp/16/taurus_144_2/output_optimized -n  8 | head -n 4
  #+end_src

  #+RESULTS:
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12    12              16.95              3.147e+02
  : ================================================================================
  : T/V                N    NB     P     Q               Time                 Gflops
  : --------------------------------------------------------------------------------
  : WR00L2L2       20000   120    12    12              40.27              1.324e+02

- Processing:
#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?) expected_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:
        csv_writer = csv.writer(out_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'real_time', 'expected_time'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 13))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
process('/home/tom/tmp/16/taurus_144_2/output_optimized', '/home/tom/tmp/16/taurus_144_2/calls_optimized.csv')
process('/home/tom/tmp/16/taurus_144_2/output_vanilla',   '/home/tom/tmp/16/taurus_144_2/calls_vanilla.csv')
#+end_src

#+RESULTS:
: None

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  calls_vanilla <- read.csv("/home/tom/tmp/16/taurus_144_2/calls_vanilla.csv");
  vanilla_dgemm = calls_vanilla[calls_vanilla$func == "dgemm",]
  vanilla_dtrsm = calls_vanilla[calls_vanilla$func == "dtrsm",]
  calls_optimized <- read.csv("/home/tom/tmp/16/taurus_144_2/calls_optimized.csv");
  optimized_dgemm = calls_optimized[calls_optimized$func == "dgemm",]
  optimized_dtrsm = calls_optimized[calls_optimized$func == "dtrsm",]
  #+end_src

  #+RESULTS:

  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  optimized_dtrsm$optimized = TRUE
  optimized_dgemm$optimized = TRUE
  vanilla_dtrsm$optimized = FALSE
  vanilla_dgemm$optimized = FALSE
  data_dtrsm = rbind(optimized_dtrsm, vanilla_dtrsm)
  data_dgemm = rbind(optimized_dgemm, vanilla_dgemm)
  data_dtrsm$size_product = data_dtrsm$m * data_dtrsm$n**2
  data_dgemm$size_product = data_dgemm$m * data_dgemm$n* data_dgemm$k
  #+end_src

  #+RESULTS:

  #+begin_src R :file trace_visualization/taurus/3.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(data_dtrsm, aes(x=size_product, y=real_time, color=optimized)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dtrsm for different sizes")
  plot1
  plot2 = ggplot(data_dgemm, aes(x=size_product, y=real_time, color=optimized)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dgemm for different sizes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/3.png]]

- So we have still the same thing for the real time. Let’s look at the expected time.

  #+begin_src R :results output :session *R* :exports both
  optimized_dgemm$size_product = optimized_dgemm$m * optimized_dgemm$n * optimized_dgemm$k
  melted_dgemm = melt(optimized_dgemm[c("expected_time", "real_time", "size_product")], id.vars = c("size_product"), variable.name="time_type", value.name="time")
  #+end_src

  #+RESULTS:
  :   size_product     time_type time
  : 1            0 expected_time    0
  : 2            0 expected_time    0
  : 3            0 expected_time    0
  : 4            0 expected_time    0
  : 5            0 expected_time    0
  : 6            0 expected_time    0

  #+begin_src R :file trace_visualization/taurus/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  ggplot(melted_dgemm, aes(x=size_product, y=time, color=time_type)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dgemm for different sizes")
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/4.png]]

- The computation of the expected time seems to be ok. But the “real time” (i.e. the time “spent” in =smpi_execute=) is
  broken. It is very strange that it works fine for other number of processes and not this one.
**** Also plotting the logs                             :SMPI:TRACING:HPL:

#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?) expected_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)')
float_str = '([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)'
reg_str = '\[taurus-[0-9]+\.lyon\.grid5000\.fr:[0-9]+:\(([0-9]+)\) {float_str}\] /root/simgrid/src/smpi/smpi_bench\.cpp:[0-9]+: \[smpi_bench/DEBUG\] Sleep for {float_str} to handle real computation time'.format(float_str=float_str)
reg_log = re.compile(reg_str)

def process(in_file, out_file, log_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f, open(log_file, 'w') as log_f:
        csv_writer = csv.writer(out_f)
        log_csv_writer = csv.writer(log_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'real_time', 'expected_time'))
        log_csv_writer.writerow(('rank', 'start_time', 'duration'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 13))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
            else:
                match = reg_log.match(line)
                if match is not None:
                    result = list(match.group(i) for i in range(1, 4))
                    log_csv_writer.writerow(result)
process('/home/tom/tmp/16/output_log', '/home/tom/tmp/16/calls.csv', '/home/tom/tmp/16/log.csv')
#+end_src

- Analysis:
  #+begin_src R :results output :session *R* :exports both
  log <- read.csv("/home/tom/tmp/16/log.csv");
  calls <- read.csv("/home/tom/tmp/16/calls.csv");
  #+end_src

  #+begin_src R :results output :session *R* :exports both
  log$idx = 1:length(log$rank)
  calls$idx = 1:length(calls$rank)
  #+end_src

  #+begin_src R :file trace_visualization/taurus/5.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(log, aes(x=idx, y=duration, color=rank)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Durations of smpi_execute")
  plot1
  plot2 = ggplot(calls, aes(x=idx, y=real_time, color=rank)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Real times of dgemm and dtrsm")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/5.png]]

  #+begin_src R :file trace_visualization/taurus/6.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(log, aes(x=idx, y=duration, color=rank)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Durations of smpi_execute")
  plot1
  plot2 = ggplot(calls, aes(x=idx, y=expected_time, color=rank)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Expected times of dgemm and dtrsm")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/taurus/6.png]]

**** Discuss with Arnaud about the =smpi_execute= bug         :SMPI:BUG:HPL:
- The issue is that we have to call =smpi_bench_stop()= before the =smpi_execute()= and =smpi_bench_begin()= after. Otherwise,
  the time of =smpi_execute= will be injected in another process.
*** 2017-05-13 Saturday
**** Replace the calls to =smpi_execute=                      :SMPI:BUG:HPL:
- Cannot directly call =smpi_bench_begin()= and =smpi_bench_end()= in HPL, these are not public functions.
- Instead, create a new function in Simgrid:
  #+begin_src c
  void smpi_execute_public(double duration) {
    smpi_bench_end();
    smpi_execute(duration);
    smpi_bench_begin();
  }
  #+end_src
- Quick test with N=144:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              15.27              3.493e+02
  HPL_pdgesv() start time Sat May 13 16:19:08 2017
  
  HPL_pdgesv() end time   Sat May 13 16:20:59 2017
  
  [15.380599] [surf_energy/INFO] Total energy consumption: 42812.084837 Joules (used hosts: 36048.466481 Joules; unused/idle hosts: 6763.618356)
  [15.380599] [smpi_kernel/INFO] Simulated time: 15.3806 seconds. 
  
  The simulation took 110.989 seconds (after parsing and platform setup)
  8.71235 seconds were actual computation of the application
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 2982.194485 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 3151.383213 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 3001.033319 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 3201.287597 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 3002.750661 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 1768.768871 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 1768.768871 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 3037.834603 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 1457.311744 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 2934.724512 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 2958.510898 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 2926.972269 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 2917.804452 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 3009.527720 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 2924.442754 Joules
  [15.380599] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 1768.768871 Joules
  #+end_example
- Recall what we got with vanilla HPL:
  #+begin_example
  ================================================================================
  T/V                N    NB     P     Q               Time                 Gflops
  --------------------------------------------------------------------------------
  WR00L2L2       20000   120    12    12              16.56              3.221e+02
  HPL_pdgesv() start time Thu May 11 14:21:41 2017
  
  HPL_pdgesv() end time   Thu May 11 14:39:39 2017
  
  --------------------------------------------------------------------------------
  ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)= 2394248404.8693275 ...... FAILED
  ||Ax-b||_oo  . . . . . . . . . . . . . . . . . =          61.313920
  ||A||_oo . . . . . . . . . . . . . . . . . . . =        5077.970709
  ||A||_1  . . . . . . . . . . . . . . . . . . . =        5073.290150
  ||x||_oo . . . . . . . . . . . . . . . . . . . =           2.271123
  ||x||_1  . . . . . . . . . . . . . . . . . . . =        8563.901030
  ||b||_oo . . . . . . . . . . . . . . . . . . . =           0.499989
  [16.918235] [surf_energy/INFO] Total energy consumption: 47289.777830 Joules (used hosts: 39849.983792 Joules; unused/idle hosts: 7439.794038)
  [16.918235] [smpi_kernel/INFO] Simulated time: 16.9182 seconds. 
  
  The simulation took 1112.83 seconds (after parsing and platform setup)
  803.275 seconds were actual computation of the application
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-1.lyon.grid5000.fr: 3294.134621 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-10.lyon.grid5000.fr: 3482.679266 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-11.lyon.grid5000.fr: 3318.958204 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-12.lyon.grid5000.fr: 3539.088201 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-13.lyon.grid5000.fr: 3319.812144 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-14.lyon.grid5000.fr: 1945.597076 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-15.lyon.grid5000.fr: 1945.597076 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-16.lyon.grid5000.fr: 3359.424506 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-2.lyon.grid5000.fr: 1603.002809 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-3.lyon.grid5000.fr: 3243.553112 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-4.lyon.grid5000.fr: 3270.738159 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-5.lyon.grid5000.fr: 3235.614523 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-6.lyon.grid5000.fr: 3225.269605 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-7.lyon.grid5000.fr: 3327.203930 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-8.lyon.grid5000.fr: 3233.507520 Joules
  [16.918235] [surf_energy/INFO] Energy consumption of host taurus-9.lyon.grid5000.fr: 1945.597076 Joules
  #+end_example
- So the issue seems to be fixed. But the accuracy is not crazy (error of about 10% for both the time and the Gflops).
**** DONE Found something strange in the topology file, need to ask :SMPI:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:37]
- State "TODO"       from              [2017-05-13 Sat 19:54]
:END:
- Why is the running power so low?
  #+begin_src sh
  grep running-power ../hpl-2.2/bin/SMPI/platform_taurus_hpl.xml
  #+end_src

  #+RESULTS:
  : smpi/running-power" value="20000
- When running the calibration on =taurus-7=:
  #+begin_src
  python calibrate_flops.py
  #+end_src

  #+begin_example
  Callibrating code compiled
  One-host XML platform file generated
  One-host hostfile generated
  Initiating binary search...
  candidate value: 250000000000.000	-->  wallclock = 9.037 (target =0.120)
  candidate value: 125000000000.000	-->  wallclock = 4.496 (target =0.120)
  candidate value: 62500000000.000	-->  wallclock = 2.246 (target =0.120)
  candidate value: 31250000000.000	-->  wallclock = 1.128 (target =0.120)
  candidate value: 15625000000.000	-->  wallclock = 0.559 (target =0.120)
  candidate value: 7812500000.000	-->  wallclock = 0.281 (target =0.120)
  candidate value: 3906250000.000	-->  wallclock = 0.140 (target =0.120)
  candidate value: 1953125000.000	-->  wallclock = 0.070 (target =0.120)
  candidate value: 2929687500.000	-->  wallclock = 0.105 (target =0.120)
  candidate value: 3417968750.000	-->  wallclock = 0.123 (target =0.120)
  candidate value: 3173828125.000	-->  wallclock = 0.114 (target =0.120)
  candidate value: 3295898437.500	-->  wallclock = 0.118 (target =0.120)
  candidate value: 3356933593.750	-->  wallclock = 0.120 (target =0.120)
  Run smpirun with --cfg=smpi/running-power:3356933593.750
  #+end_example
- Answer: the running power is set to an arbitrary value, equal to the speed of all the nodes. This is done when the
  simulation runs on the target platform, it basically says “do not scale the measured times”, take them as is.
**** Comparing the results                          :SMPI:EXPERIMENTS:HPL:
- Updated the script to use the energy plugin and take as input an existing topology file.
- Simgrid commit: =6a1d642d50bc3a7d4b4c6b86755822f7ea773395= (not pushed (yet))
- HPL commit: =9ab3bdea0ca8750207b827837b509f4f5ce5d61a=
- Script commit: =d8c903226a89192f438c562a7eaa71e643493726=
- Compilation:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.445036e-10 -DSMPI_DTRSM_COEFFICIENT=1.259681e-10" -j 4 arch=SMPI
  #+end_src
- Command lines to run the experiment:
  #+begin_src sh
  cpupower -c all frequency-set -g userspace
  cpupower -c all frequency-set -d 2300MHz -u 2300MHz
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv /root/results.csv --nb_runs 1 --size 20000 --nb_proc
  12,24,36,48,60,72,84,96,108,120,132,144,156,168,180,192 --topo ../hpl-2.2/bin/SMPI/platform_taurus_hpl.xml
  --experiment HPL --energy --nb_cpu 12
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  get_results <- function(nb_proc) {
      result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
      result$time = max(result$time)
      result$total_energy = sum(result$power_consumption)
      used_energy = 0
      for(i in (1:(nb_proc/12))) {
          used_energy = used_energy + result[result$hostname == paste('taurus-', i, sep=''),]$power_consumption
      }
      result$used_energy = used_energy
      result$nb_proc = nb_proc
      return(unique(result[c('nb_proc', 'time', 'total_energy', 'used_energy')]))
  }
  original_results = data.frame()
  #  for(i in (c(1,4,8,12,48,96,144))) {
  for(i in (c(12,48,96,144))) {
    original_results = rbind(original_results, get_results(i))
  }
  original_results$optimized = FALSE
  new_results <- read.csv('hpl_analysis/taurus/hpl.csv')
  new_results$optimized = TRUE
  results = rbind(original_results, new_results[c('nb_proc', 'time', 'total_energy', 'used_energy', 'optimized')])
  #+end_src

  #+begin_src R :file hpl_analysis/taurus/1.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=nb_proc, y=time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/1.png]]

- Again, the big issue we had with =smpi_execute= seems to really be fixed, but there remains some problems, especially
  for the low number of processes.
- An hypothesis for this bad accuracy is that optimized HPL does not do the initialization and the verifications. It
  seems that these two phases are accounted in the time and the energy reported in the paper. For instance, for 12
  processes, there is a difference of 3 seconds.
- With the given topology file, we can run up to 12\times16 processes. But for a higher number, a new topology file has to be
  generated (with all the energy-related coefficients).
  #+begin_src R :file hpl_analysis/taurus/2.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(new_results, aes(x=nb_proc, y=simulation_time)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Simulation time for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(new_results, aes(x=nb_proc, y=memory_size)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Memory consumption for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/2.png]]

- No surprise here, the simulation time is linear in the number of processes and the memory consumption is very low.
**** DONE Tasks for the energy paper [3/3]                      :SMPI:HPL:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-05-18 Thu 18:38]
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:38]
- State "TODO"       from "TODO"       [2017-05-18 Thu 18:38]
- State "TODO"       from              [2017-05-13 Sat 23:20]
:END:
- [X] Fix the accuracy issue. Add a compilation option to re-enable the initialization and the verification in HPL.
- [X] Re-do the above tests when the accuracy is fixed.
- [X] Test for larger number of processes.
*** 2017-05-14 Sunday
**** Run HPL with the initialization and the verification :SMPI:EXPERIMENTS:HPL:
- No, HPL can be compiled with option =-DSMPI_DO_INITIALIZATION_VERIFICATION= (in addition of =-DSMPI_OPTIMIZATION=) to do
  the initialization and the verification. The “full time” is also reported by the script.
- Updated the script to use the energy plugin and take as input an existing topology file.
- Simgrid commit: =6a1d642d50bc3a7d4b4c6b86755822f7ea773395= (not pushed (yet))
- HPL commit: =845162e571ec9490492215c4e7f874ad3cbe98c2=
- Script commit: =a340f31e37d8c3ebaaf13a7ef12693bc97e09b53=
- Compilation:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_DO_INITIALIZATION_VERIFICATION -DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.445036e-10
  -DSMPI_DTRSM_COEFFICIENT=1.259681e-10" -j 4 arch=SMPI
  #+end_src
- Command lines to run the experiment:
  #+begin_src sh
  cpupower -c all frequency-set -g userspace
  cpupower -c all frequency-set -d 2300MHz -u 2300MHz
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv /root/results.csv --nb_runs 1 --size 20000 --nb_proc
  12,24,36,48,60,72,84,96,108,120,132,144,156,168,180,192 --topo ../hpl-2.2/bin/SMPI/platform_taurus_hpl.xml
  --experiment HPL --energy --nb_cpu 12
  #+end_src
- Analysis:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  get_results <- function(nb_proc) {
      result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
      result$full_time = max(result$time)
      result$total_energy = sum(result$power_consumption)
      used_energy = 0
      for(i in (1:(nb_proc/12))) {
          used_energy = used_energy + result[result$hostname == paste('taurus-', i, sep=''),]$power_consumption
      }
      result$used_energy = used_energy
      result$nb_proc = nb_proc
      return(unique(result[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]))
  }
  original_results = data.frame()
  #  for(i in (c(1,4,8,12,48,96,144))) {
  for(i in (c(12,48,96,144))) {
    original_results = rbind(original_results, get_results(i))
  }
  original_results$optimized = FALSE
  new_results <- read.csv('hpl_analysis/taurus/hpl2.csv')
  new_results$optimized = TRUE
  results = rbind(original_results, new_results[c('nb_proc', 'full_time', 'total_energy', 'used_energy', 'optimized')])
  #+end_src

  #+RESULTS:

  #+begin_src R :file hpl_analysis/taurus/3.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=nb_proc, y=full_time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/3.png]]

- The plots are very similar. The distances between the two curves has been slightly reduced in the time plot, but
  slightly increased in the energy plot.
*** 2017-05-15 Monday
**** Fix the plots                                            :SMPI:R:HPL:
- The energy plots were wrong. To compute the total energy consumption, it was assumed that the nodes involved in the
  computation were the first n nodes (numerical order). Instead, we have to take the n biggest energy consummers.
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  get_results <- function(nb_proc) {
      result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
      result$full_time = max(result$time)
      result$total_energy = sum(result$power_consumption)

      used_energy = 0
      result = result[with(result, order(-power_consumption)),] # sort by power consumption
      result$used_energy = sum(head(result, nb_proc/12)$power_consumption)
      result$nb_proc = nb_proc
      return(unique(result[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]))
  }
  original_results = data.frame()
  #  for(i in (c(1,4,8,12,48,96,144))) {
  for(i in (c(12,48,96,144))) {
    original_results = rbind(original_results, get_results(i))
  }
  original_results$optimized = FALSE
  new_results <- read.csv('hpl_analysis/taurus/hpl2.csv')
  new_results$optimized = TRUE
  results = rbind(original_results, new_results[c('nb_proc', 'full_time', 'total_energy', 'used_energy', 'optimized')])
  #+end_src

  #+RESULTS:

  #+begin_src R :file hpl_analysis/taurus/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=nb_proc, y=full_time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/4.png]]

- Indeed, it is better!
**** Re-run the experiment, but with a fat-tree               :SMPI:R:HPL:
- We do the same experiment (same matrix size, same number of nodes), but with a new topology file. It is a fat-tree
  that is (hopefully) close to the initial topology file.
- New topology file:
  #+begin_src xml
  <?xml version='1.0' encoding='ASCII'?>
  <!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
  <platform version="4">
  <config id="General">
   <prop id="contexts/stack-size" value="1280000"/>
   <prop id="contexts/factory" value="raw"/>
   <prop id="surf/precision" value="1e-9"/>
   <prop id="tracing/precision" value="9"/>
   <prop id="smpi/os" value="0:3.79946267082783e-06:1.09809596167633e-10;1420:4.06752467953734e-06:8.98782555257323e-11;33500:6.01204627458251e-06:7.10122202865045e-11;65536:7.28270630967833e-05:1.9683266729216e-10;320000:0:0"/>
   <prop id="smpi/ois" value="0:3.65848336553355e-06:1.33280621516301e-10;1420:3.83673729379869e-06:7.84867337035856e-11;33500:5.57232433176236e-06:6.5668893954931e-11;65536:4.17803219267394e-06:2.37460347640595e-12;320000:4.70677307448713e-06:3.38065421824938e-13"/>
   <prop id="smpi/or" value="0:3.51809764924934e-06:3.01847204118237e-10;1420:8.16124874852713e-06:2.66840481979518e-10;33500:1.49347740713389e-05:1.97645004617501e-10;65536:5.88893263987424e-05:1.29160163208845e-09;320000:0:0"/>
   <prop id="smpi/bw-factor" value="0:0.0489825651012801;1420:0.824385608826111;33500:0.600278012183156;65536:1;320000:0.536759617074721"/>
   <prop id="smpi/lat-factor" value="0:1;1420:2.16408517748122;33500:1.76905573216394;65536:2.9114462429055;320000:2.5981998109037"/>
   <prop id="smpi/async-small-thresh" value="65536"/>
   <prop id="smpi/send-is-detached-thresh" value="320000"/>
   <prop id="smpi/wtime" value="4.195637e-08"/>
   <prop id="smpi/iprobe" value="3.49479752916953e-07"/>
   <prop id="smpi/test" value="2.46327639751553e-07"/>
   <prop id="smpi/display-timing" value="yes"/>
   <prop id="smpi/running-power" value="20000"/>
   <prop id="smpi/init" value="0.1"/>
  </config>

  <AS id="AS0" routing="Full">
    <cluster id="cluster0" prefix="host-" suffix=".hawaii.edu" radical="0-255" speed="20000" bw="10Gbps" lat="2.4E-5s" loopback_bw="5120MiBps" loopback_lat="1.5E-9s" topology="FAT_TREE" topo_parameters="2;16,16;1,2;1,1" core="12">
      <prop id="watt_per_state" value="115:120.75:214.75,94.62:119.38:207,94.38:118.12:198.94,94.5:116.88:190.62,94.44:115.62:183.62,94.5:114.62:177,94.62:113.5:170.12,94.62:112.25:163.44,94.62:111.5:157.75,94.62:110.5:152.12,94.62:109.62:146.815,94.62:108.75:142.12"/>
      <prop id="watt_off" value="10"/>
    </cluster>
  </AS>
  </platform>
  #+end_src
- HPL commit: =845162e571ec9490492215c4e7f874ad3cbe98c2=
- Script commit: =a1ad4b1752bdb2feb30b90a56373c90e1ab83886=
- Simgrid commit: =6a1d642d50bc3a7d4b4c6b86755822f7ea773395=
- Compilation:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_DO_INITIALIZATION_VERIFICATION -DSMPI_OPTIMIZATION -DSMPI_DGEMM_COEFFICIENT=2.445036e-10
  -DSMPI_DTRSM_COEFFICIENT=1.259681e-10" -j 4 arch=SMPI
  #+end_src
- Command lines to run the experiment:
  #+begin_src sh
  cpupower -c all frequency-set -g userspace
  cpupower -c all frequency-set -d 2300MHz -u 2300MHz
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv /root/results.csv --nb_runs 1 --size 20000 --nb_proc
  12,24,36,48,60,72,84,96,108,120,132,144,156,168,180,192 --topo big_taurus.xml
  --experiment HPL --energy --nb_cpu 12
  #+end_src

  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  original_results <- read.csv('hpl_analysis/taurus/hpl2.csv')
  original_results$topo = 'full_description'
  new_results <- read.csv('hpl_analysis/taurus/hpl_fattree.csv')
  new_results$topo = 'fat_tree'
  results = rbind(original_results, new_results)
  #+end_src

  #+RESULTS:

  #+begin_src R :file hpl_analysis/taurus/5.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=nb_proc, y=full_time, color=topo)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy, color=topo)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/5.png]]

- Good news: the new topology file has a nelgigible impact on the prediction. Thus, the prediction with a larger number
  of processes should be faithful.
- Note that the points for nb_proc=12 do not overlap well. The issue is that with the new topology file, there is no
  contention. It is impossible to fix with the current file format.
**** Run the experiment at larger scale                         :SMPI:HPL:
- Use the same command than previous section, but with larger numbers of processes.
- This was done in several taurus nodes, to speed-up things. So the results are split in separate files. Each time, the
  experiment with 192 nodes have been repeated, to control the accuracy.
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)
  get_results <- function(nb_proc) {
      result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
      result$full_time = max(result$time)
      result$total_energy = sum(result$power_consumption)

      used_energy = 0
      result = result[with(result, order(-power_consumption)),] # sort by power consumption
      result$used_energy = sum(head(result, nb_proc/12)$power_consumption)
      result$nb_proc = nb_proc
      return(unique(result[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]))
  }
  original_results = data.frame()
  #  for(i in (c(1,4,8,12,48,96,144))) {
  for(i in (c(12,48,96,144))) {
    original_results = rbind(original_results, get_results(i))
  }
  original_results$optimized = FALSE
  new_results <- read.csv('hpl_analysis/taurus/hpl_fattree.csv')
  for(i in (c(2, 3, 4, 5, 6, 7))) {
    new_results = rbind(new_results, read.csv(paste('hpl_analysis/taurus/hpl_fattree', i, '.csv', sep='')))
  }
  new_results$optimized = TRUE
  results = rbind(original_results, new_results[c('nb_proc', 'full_time', 'total_energy', 'used_energy', 'optimized')])
  new_results[new_results$nb_proc == 96,]$Gflops
  #+end_src

  #+RESULTS:
  : [1] 300.6

  #+begin_src R :file hpl_analysis/taurus/6.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results, aes(x=nb_proc, y=full_time, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy, color=optimized)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/6.png]]

  #+begin_src R :file hpl_analysis/taurus/7.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(new_results, aes(x=nb_proc, y=simulation_time)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Simulation time for different number of processes\nMatrix size: 20000")
  plot2 = ggplot(new_results[new_results$memory_size>0,], aes(x=nb_proc, y=memory_size)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Memory consumption for different number of processes\nMatrix size: 20000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/7.png]]

  #+begin_src R :results output :session *R* :exports both
  new_results[new_results$nb_proc > 3000,]
  #+end_src

  #+RESULTS:
  :                topology nb_roots nb_proc  size full_time  time Gflops
  : 29 file(big_taurus.xml)       -1    3072 20000   15.3762 15.11  353.1
  :    total_energy used_energy unused_energy simulation_time application_time
  : 29     824235.5    824235.5             0         6381.92          862.107
  :    user_time system_time major_page_fault minor_page_fault cpu_utilization uss
  : 29   6225.52      156.07                0          2229845            0.99   0
  :    rss page_table_size memory_size optimized
  : 29   0               0           0      TRUE

- The measure of the memory consumption for the experiment with 3072 processes obviously failed. Hypothesis: at line 157
  of the script, =smpimain= was not started yet when =get_pid= was called, so the =CalledProcessError= exception was raised
  and caught which led to an early exit of the function. This had no impact on the other metrics.
- Idea of fix: if the =get_pid= fails, retry a few times before exiting the function.
- Recall that =memory_size= is a rough estimation of the memory consumption. It should include =uss= and =page_table_size=
  (with huge pages, this is currently the only way to measure memory consumption). For small matrix sizes such as 20000,
  this seems quite inaccurate. Maybe just use =uss+page_table_size=.
  #+begin_src R :results output :session *R* :exports both
  new_results$expected_memory_size = new_results$uss + new_results$page_table_size
  new_results[c("expected_memory_size", "memory_size")]
  #+end_src

  #+RESULTS:
  #+begin_example
     expected_memory_size memory_size
  1              26888256    20561920
  2              56927424    32997376
  3              52372032    28852224
  4              73947456    36708352
  5              27260544     7053312
  6              68710464    33894400
  7              76880832    35110912
  8              59431008    34123776
  9              66302944    33714176
  10             63081920    35557376
  11             20641440    10022912
  12             39610976    20791296
  13             51085152    30978048
  14             46783520    27922432
  15             70239744    39161856
  16             59996896    33476608
  17            279526016   164806656
  18            199123488   113041408
  19            132225088    75259904
  20             76614784    47927296
  21            464356480   286871552
  22             77634400    51994624
  23            701232704   465805312
  24            578336192   373837824
  25             77126784    47927296
  26            116276512    68075520
  27            106086720    64073728
  28             91469920    57147392
  29                    0           0
  30             77106112    37146624
  31            828978304   543621120
  32           1425625856  1026359296
  33             76586112    40923136
  #+end_example

**** Experiment with larger matrix                              :SMPI:HPL:
- Tests for a matrix size of 65536. Same commands and commits than previous section.
  #+begin_src R :file hpl_analysis/taurus/8.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  results <- rbind(read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix1.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix2.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix3.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix4.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix5.csv'))
  plot1 = ggplot(results, aes(x=nb_proc, y=full_time)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of HPL for different number of processes\nMatrix size: 65536")
  plot2 = ggplot(results, aes(x=nb_proc, y=used_energy)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Energy consumption of HPL for different number of processes\nMatrix size: 65536")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/8.png]]

  #+begin_src R :file hpl_analysis/taurus/9.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  results <- rbind(read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix1.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix2.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix3.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix4.csv'), read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix5.csv'))
  plot1 = ggplot(results, aes(x=nb_proc, y=simulation_time)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Simulation time for different number of processes\nMatrix size: 65536")
  plot2 = ggplot(results, aes(x=nb_proc, y=memory_size)) +
      geom_line(size=1) + geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Memory consumption for different number of processes\nMatrix size: 65536")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/9.png]]
**** Doing initialization and verification in HPL is really a pain :SMPI:HPL:
- With initialization and verification, for 192 processes, the simulation time is:
  #+begin_src R :results output :session *R* :exports both
  results = read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix1.csv')
  results[results$nb_proc==192,]$simulation_time
  #+end_src

  #+RESULTS:
  : [1] 916.763
- Without initialization and verification, for 192 processes:
  #+begin_src R :results output :session *R* :exports both
  read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix_noinitverif.csv')$simulation_time
  #+end_src

  #+RESULTS:
  : [1] 566.684

- With initialization and verification, for 3072 processes, the simulation time is:
  #+begin_src R :results output :session *R* :exports both
  results = read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix5.csv')
  results[results$nb_proc==3072,]$simulation_time
  #+end_src

  #+RESULTS:
  : [1] 21934.8
- Without initialization and verification, for 192 processes:
  #+begin_src R :results output :session *R* :exports both
  results = read.csv('hpl_analysis/taurus/hpl_fattree_bigmatrix_noinitverif2.csv')
  results[results$nb_proc == 3072,]$simulation_time
  #+end_src

  #+RESULTS:
  : [1] 21105.2
- If we really want to take into account the initialization and the verification in the time and energy estimations, we
  should try to optimize them.
**** Figure for the article                            :SMPI:R:ARNAUD:HPL:

#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(reshape2)
library(scales)
library(gridExtra)
#+end_src

#+RESULTS:

Loading =20,000= data:
#+begin_src R :results output :session *R* :exports both
get_results <- function(nb_proc) {
    result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
    result$full_time = max(result$time)
    result$total_energy = sum(result$power_consumption)
    
    used_energy = 0
    result = result[with(result, order(-power_consumption)),] # sort by power consumption
    result$used_energy = sum(head(result, nb_proc/12)$power_consumption)
    result$nb_proc = nb_proc
    return(unique(result[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]))
}
original_results = data.frame()
                                        #  for(i in (c(1,4,8,12,48,96,144))) {
for(i in (c(12,48,96,144))) {
    original_results = rbind(original_results, get_results(i))
}
original_results$optimized = FALSE
new_results <- read.csv('hpl_analysis/taurus/hpl_fattree.csv')
for(i in (c(2, 3, 4, 5, 6, 7))) {
    new_results = rbind(new_results, read.csv(paste('hpl_analysis/taurus/hpl_fattree', i, '.csv', sep='')))
}
new_results$optimized = TRUE
results = rbind(original_results, new_results[c('nb_proc', 'full_time', 'total_energy', 'used_energy', 'optimized')])

results$type="Reality"
results[results$optimized,]$type="Simulation"
results$Msize = 20000
#+end_src

#+RESULTS:


Loading =65,536= data and merging
#+begin_src R :results output :session *R* :exports both
get_results2 <- function(file) {
    results <- read.csv(file,header=T);
    results = results[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]
    results$optimized = TRUE;
    results$type="Simulation";
    results$Msize = 65536;
    return(results)
}

df <- rbind(get_results2('hpl_analysis/taurus/hpl_fattree_bigmatrix1.csv'), 
            get_results2('hpl_analysis/taurus/hpl_fattree_bigmatrix2.csv'),
            get_results2('hpl_analysis/taurus/hpl_fattree_bigmatrix3.csv'),
            get_results2('hpl_analysis/taurus/hpl_fattree_bigmatrix4.csv'),
            get_results2('hpl_analysis/taurus/hpl_fattree_bigmatrix5.csv')
)
df = rbind(df,data.frame(nb_proc=1, full_time=2000, total_energy=450000, used_energy=450000, optimized=F, type="Reality", Msize=65536))

results = rbind(results,df)
results$type=as.factor(results$type);
#+end_src

#+RESULTS:

#+begin_src R :file hpl_analysis/taurus/energy_paper.pdf :results value graphics :results output :session *R* :exports both  :width 6 :height 5.5
app = "HPL"
library(gridExtra)
get_legend<-function(myggplot){
    tmp <- ggplot_gtable(ggplot_build(myggplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
}


myscale = c(1,seq(from=64,to=257,by=64));
myproclist =  results$nb_proc
myproclist = c(12,4*12,8*12,12*12,myproclist[myproclist> 16*12])
df = results


## df = df[results$optimized == F | df$nb_proc %in% myproclist,]
makespan <- max(df[df$Msize==20000,]$full_time)
ideal_makespan <- function(procs) { 
    makespan*12/procs
}
p1 =  ggplot(df[df$Msize==20000  & (df$nb_proc %in% myproclist),], aes(x=as.numeric(nb_proc),y=full_time,color=type,linetype=type,shape=type)) + theme_bw() + 
    annotate(geom="text", label="Matrix Size: 20,000", color="black", size=4, x=128*12, y=25) +
    stat_function(fun=ideal_makespan, xlim=c(12,12*256), color="grey") + 
    annotate(geom="text", label="Ideal scaling", color="darkgrey", size=4, x=100*12, y=3) +
    scale_linetype_manual(name=app, values = c("solid", "dashed", "dotted")) +
    scale_color_manual(name=app, values = c("red", "darkgreen", "blue")) +
    scale_shape_manual(name=app, values = c(27,17,17)) +
    geom_line() + geom_point() + 
    geom_vline(xintercept = 16*12, color="darkgray", linetype="dashed") + annotate(geom="text", label="Above 1 switch", color="darkgrey", size=3, x=70*12, y=19) +
    scale_x_continuous(limits=c(0,NA), breaks=12*myscale, labels=paste0(myscale,"x12")) + 
    scale_y_continuous(limits=c(0,30), labels = comma) + 
    theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) + ylab("Run-time (in s)") +
    labs("x" = '') # + xlim(0,144)
p1_legend = get_legend(p1)
p1 = p1 + theme(legend.position="none")

p2 = ggplot(df[df$Msize==20000 & (df$optimized == F | df$nb_proc %in% myproclist),], aes(x=as.numeric(nb_proc),y=used_energy/1E3,color=type,linetype=type,shape=type)) + theme_bw() + 
                                        # scale_color_brewer(palette="Set1",name="") +
    scale_linetype_manual(name=app, values = c("solid", "dashed", "dotted")) +
    scale_color_manual(name=app, values = c("red", "darkgreen", "blue")) +
    scale_shape_manual(name=app, values = c(16,17,17)) +
    geom_line() + geom_point() +
    geom_vline(xintercept = 16*12, color="darkgray", linetype="dashed") + annotate(geom="text", label="Above 1 switch", color="darkgrey", size=3, x=70*12, y=500) +
    ylim(0,NA) +
    scale_x_continuous(limits=c(0,NA), breaks=12*myscale, labels=paste0(myscale,"x12")) + 
    scale_y_continuous(limits=c(0,NA), labels = comma) + 
    theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) + ylab("Energy (in kJ)") +
    labs("x" = "") 
p2 = p2 + theme(legend.position="none")

makespan2 <- max(df[df$Msize!=20000,]$full_time)
ideal_makespan2 <- function(procs) { 
    makespan2*12/procs
}
p3 =  ggplot(df[df$Msize!=20000,], aes(x=as.numeric(nb_proc),y=full_time,color=type,linetype=type,shape=type)) + theme_bw() + 
    annotate(geom="text", label="Matrix Size: 65,536", color="black", size=4, x=128*12, y=1000) +
    stat_function(fun=ideal_makespan2, xlim=c(12,12*256), color="grey") + 
    annotate(geom="text", label="Ideal scaling", color="darkgrey", size=4, x=100*12, y=100) +
    scale_linetype_manual(name=app, values = c("solid", "dashed", "dotted")) +
    scale_color_manual(name=app, values = c("red", "darkgreen", "blue")) +
    scale_shape_manual(name=app, values = c(27,17,17)) +
    geom_vline(xintercept = 16*12, color="darkgray", linetype="dashed") + annotate(geom="text", label="Above 1 switch", color="darkgrey", size=3, x=70*12, y=500) +
    geom_line() + geom_point() +
    scale_x_continuous(limits=c(0,NA), breaks=12*myscale, labels=paste0(myscale,"x12")) + 
    scale_y_continuous(limits=c(0,1200), labels = comma) + 
    theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) + ylab("Run-time (in s)") +
    labs("x" = '') # + xlim(0,144)
p3 = p3 + theme(legend.position="none")

p4 = ggplot(df[df$Msize!=20000,], aes(x=as.numeric(nb_proc),y=used_energy/1E3,color=type,linetype=type,shape=type)) + theme_bw() + 
                                        # scale_color_brewer(palette="Set1",name="") +
    scale_linetype_manual(name=app, values = c("solid", "dashed", "dotted")) +
    scale_color_manual(name=app, values = c("red", "darkgreen", "blue")) +
    scale_shape_manual(name=app, values = c(27,17,17)) +
    geom_line() + geom_point() +
    geom_vline(xintercept = 16*12, color="darkgray", linetype="dashed") + annotate(geom="text", label="Above 1 switch", color="darkgrey", size=3, x=70*12, y=300) +
    scale_x_continuous(limit=c(0,max(12*myscale)), breaks=12*myscale, labels=paste0(myscale,"x12")) + 
    scale_y_continuous(limits=c(0,3250), labels = comma) +
    theme(legend.position="top", legend.background = element_rect(fill = "white", colour = NA)) + ylab("Energy (in kJ)") +
    labs("x" = "") 
p4 = p4 + theme(legend.position="none")

lay <- rbind(c(1,1), c(2,3), c(4,5));
grid.arrange(p1_legend,p1,p2,p3,p4, layout_matrix = lay,widths=c(4,4), heights=c(.5,5,5));
#+end_src


#+RESULTS:
[[file:hpl_analysis/taurus/energy_paper.pdf]]
*** 2017-05-16 Tuesday
**** Add the report template                                       :LATEX:
- Mix the template given in the [[http://mosig.imag.fr/ProgramEn/ProjectOverview][Mosig website]] with [[https://github.com/swhatelse/M2_internship][this org-mode report]].
*** 2017-05-17 Wednesday
**** Add [[file:publication_574.pdf][D-mod-K routing providing non-blockign traffic  for shift permutations on real life fat trees]] :PAPER:
Bibtex: Zahavi10
**** Add [[file:desai2008.pdf][Are nonblockign networks really needed for high-end-computing workloads?]] :PAPER:
Bibtex: conf/cluster/DesaiBSI08
**** Add [[file:cf523c09d11aae609babd734caf43d88fb16.pdf][Technology driven, highly-scalable dragonfly topology]]     :PAPER:
Bibtex: Kim:2008:THD:1381306.1382129
**** Add [[file:2edfb9e580eb29688b4d0e100d621f4d2900.pdf][HyperX: topology, routing and packaging of efficient large-scale networks]] :PAPER:
Bibtex: Ahn:2009:HTR:1654059.1654101
*** 2017-05-18 Thursday
**** Created pull requests for last changes in Simgrid          :SMPI:GIT:
- Function =smpi_execute_public=, PR [[https://github.com/simgrid/simgrid/pull/167][#167]].
- Support for huge pages in SMPI, PR [[https://github.com/simgrid/simgrid/pull/168][#168]].
**** TODO Update the bibtex entry for the energy paper             :PAPER:
:LOGBOOK:
- State "TODO"       from              [2017-05-18 Thu 18:29]
:END:
- Grab the version submitted to Cluster once it is available publicly.
*** 2017-05-19 Friday
**** Work on the test of huge pages                                 :SMPI:
- See PR [[https://github.com/simgrid/simgrid/pull/168][#168]].
- The feature needs to be tested, but writing a test is non-trivial. Discussion on the IRC channel on how to do this.
- These two commands need to be done before:
  #+begin_src sh
  mkdir /home/huge && sudo mount none /home/huge -t hugetlbfs -o rw,mode=0777
  sudo sh -c "/bin/echo 1 > /proc/sys/vm/nr_hugepages"
  #+end_src
- This feature is (at least currently) only available on Linux.
- Proposed solution: do these setup commands in the setup of the continuous integration tool, not in the test (e.g. in
  =.travis.yml= for Travis).
- Then we have to enable the test only if certain conditions are met (e.g. the file system is mounted and there is at
  least one huge page available). In bash, this can be tested with:
  #+begin_src sh
  grep "/home/huge" /proc/mounts | grep hugetlbfs && [ `cat /proc/sys/vm/nr_hugepages` -gt 0 ]
  #+end_src
- But Martin said to do this with CMake. Not sure how it is possible.
- There is already a mounted hugetlbfs on Linux, in =/dev/hugepages=, so we could use it instead of mounting a new
  one. The problem is that it requires superuser rights (and we do not want to run =smpirun= in superuser mode...).
**** Add levels of optimization in HPL                          :SMPI:HPL:
- When compiling HPL for SMPI, one can pass a level of optimization with variable =SMPI_OPTIMIZATION_LEVEL=. For instance:
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=3 -DSMPI_DGEMM_COEFFICIENT=1.658198e-10 -DSMPI_DTRSM_COEFFICIENT=8.624970e-11" arch=SMPI
  #+end_src
- There is currently four levels:
  + Level 0: vanilla HPL
  + Level 1: =dgemm= and =dtrsm= are replaced by =smpi_execute=
  + Level 2: shared malloc is used for the matrix and the panels, panels are reused
  + Level 3: skipping the last expensive functions (including initialization and verification)
- The old option =SMPI_OPTIMIZATION= is still valid, it gives the maximal level.
- This structure is the one used in the report: three parts, one per optimization level. It should made the experiments
  easier and more reproducible.
- The different levels (and thus parts of the report) should be discussed. For instance, it makes sense to group the
  replacement of =dgemm= and =dtrsm= by =smpi_execute=, or to group the use of shared malloc for the matrix and the
  panels. But does it make sense to put the panel reusing in the shared malloc part? To put the huge pages in the same
  part than the removal of useless but heavy functions? Not sure. But splitting the report in too much parts would maybe
  be quite ugly.
*** 2017-05-22 Monday
**** Flame graph of vanilla HPL                       :SMPI:PROFILING:HPL:
- Using N=20000, P=Q=8
- Simgrid commit: =74ddd092a29a7f29f0c90d503ec2a3b1c5dc984c=
- HPL commit: =3984984256bef01eeffeddd2f7e593b0a108c13c=
- Compilation:
  #+begin_src sh
  make -j 4 arch=SMPI
  #+end_src
- Execution:
  #+begin_src sh
  sudo smpirun -wrapper "perf record -F1000 --call-graph dwarf" --cfg=smpi/bcast:mpich
  --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes --cfg=smpi/privatize-global-variables:dlopen
  --cfg=smpi/keep-temps:true -np 64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml ./xhpl
  #+end_src
- Post-processing:
  #+begin_src sh
  sudo perf script | ~/Documents/FlameGraph/stackcollapse-perf.pl --kernel | ~/Documents/FlameGraph/flamegraph.pl > flame_vanilla.svg
  #+end_src
- Result:
  [[file:flame_graphs/flame_vanilla.svg]]
**** Re-do plots for =dgemm= and =dtrsm= in HPL :SMPI:PYTHON:R:TRACING:HPL:REPORT:
- Using N=20000, P=Q=8
- Simgrid commit: =74ddd092a29a7f29f0c90d503ec2a3b1c5dc984c=
- HPL commit: =3984984256bef01eeffeddd2f7e593b0a108c13c=
- Compilation:
  #+begin_src sh
  make -j 4 SMPI_OPTS="-DSMPI_MEASURE" arch=SMPI
  #+end_src
- Execution:
  #+begin_src sh
  smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes
  --cfg=smpi/privatize-global-variables:dlopen -np 64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml
  ./xhpl > /tmp/output
  #+end_src
- Processing:
#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?) expected_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:
        csv_writer = csv.writer(out_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'time'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 12))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
process('/home/tom/tmp/17/output',   '/home/tom/tmp/17/data.csv')
#+end_src
- Plots
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  results <- read.csv("/home/tom/tmp/17/data.csv");
  results$id_call = 1:length(results$time)
  #+end_src

  #+begin_src R :file trace_visualization/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results[results$func == "dgemm",], aes(x=id_call, y=time, alpha=0.3)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      theme_bw()+
      theme(legend.position="none")+
      xlab("Call index")+
      ylab("Duration (seconds)")+
      ggtitle("Duration of dgemm during the execution of HPL")
  plot2 = ggplot(results[results$func == "dtrsm",], aes(x=id_call, y=time, alpha=0.3)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      theme_bw()+
      theme(legend.position="none")+
      xlab("Call index")+
      ylab("Duration (seconds)")+
      ggtitle("Duration of dtrsm during the execution of HPL")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/4.png]]

  #+begin_src R :file trace_visualization/blas_durations.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  grid.arrange(plot1, plot2, ncol=2) # very heavy pdf... cannot use it
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/blas_durations.pdf]]

  #+begin_src sh
  convert -density 400 trace_visualization/blas_durations.pdf -resize "50%" trace_visualization/blas_durations.png
  #+end_src

  [[file:trace_visualization/blas_durations.png]]

**** New plots for regression of =dgemm= and =dtrsm=                :R:REPORT:
- Take the results of the script =cblas_tests/runner.py=.
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  dgemm <- read.csv("blas_reg/laptop_dgemm.csv");
  storage.mode(dgemm$m) <- "double" # avoiding integer overflow when taking the product
  storage.mode(dgemm$n) <- "double"
  storage.mode(dgemm$k) <- "double"
  dtrsm <- read.csv("blas_reg/laptop_dtrsm.csv");
  #+end_src

  #+begin_src R :file blas_reg/dgemm.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  ggplot(dgemm, aes_string(x="m*n*k", y="time")) +
      geom_point()+
      geom_abline(slope=1.658198e-10, alpha=0.5)+
      expand_limits(x=0, y=0)+
      theme_bw()+
      ylab("Time (seconds)")+
      ggtitle("Linear regression of dgemm")
  #+end_src

  #+RESULTS:
  [[file:blas_reg/dgemm.pdf]]

  #+begin_src R :file blas_reg/dtrsm.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  ggplot(dtrsm, aes_string(x="m*n^2", y="time")) +
      geom_point() +
      geom_abline(slope=8.624970e-11, alpha=0.5)+
      expand_limits(x=0, y=0)+
      theme_bw()+
      ylab("Time (seconds)")+
      ggtitle("Linear regression of dtrsm")
  #+end_src

  #+RESULTS:
  [[file:blas_reg/dtrsm.pdf]]
**** Add [[file:predicting-energy-consumption-at-scale.pdf][Predicting energy consumption at scale]] :PAPER:
Bibtex: heinrich:hal-01523608
*** 2017-05-23 Tuesday
**** Validation of the smpi_execute      :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =3984984256bef01eeffeddd2f7e593b0a108c13c=
- Script commit: =eb071f09d822e1031ea0776949058bf2f55cb94a=
- Compilation and execution for vanilla HPL (made on =nova-10=)
  #+begin_src sh
  make arch=SMPI
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_proc.csv --nb_runs 1 --size 20000 --nb_proc 8,16,32,64,96,128 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 1 --size 5000,10000,15000,20000 --nb_proc 64 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8
  #+end_src
- Compilation and execution for optimized HPL (made on =nova-10=)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=1 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_proc.csv --nb_runs 3 --size 20000 --nb_proc 8,16,32,48,64,80,96,128 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 5000,10000,15000,20000,30000,40000 --nb_proc 64
  --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8
  #+end_src
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  library(grid)
  old <- rbind(read.csv("validation/result_size_L0.csv"), read.csv("validation/result_proc_L0.csv"))
  new <- rbind(read.csv("validation/result_size_L1.csv"), read.csv("validation/result_proc_L1.csv"))
  old$kernel_sampling = FALSE
  new$kernel_sampling =  TRUE
  results = rbind(old, new)
  generic_do_plot <- function(plot, fixed_shape=TRUE) {
      xrange = ggplot_build(plot)$panel$ranges[[1]]$x.range
      xwidth = xrange[2] - xrange[1]
      if(fixed_shape) {
          point = stat_summary(fun.y = mean, geom="point", shape=21)
      }
      else {
          point = stat_summary(fun.y = mean, geom="point")
      }
      return(plot +
          stat_summary(fun.data = mean_se, geom = "errorbar", width=xwidth/20)+
          stat_summary(fun.y = mean, geom="line")+
          point+
          theme_bw()+
          expand_limits(x=0, y=0))
  }
  do_plot <- function(df, x, y, color, color_title, fixed_val, other_fixed_val=-1) {
      if(y == "simulation_time") {
          y_title = "Simulation time (seconds)"
          title = "Simulation time"
      }
      else if(y == "memory_size") {
          y_title = "Memory consumption (bytes)"
          title = "Memory consumption"
      }
      else {
          stopifnot(y == "Gflops")
          y_title = "Performance estimation (Gflops)"
          title = "Performance estimation"
      }
      if(x == "size") {
          fixed_arg = "nb_proc"
          x_title = "Matrix size"
          title = paste(title, "for different matrix sizes\nUsing", fixed_val, "MPI processes")
      }
      else {
          stopifnot(x == "nb_proc")
          fixed_arg = "size"
          x_title = "Number of processes"
          title = paste(title, "for different number of processes\nUsing a matrix size of", format(fixed_val,big.mark=",",scientific=FALSE))
      }
      sub_df = df[df[fixed_arg] == fixed_val,]
      p = generic_do_plot(ggplot(sub_df, aes_string(x=x, y=y, linetype=color, color=color, group=color))) +
          ggtitle(title)+
          xlab(x_title)+
          ylab(y_title)+
          labs(colour=color_title)+
          labs(linetype=color_title)
      if(other_fixed_val != -1) {
          rect <- data.frame(xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf)
          my_xmin = other_fixed_val * 0.9
          my_xmax = other_fixed_val * 1.1
          my_ymax = max(sub_df[sub_df[x] == other_fixed_val,][y])
          y_delta = my_ymax * 0.1
          my_ymax = my_ymax + y_delta
          my_ymin = min(sub_df[sub_df[x] == other_fixed_val,][y]) - y_delta
          p = p + geom_rect(data=rect, aes(xmin=my_xmin, xmax=my_xmax, ymin=my_ymin, ymax=my_ymax),color="grey20", alpha=0.1, inherit.aes=FALSE)
      }
      return(p)
  }

  # From https://stackoverflow.com/a/38420690/4110059
  grid_arrange_shared_legend <- function(..., nrow = 1, ncol = length(list(...)), position = c("bottom", "right")) {

    plots <- list(...)
    position <- match.arg(position)
    g <- ggplotGrob(plots[[1]] + theme(legend.position = position))$grobs
    legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
    lheight <- sum(legend$height)
    lwidth <- sum(legend$width)
    gl <- lapply(plots, function(x) x + theme(legend.position = "none"))
    gl <- c(gl, nrow = nrow, ncol = ncol)

    combined <- switch(position,
                       "bottom" = arrangeGrob(do.call(arrangeGrob, gl),
                                              legend,
                                              ncol = 1,
                                              heights = unit.c(unit(1, "npc") - lheight, lheight)),
                       "right" = arrangeGrob(do.call(arrangeGrob, gl),
                                             legend,
                                             ncol = 2,
                                             widths = unit.c(unit(1, "npc") - lwidth, lwidth)))
    grid.newpage()
    grid.draw(combined)

  }

  do_multiple_plot <- function(df, x1, x2, y, color, color_title, fixed_x1, fixed_x2) {
      my_ymax = max(df[y])
      return(
          grid_arrange_shared_legend(
              do_plot(df, x1, y, color, color_title, fixed_x1, fixed_x2) + expand_limits(x=0, y=my_ymax),
              do_plot(df, x2, y, color, color_title, fixed_x2, fixed_x1) + expand_limits(x=0, y=my_ymax),
              nrow=1, ncol=2
          ))
  }
  #+end_src

  #+RESULTS:

  #+begin_src R :file validation/L1/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "simulation_time", "kernel_sampling", "Kernel sampling", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/1.pdf]]

  #+begin_src R :file validation/L1/2.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "simulation_time", "kernel_sampling", "Kernel sampling", 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/2.pdf]]

  #+begin_src R :file validation/L1/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "memory_size", "kernel_sampling", "Kernel sampling", 64)
  #+end_src

  #+results:
  [[file:validation/L1/3.pdf]]

  #+begin_src R :file validation/L1/4.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "memory_size", "kernel_sampling", "Kernel sampling", 20000)
  #+end_src

  #+results:
  [[file:validation/L1/4.pdf]]

  #+begin_src R :file validation/L1/5.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "Gflops", "kernel_sampling", "kernel sampling", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/5.pdf]]

  #+begin_src R :file validation/L1/6.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "Gflops", "kernel_sampling", "kernel sampling", 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/6.pdf]]

  #+begin_src R :file validation/L1/report_plot_gflops.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  do_multiple_plot(results, "size", "nb_proc", "Gflops", "kernel_sampling", "Kernel sampling", 64, 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/report_plot_gflops.pdf]]

  #+begin_src R :file validation/L1/report_plot_time.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  do_multiple_plot(results, "size", "nb_proc", "simulation_time", "kernel_sampling", "Kernel sampling", 64, 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/report_plot_time.pdf]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      x = x[x$size %in% (c(5000, 10000, 15000, 20000, 30000)),]
      x = x[x$nb_proc %in% (c(8, 16, 32, 64, 96, 128)),]
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(old)
  aggr_new = aggregate_results(new)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_old$Gflops
  #+end_src

  #+begin_src R :file validation/L1/3.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(aggr_new[aggr_new$nb_proc==64,], aes(x=size, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(aggr_new[aggr_new$size==20000,], aes(x=nb_proc, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different number of processes\nUsing a matrix size of 20,000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/L1/3.png]]
*** 2017-05-24 Wednesday
**** Re-do plots for =dgemm= and =dtrsm= in HPL    :SMPI:PYTHON:R:TRACING:HPL:
- Same thing than for [2017-05-22 Mon], but with =libatlas3-base= instead of =libatlas-cpp-0.6-2-dbg=.
- Using N=20000, P=Q=8
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =3984984256bef01eeffeddd2f7e593b0a108c13c=
- Compilation:
  #+begin_src sh
  make -j 4 SMPI_OPTS="-DSMPI_MEASURE" arch=SMPI
  #+end_src
- Execution:
  #+begin_src sh
  smpirun --cfg=smpi/bcast:mpich --cfg=smpi/running-power:6217956542.969 --cfg=smpi/display-timing:yes
  --cfg=smpi/privatize-global-variables:dlopen -np 64 -hostfile ./hostfile_64.txt -platform ./cluster_fat_tree_64.xml
  ./xhpl > /tmp/output
  #+end_src
- Processing:
#+begin_src python
import re
import csv
reg = re.compile('function=([a-zA-Z]+) file=([a-zA-Z0-9/_.-]+) line=([0-9]+) rank=([0-9]+) m=([0-9]+) n=([0-9]+) k=(-?[0-9]+) lead_A=([0-9]+) lead_B=([0-9]+) lead_C=(-?[0-9]+) real_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?) expected_time=([-+]?[0-9]*\.?[0-9]+(?:[eE][-+]?[0-9]+)?)')

def process(in_file, out_file):
    with open(in_file, 'r') as in_f, open(out_file, 'w') as out_f:
        csv_writer = csv.writer(out_f)
        csv_writer.writerow(('func', 'file', 'line', 'rank', 'n', 'm', 'k', 'lead_A', 'lead_B', 'lead_C', 'time'))
        for line in in_f:
            match = reg.match(line)
            if match is not None:
                result = list(match.group(i) for i in range(1, 12))
                result[1] = result[1][result[1].index('/hpl'):].lower()
                csv_writer.writerow(result)
process('/home/tom/tmp/18/output',   '/home/tom/tmp/18/data.csv')
#+end_src

- Plots
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  results <- read.csv("/home/tom/tmp/18/data.csv");
  results$id_call = 1:length(results$time)
  #+end_src

  #+begin_src R :file trace_visualization/5.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(results[results$func == "dgemm",], aes(x=id_call, y=time)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Time of dgemm during the execution of HPL")
  plot2 = ggplot(results[results$func == "dtrsm",], aes(x=id_call, y=time)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Time of dtrsm during the execution of HPL")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/5.png]]
**** Errors of the prediction of =dgemm= and =dtrsm= time   :SMPI:TRACING:HPL:
- Take the trace of *real* times for =dgemm= and =dtrsm= and compute the expected time, thanks to the coefficient we got.
- The goal is to try to understand why we get accuracy issues.
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  results <- read.csv("/home/tom/tmp/18/data.csv");
  results$id_call = 1:length(results$time)
  dgemm = results[results$func == "dgemm",]
  dtrsm = results[results$func == "dtrsm",]
  dgemm$expected_time = dgemm$m * dgemm$n * dgemm$k * 1.658198e-10 * (6217956542.969/1e9) # don’t forget to convert in virtual time
  dtrsm$expected_time = dtrsm$m * dtrsm$n**2 * 8.624970e-11 * (6217956542.969/1e9) # don’t forget to convert in virtual time
  #+end_src

  #+begin_src R :results output :session *R* :exports both
  sum(dgemm[dgemm$rank==1,]$time)
  sum(dgemm[dgemm$rank==1,]$expected_time)
  sum(dtrsm[dtrsm$rank==1,]$time)
  sum(dtrsm[dtrsm$rank==1,]$expected_time)
  #+end_src

  #+RESULTS:
  : [1] 42.64254
  : [1] 41.4653
  : [1] 2.165024
  : [1] 21.68943

  #+begin_src R :results output :session *R* :exports both
  dgemm$error = (dgemm$expected_time - dgemm$time)/dgemm$time
  dtrsm$error = (dtrsm$expected_time - dtrsm$time)/dtrsm$time
  #+end_src

  #+RESULTS:

  #+begin_src R :file trace_visualization/6.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(dgemm, aes(x=expected_time, y=error)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Error of dgemm time prediction")
  plot2 = ggplot(dtrsm, aes(x=expected_time, y=error)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Error of dtrsm time prediction")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/6.png]]

  #+begin_src R :file trace_visualization/7.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  ggplot(dgemm, aes_string(x="m*n*k", y="time")) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Real execution time of dgemm")+
      geom_abline(slope=1.658198e-10*(6217956542.969/1e9), color="red")
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/7.png]]

  #+begin_src R :file trace_visualization/8.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(dgemm, aes(x=id_call, y=m)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Values of the m parameter of dgemm")
  plot2 = ggplot(dgemm, aes(x=id_call, y=n)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Values of the n parameter of dgemm")
  plot3 = ggplot(dgemm, aes(x=id_call, y=k)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Values of the k parameter of dgemm")
  grid.arrange(plot1, plot2, plot3, ncol=3)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/8.png]]

  #+begin_src R :file trace_visualization/9.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  ggplot(dtrsm, aes_string(x="m*n**2", y="time")) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Real execution time of dtrsm")+
      geom_abline(slope=8.624970e-11*(6217956542.969/1e9), color="red")
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/9.png]]
  #+begin_src R :file trace_visualization/10.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = ggplot(dtrsm, aes(x=id_call, y=m)) +
      geom_point()+
      expand_limits(x=0, y=0)+
      ggtitle("Values of the m parameter of dtrsm")
  plot2 = ggplot(dtrsm, aes(x=id_call, y=n)) +
      geom_point() +
      expand_limits(x=0, y=0)+
      ggtitle("Values of the n parameter of dtrsm")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:trace_visualization/10.png]]

  - For =dgemm=, the predicted time is slightly lower, whereas for =dtrsm= the predicted time is completely off. Worse, for
    =dtrsm=, the real time is not proportionnal to mn^2, but seems to be proportionnal to m^2n. Maybe the configuration
    used in HPL for =dtrsm= is different than the one used in the calibration?
  - For both functions, one of the sizes is always less than 120. The other size(s) are much more variable and culminate
    at 2500 (which is 20000 / sqrt(64)). Should we keep calibraring with sizes sampled randomly in [0, 5000]? Or should
    we fix one to a small value?
- Let’s have a look at where these functions are called.
    #+begin_src R :results output :session *R* :exports both
    library(data.table)
    x = data.table(dtrsm)
    as.data.frame(x[, list(count=length(time), sum_time=sum(time), sum_expected_time=sum(expected_time)), by=c("file", "line")])
    as.data.frame(x[, list(count=length(time), mean_m=mean(m), mean_n=mean(n)), by=c("file", "line")])
    #+end_src

    #+RESULTS:
    :                                  file line  count   sum_time sum_expected_time
    : 1  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171 157328   0.610859         0.2046244
    : 2 /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363  10408 140.707661      1418.8813040
    :                                  file line  count     mean_m      mean_n
    : 1  /hpl-2.2/src/pfact/hpl_pdrpanllt.c  171 157328   3.118072    6.031933
    : 2 /hpl-2.2/src/pgesv/hpl_pdupdatett.c  363  10408 119.969254 1273.518063
- So there are ten times more calls in =pdrpanllt= than in =pdupdatett= but the total time of these calls is several order
  of magnitude lower. Also, the error made by =pdupdatett= is higher.
- The call made by =pdupdatett= is the following:
  #+begin_src c
  HPL_dtrsm( HplColumnMajor, HplRight, HplUpper, HplNoTrans, HplUnit, nn, jb, HPL_rone, L1ptr, jb, Uptr, LDU );
  #+end_src
- The call currently made by the calibration program is the following:
  #+begin_src c
  cblas_dtrsm(CblasColMajor, CblasRight, CblasLower, CblasNoTrans, CblasUnit, m, n, alpha, A, lead_A, B, lead_B);
  #+end_src
- The only difference seems to be upper vs lower. Replacing =CblasLower= by =CblasUpper= increases slightly the coefficient
  but does not change the model (we still have m \times n^2).
**** Be carefull with the BLAS library used              :PERFORMANCE:BUG:
- Spent one hour trying to figure out why I could not reproduce on my laptop the results from [2017-05-22 Mon] (“Re-do
  plots for =dgemm= and =dtrsm= in HPL”) with optimized HPL.
- The issue was that I used the coefficients:
  #+begin_example
  -DSMPI_DGEMM_COEFFICIENT=1.658198e-10 -DSMPI_DTRSM_COEFFICIENT=8.624970e-11
  #+end_example
- These coefficients come from files [[file:blas_reg/laptop_dgemm.csv]] and [[file:blas_reg/laptop_dtrsm.csv]]. They have been
  obtained with the library from package =libatlas3-base=.
- Meanwhile, I replaced it by =libatlas-cpp-0.6-2-dbg=, without thinking of changing the coefficients.
*** 2017-05-30 Tuesday
**** Accuracy bug with level 2 optimization                 :SMPI:BUG:HPL:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =3984984256bef01eeffeddd2f7e593b0a108c13c=
- The Gflops prediction is wrong when compiling with =SMPI_OPTIMIZATION_LEVEL=2= but not with levels 1 or 3, which is very
  strange. It seems to come from the application time.
- Example, for N=20000 and P=Q=4:
  | Level |    Gflops | Application time | Optimization                                                              |
  |-------+-----------+------------------+---------------------------------------------------------------------------|
  |     1 | 2.368e+01 |          23.5072 | using =smpi_execute= for =dgemm= and =dtrsm=                                    |
  |     2 | 2.086e+01 |          41.6384 | using shared malloc for matrix and panel buffer, reusing the panel buffer |
  |     3 | 2.418e+01 |         0.571554 | everything                                                                |
- The error made at level 2 is even higher for larger scales. Also, there is no reason for the level 2 optimization to
  increase the application time.
- The error still happens when removing the optimization for the panels (shared malloc and reuse).
- The error does not happen if we remove the “hack” in =HPL_pdmxswp= which sets =WORK[0]= to a constant to avoid a
  deadlock. But without this, HPL fails at larger scales. It is strange, this used to work fine.
- Tried to replace the constant value by a random one, but this causes a segmentation fault.
- It seems that we can get rid of this hack at optimization level 3, without having a deadlock (tested for a size up to
  300,000). The deadlock used to happen because, when using shared malloc, the values of the matrix grow and become =NaN=
  when reaching the maximal double. With optimization level 3, all the CBLAS functions have been removed, so no
  computation is done on the matrix and thus the values do not grow anymore.
- Let’s avoid the problem: reorder the sections of the report. Put the removal of useless functions right after the
  kernel sampling and before the memory folding. Thus, HPL without any expensive functions becomes level 2, and the
  shared malloc is done only at level 3 (without the ugly fix this time).
**** Validation of the removal of HPL functions :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =a34d2a0b1822bc07067c111ed6876e81926a6972=
- Script commit: =eb071f09d822e1031ea0776949058bf2f55cb94a=
- Compilation and execution for optimized HPL (made on =nova-10=)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=2 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_proc.csv --nb_runs 3 --size 20000 --nb_proc 8,16,32,48,64,80,96,128 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 5000,10000,15000,20000,30000,40000 --nb_proc 64
  --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8
  #+end_src
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  old <- rbind(read.csv("validation/result_size_L1.csv"), read.csv("validation/result_proc_L1.csv"))
  new <- rbind(read.csv("validation/result_size_L2.csv"), read.csv("validation/result_proc_L2.csv"))
  old$computation_pruning = FALSE
  new$computation_pruning =  TRUE
  results = rbind(old, new)
  #+end_src

  #+RESULTS:

  #+begin_src R :file validation/L2/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "simulation_time", "computation_pruning", "Computation pruning", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/1.pdf]]

  #+begin_src R :file validation/L2/2.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "simulation_time", "computation_pruning", "Computation pruning", 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/2.pdf]]


  #+begin_src R :file validation/L2/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "memory_size", "computation_pruning", "computation pruning", 64)
  #+end_src

  #+results:
  [[file:validation/L2/3.pdf]]

  #+begin_src R :file validation/L2/4.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "memory_size", "computation_pruning", "computation pruning", 20000)
  #+end_src

  #+results:
  [[file:validation/L2/4.pdf]]

  #+begin_src R :file validation/L2/5.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "Gflops", "computation_pruning", "computation pruning", 64)
  #+end_src

  #+results:
  [[file:validation/L2/5.pdf]]

  #+begin_src R :file validation/L2/6.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "Gflops", "computation_pruning", "computation pruning", 20000)
  #+end_src

  #+results:
  [[file:validation/L2/6.pdf]]

  #+begin_src R :file validation/L2/report_plot_time.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  do_multiple_plot(results, "size", "nb_proc", "simulation_time", "computation_pruning", "Computation pruning", 64, 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/report_plot_time.pdf]]

  #+begin_src R :file validation/L2/report_plot_memory.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  do_multiple_plot(results, "size", "nb_proc", "memory_size", "computation_pruning", "Computation pruning", 64, 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/report_plot_memory.pdf]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      x = x[x$size %in% (c(5000, 10000, 15000, 20000, 30000, 40000)),]
      x = x[x$nb_proc %in% (c(8, 16, 32, 48, 64, 80, 96, 128)),]
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(old)
  aggr_new = aggregate_results(new)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_new$Gflops
  #+end_src

  #+begin_src R :file validation/L2/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(aggr_new[aggr_new$nb_proc==64,], aes(x=size, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(aggr_new[aggr_new$size==20000,], aes(x=nb_proc, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different number of processes\nUsing a matrix size of 20,000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/4.png]]
  - Changing the optimization levels showed something: removing all the time consumming functions is not only a good
    optimization for the simulation time, but also for the memory consumption which dropped by more than one order of magnitude.
  - The reason for that seems to be due to the laziness of the operating systems. It allocates a physical page only when
    the corresponding virtual page is accessed. Thus, if we remove all the functions that use this page, we save memory.
  - The Gflops error is not so low. Maybe we should try to inject the durations of some of these functions, as a future work?
  #+begin_src R :file validation/L2/5.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(results[results$nb_proc==64,], aes(x=size, y=minor_page_fault, color=computation_pruning))) +
      ggtitle("Number of page faults for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(results[results$nb_proc==64,], aes(x=size, y=system_time, color=computation_pruning))) +
      ggtitle("System time for different matrix sizes\nUsing 64 MPI processes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/L2/5.png]]
  - The plots confirm the hypothesis: much less page faults are done, so the physical pages are certainly not allocated.
**** Add a new optimization level                               :SMPI:HPL:
- The level three added to the level two the shared malloc *and* the panel reuse.
- This is now splitted into two levels:
  + Level 3 adds the shared malloc.
  + Level 4 adds the panel reuse.
**** Validation of the shared malloc     :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =eb071f09d822e1031ea0776949058bf2f55cb94a=
- Compilation and execution for optimized HPL (made on =nova-10=)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=3 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_proc.csv --nb_runs 3 --size 20000 --nb_proc 8,16,32,48,64,80,96,128 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 5000,10000,15000,20000,30000,40000 --nb_proc 64
  --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8
  #+end_src
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  old <- rbind(read.csv("validation/result_size_L2.csv"), read.csv("validation/result_proc_L2.csv"))
  new <- rbind(read.csv("validation/result_size_L3.csv"), read.csv("validation/result_proc_L3.csv"))
  old$shared_malloc = FALSE
  new$shared_malloc =  TRUE
  results = rbind(old, new)
  #+end_src

  #+begin_src R :file validation/L3/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "simulation_time", "shared_malloc", "Shared malloc", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/L3/1.pdf]]

  #+begin_src R :file validation/L3/2.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "simulation_time", "shared_malloc", "Shared malloc", 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L3/2.pdf]]


  #+begin_src R :file validation/L3/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "memory_size", "shared_malloc", "Shared malloc", 64)
  #+end_src

  #+results:
  [[file:validation/L3/3.pdf]]

  #+begin_src R :file validation/L3/4.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "memory_size", "shared_malloc", "Shared malloc", 20000)
  #+end_src

  #+results:
  [[file:validation/L3/4.pdf]]

  #+begin_src R :file validation/L3/5.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "Gflops", "shared_malloc", "Shared malloc", 64)
  #+end_src

  #+results:
  [[file:validation/L3/5.pdf]]

  #+begin_src R :file validation/L3/6.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "Gflops", "shared_malloc", "Shared malloc", 20000)
  #+end_src

  #+results:
  [[file:validation/L3/6.pdf]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      x = x[x$size %in% (c(5000, 10000, 15000, 20000, 30000, 40000)),]
      x = x[x$nb_proc %in% (c(8, 16, 32, 48, 64, 80, 96, 128)),]
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(old)
  aggr_new = aggregate_results(new)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_new$Gflops
  #+end_src

  #+begin_src R :file validation/L3/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(aggr_new[aggr_new$nb_proc==64,], aes(x=size, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(aggr_new[aggr_new$size==20000,], aes(x=nb_proc, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different number of processes\nUsing a matrix size of 20,000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/L3/4.png]]
- The benefit in terms of memory saving is still impressive, despite the huge gain already done thanks to the
  computaiton pruning.
- The Gflops error is negligible.
*** 2017-05-31 Wednesday
**** Validation of the panel reuse       :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =eb071f09d822e1031ea0776949058bf2f55cb94a=
- Compilation and execution for optimized HPL (made on =nova-10=)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=4 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_proc.csv --nb_runs 3 --size 20000 --nb_proc 8,16,32,48,64,80,96,128 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 5000,10000,15000,20000,30000,40000 --nb_proc 64
  --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8
  #+end_src
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  old <- rbind(read.csv("validation/result_size_L3.csv"), read.csv("validation/result_proc_L3.csv"))
  new <- rbind(read.csv("validation/result_size_L4.csv"), read.csv("validation/result_proc_L4.csv"))
  old$panel_reuse = FALSE
  new$panel_reuse =  TRUE
  results = rbind(old, new)
  #+end_src

  #+begin_src R :file validation/L4/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "simulation_time", "panel_reuse", "Panel reuse", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/L4/1.pdf]]

  #+begin_src R :file validation/L4/2.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "simulation_time", "panel_reuse", "Panel reuse", 20000)
  #+end_src

  #+RESULTS:
  [[file:validation/L4/2.pdf]]


  #+begin_src R :file validation/L4/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "memory_size", "panel_reuse", "Panel reuse", 64)
  #+end_src

  #+results:
  [[file:validation/L4/3.pdf]]

  #+begin_src R :file validation/L4/4.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "memory_size", "panel_reuse", "Panel reuse", 20000)
  #+end_src

  #+results:
  [[file:validation/L4/4.pdf]]

  #+begin_src R :file validation/L4/5.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "Gflops", "panel_reuse", "Panel reuse", 64)
  #+end_src

  #+results:
  [[file:validation/L4/5.pdf]]

  #+begin_src R :file validation/L4/6.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "nb_proc", "Gflops", "panel_reuse", "Panel reuse", 20000)
  #+end_src

  #+results:
  [[file:validation/L4/6.pdf]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      x = x[x$size %in% (c(5000, 10000, 15000, 20000, 30000, 40000)),]
      x = x[x$nb_proc %in% (c(8, 16, 32, 48, 64, 80, 96, 128)),]
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(old)
  aggr_new = aggregate_results(new)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_new$Gflops
  #+end_src

  #+begin_src R :file validation/L4/4.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(aggr_new[aggr_new$nb_proc==64,], aes(x=size, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(aggr_new[aggr_new$size==20000,], aes(x=nb_proc, y=Gflops_error))) +
      ggtitle("Gflops estimation error for different number of processes\nUsing a matrix size of 20,000")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/L4/4.png]]
- The benefit in terms of memory saving is still impressive, despite the huge gain already done thanks to the
  computaiton pruning.
- The Gflops error is negligible.
**** DONE Use SVG instead of PNG for the report's figures
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-06-08 Thu 14:50]
- State "TODO"       from              [2017-05-31 Wed 15:46]
:END:
** 2017-06 June
*** 2017-06-01 Thursday
**** Redo validation of huge pages       :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =eb071f09d822e1031ea0776949058bf2f55cb94a=
- Compilation and execution for optimized HPL (made on =nova-10= without the huge pages, =nova-11= with the huge pages)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=4 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  sysctl -w vm.overcommit_memory=1 && sysctl -w vm.max_map_count=40000000

  mount none /root/huge -t hugetlbfs -o rw,mode=0777 && echo 1 >> /proc/sys/vm/nr_hugepages
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 50000,100000,150000,200000,250000,300000 --nb_proc
  64 --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8

  ./run_measures.py --global_csv result_size.csv --nb_runs 3 --size 50000,100000,150000,200000,250000,300000 --nb_proc
  64 --topo "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge
  #+end_src
- Analysis
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(gridExtra)
  old <- rbind(read.csv("validation/result_size_L4_big_nohugepage.csv"), read.csv("validation/result_size_L4_big_nohugepage_2.csv"))
  new <- read.csv("validation/result_size_L4_big_hugepage.csv")
  old$hugepage = FALSE
  new$hugepage =  TRUE
  results = rbind(old, new)
  #+end_src

  #+RESULTS:

  #+begin_src R :file validation/hugepage/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "simulation_time", "hugepage", "Huge page", 64)
  #+end_src

  #+RESULTS:
  [[file:validation/hugepage/1.pdf]]

  #+begin_src R :file validation/hugepage/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "memory_size", "hugepage", "Huge page", 64)
  #+end_src

  #+results:
  [[file:validation/hugepage/3.pdf]]

  #+begin_src R :file validation/hugepage/5.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  do_plot(results, "size", "Gflops", "hugepage", "Huge page", 64)
  #+end_src

  #+results:
  [[file:validation/hugepage/5.pdf]]

  #+begin_src R :file validation/hugepage/report_plot.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  grid_arrange_shared_legend(
      do_plot(results, "size", "simulation_time", "hugepage", "Huge page", 64),
      do_plot(results, "size", "memory_size", "hugepage", "Huge page", 64),
      nrow=1, ncol=2
  )
  #+end_src

  #+RESULTS:
  [[file:validation/hugepage/report_plot.pdf]]

  #+begin_src R :file validation/hugepage/2.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  plot1 = generic_do_plot(ggplot(results, aes(x=size, y=cpu_utilization, color=hugepage))) +
      ggtitle("CPU utilization for different matrix sizes\nUsing 64 MPI processes")
  plot2 = generic_do_plot(ggplot(results, aes(x=size, y=minor_page_fault, color=hugepage))) +
      ggtitle("Number of page faults for different matrix sizes\nUsing 64 MPI processes")
  grid.arrange(plot1, plot2, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:validation/hugepage/2.png]]


  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(simulation_time=mean(simulation_time), Gflops=mean(Gflops), application_time=mean(application_time)), by=c("size", "nb_proc")])
      return(x[with(x, order(size, nb_proc)),])
  }
  aggr_old = aggregate_results(old)
  aggr_new = aggregate_results(new)
  aggr_new$Gflops_error = (aggr_new$Gflops - aggr_old$Gflops)/aggr_new$Gflops
  #+end_src

  #+begin_src R :file validation/hugepage/3.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  generic_do_plot(ggplot(aggr_new, aes(x=size, y=Gflops_error)))
  #+end_src

  #+RESULTS:
  [[file:validation/hugepage/3.png]]

  - The Gflops error is negligible.
  - The gain of using huge pages is pretty neat for both the simulation time and the memory consumption.
  - Very large variability of the CPU utilization, something weird has happened.
**** Scalability test                    :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =8cfd8d16787f39a29342b64599cf02166af6d632=
- Compilation and execution for optimized HPL (made on =nova-10= and =nova-11=)
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=4 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  sysctl -w vm.overcommit_memory=1 && sysctl -w vm.max_map_count=40000000

  mount none /root/huge -t hugetlbfs -o rw,mode=0777 && echo 1 >> /proc/sys/vm/nr_hugepages
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_size_1000000_512.csv --nb_runs 1 --size 1000000 --nb_proc 512 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_1000000_1024.csv --nb_runs 1 --size 1000000 --nb_proc 1024 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_1000000_2048.csv --nb_runs 1 --size 1000000 --nb_proc 2048 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_2000000_512.csv --nb_runs 1 --size 2000000 --nb_proc 512 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_2000000_1024.csv --nb_runs 1 --size 2000000 --nb_proc 1024 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_2000000_2048.csv --nb_runs 1 --size 2000000 --nb_proc 2048 --topo
  "2;16,32;1,16;1,1" --experiment HPL --running_power 5004882812.500 --nb_cpu 8 --hugepage /root/huge
  #+end_src
- Results:
  #+begin_src R :results output :session *R* :exports both
  rbind(
      read.csv('scalability/result_1000000_512.csv'),
      read.csv('scalability/result_1000000_1024.csv'),
      read.csv('scalability/result_1000000_2048.csv'),
      read.csv('scalability/result_2000000_512.csv'),
      read.csv('scalability/result_2000000_1024.csv'),
      read.csv('scalability/result_2000000_2048.csv')
  )
  #+end_src

  #+RESULTS:
  #+begin_example
            topology nb_roots nb_proc    size full_time      time Gflops
  1 2;16,32;1,16;1,1       16     512 1000000    716521  716521.0  930.4
  2 2;16,32;1,16;1,1       16    1024 1000000    363201  363201.0 1836.0
  3 2;16,32;1,16;1,1       16    2048 1000000    186496  186495.7 3575.0
  4 2;16,32;1,16;1,1       16     512 2000000   5685080 5685077.7  938.1
  5 2;16,32;1,16;1,1       16    1024 2000000   2861010 2861012.5 1864.0
  6 2;16,32;1,16;1,1       16    2048 2000000   1448900 1448899.1 3681.0
    simulation_time application_time user_time system_time major_page_fault
  1         2635.10           500.97   2367.19      259.91                0
  2         6037.89          1036.96   5515.36      515.05                0
  3        12391.90          2092.95  11389.36      995.39                0
  4         6934.86          1169.66   6193.80      683.73                0
  5        15198.30          2551.10  13714.01     1430.93                0
  6        32263.60          5236.56  29357.92     2844.89                0
    minor_page_fault cpu_utilization        uss        rss page_table_size
  1          1916208            0.99  153665536 2317279232        10600000
  2          2002989            0.99  369676288 4837175296        21252000
  3          2154982            0.99 1010696192 7774138368        42908000
  4          3801905            0.99  150765568 2758770688        10604000
  5          3872820            0.99  365555712 5273034752        21220000
  6          4038099            0.99 1009606656 7415914496        42884000
    memory_size
  1   894443520
  2  1055309824
  3  1581170688
  4  3338420224
  5  3497111552
  6  4027408384
  #+end_example
**** Add the Stampede output file in the repository                  :HPL:
- File [[file:fullrun2.run1.notestmode.20000m.log]]
*** 2017-06-02 Friday
**** DONE New scalability tests to run [6/6]                    :SMPI:HPL:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-06-05 Mon 19:33]
- State "TODO"       from "TODO"       [2017-06-04 Sun 20:22]
- State "TODO"       from "TODO"       [2017-06-04 Sun 20:22]
- State "TODO"       from "TODO"       [2017-06-04 Sun 20:22]
- State "TODO"       from "TODO"       [2017-06-03 Sat 19:05]
- State "TODO"       from "TODO"       [2017-06-03 Sat 19:05]
- State "TODO"       from              [2017-06-02 Fri 09:48]
:END:
- [X] N=1000000, nbproc=4096, expected time \approx 206min \times 2.2 \approx 7.5h
- [X] N=2000000, nbproc=4096, expected time \approx 537min \times 2.2 \approx 19.7h
- [X] N=4000000, nbproc=512,  expected time \approx 115min \times 2.6 \approx 5h
- [X] N=4000000, nbproc=1024, expected time \approx 253min \times 2.6 \approx 11h
- [X] N=4000000, nbproc=2048, expected time \approx 537min \times 2.6 \approx 23.3h
- [X] N=4000000, nbproc=4096, expected time \approx 537min \times 2.6 \times 2.2 \approx 51h
**** Cannot connect anymore on G5K nodes in Lyon                 :BUG:G5K:
- Reserved a job and made a deployment in =lyon=. Then, *cannot* connect to the node (both as =tocornebize= and as =root=).
- Reserved a job and made a deployment in =grenoble=. Then, *can* connect to the node (both as =tocornebize= and as =root=).
- Looked at the =.ssh= directories of =grenoble= and =lyon=, they look the same.
- Can =ssh= from =lyon= to =grenoble= (or any other site) but cannot =ssh= from =grenoble= (or any other site) to =lyon=.
- Fixed by replacing the =.ssh= folder from =lyon= by the =.ssh= folder from =grenoble= (might have messed up something...).
**** First capacity planning test            :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =4ff3ccbcbb77e126e454a16dea0535493ff1ff0b=
- Compilation and execution (on =nova-6= and =nova-8=).
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=4 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  sysctl -w vm.overcommit_memory=1 && sysctl -w vm.max_map_count=40000000

  mount none /root/huge -t hugetlbfs -o rw,mode=0777 && echo 1 >> /proc/sys/vm/nr_hugepages
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_capacity_50000.csv --nb_runs 1 --size 50000 --nb_proc 512 --topo "2;16,32;1,1:16;1,1"
  --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_capacity_100000.csv --nb_runs 1 --size 100000 --nb_proc 512 --topo "2;16,32;1,1:16;1,1"
  --experiment HPL --running_power 5004882812.500 --hugepage /root/huge
  #+end_src
- Results:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results <- rbind(read.csv("capacity_planning/result_capacity_50000.csv"), read.csv("capacity_planning/result_capacity_100000.csv"))
  #+end_src

  #+begin_src R :file capacity_planning/1.png :results value graphics :results output :session *R* :exports both :width 800 :height 400
  ggplot(results, aes(x=nb_roots, y=Gflops, color=size, group=size)) +
      stat_summary(fun.y = mean, geom="line")+
      stat_summary(fun.y = mean, geom="point")+
      expand_limits(x=0, y=0)+
      ggtitle("Gflops estimation for different number of root switches and matrix sizes\nUsing 512 MPI processes")
  #+end_src

  #+RESULTS:
  [[file:capacity_planning/1.png]]

- In this experiment, we use a fat-tree which has a total of 512 nodes, all having only one core. We use 512 processes,
  one per node. We change the number of up-ports of the L1 switches and therefore the number of L2 switches.
- It is strange, there is apparently no impact on the performances of HPL, we get the same performances with only one L2
  switch than with 16 L2 switches.
- Maybe we could try with a bigger matrix, to maybe have some network contention? But the experiment might take some time.
- We could also try with a hostfile randomly shuffled, to maybe have a less good mapping and thus more traffic going
  through the L2 switches?
- We could also try a fat-tree more “high” and less “wide”. We could have a third layer of switches, but decrease the
  number of ports to keep the same number of nodes. For instance, =3;8,8,8;1,8,16;1,1,1= instead of =2;16,32;1,16;1,1= (all
  have 512 nodes). But it is a bit artificial, such topology would certainly never happen in “real life”.
*** 2017-06-03 Saturday
**** New scalability tests                   :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =4ff3ccbcbb77e126e454a16dea0535493ff1ff0b=
- Compilation and execution (made on =nova-5=, =nova-11=, =nova-13=, =nova-14=):
  #+begin_src sh
  make SMPI_OPTS="-DSMPI_OPTIMIZATION_LEVEL=4 -DSMPI_DGEMM_COEFFICIENT=1.742435e-10
  -DSMPI_DTRSM_COEFFICIENT=8.897459e-11" arch=SMPI
  #+end_src
  #+begin_src sh
  sysctl -w vm.overcommit_memory=1 && sysctl -w vm.max_map_count=2000000000

  mount none /root/huge -t hugetlbfs -o rw,mode=0777 && echo 1 >> /proc/sys/vm/nr_hugepages
  #+end_src
  #+begin_src sh
  ./run_measures.py --global_csv result_size_1000000_4096.csv --nb_runs 1 --size 1000000 --nb_proc 4096 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_4000000_512.csv --nb_runs 1 --size 4000000 --nb_proc 512 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_4000000_1024.csv --nb_runs 1 --size 4000000 --nb_proc 1024 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_2000000_4096.csv --nb_runs 1 --size 2000000 --nb_proc 4096 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_4000000_2048.csv --nb_runs 1 --size 4000000 --nb_proc 2048 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  ./run_measures.py --global_csv result_size_4000000_4096.csv --nb_runs 1 --size 4000000 --nb_proc 4096 --topo
  "2;16,32;1,16;1,1;8" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge

  #+end_src

  #+begin_src R :results output :session *R* :exports both
  rbind(
      read.csv('scalability/result_500000_512.csv'),
      read.csv('scalability/result_500000_1024.csv'),
      read.csv('scalability/result_500000_2048.csv'),
      read.csv('scalability/result_500000_4096.csv'),
      read.csv('scalability/result_1000000_4096.csv'),
      read.csv('scalability/result_2000000_4096.csv'),
      read.csv('scalability/result_4000000_512.csv'),
      read.csv('scalability/result_4000000_1024.csv'),
      read.csv('scalability/result_4000000_2048.csv'),
      read.csv('scalability/result_4000000_4096.csv')
  )
  #+end_src

  #+RESULTS:
  #+begin_example
               topology nb_roots nb_proc    size  full_time        time Gflops
  1  2;16,32;1,16;1,1;8       16     512  500000    91246.1    91246.02  913.3
  2  2;16,32;1,16;1,1;8       16    1024  500000    46990.1    46990.02 1773.0
  3  2;16,32;1,16;1,1;8       16    2048  500000    24795.5    24795.50 3361.0
  4  2;16,32;1,16;1,1;8       16    4096  500000    13561.0    13561.01 6145.0
  5  2;16,32;1,16;1,1;8       16    4096 1000000    97836.6    97836.54 6814.0
  6  2;16,32;1,16;1,1;8       16    4096 2000000   742691.0   742690.59 7181.0
  7  2;16,32;1,16;1,1;8       16     512 4000000 45305100.0 45305083.56  941.8
  8  2;16,32;1,16;1,1;8       16    1024 4000000 22723800.0 22723820.45 1878.0
  9  2;16,32;1,16;1,1;8       16    2048 4000000 11432900.0 11432938.62 3732.0
  10 2;16,32;1,16;1,1;8       16    4096 4000000  5787160.0  5787164.09 7373.0
     simulation_time application_time user_time system_time major_page_fault
  1          1191.99          204.992   1098.25       93.12                0
  2          2482.28          441.897   2296.51      184.70                0
  3          5091.97          872.425   4741.26      349.79                0
  4         11321.60         1947.320  10640.63      679.53                0
  5         26052.50         4362.660  24082.38     1966.10                0
  6         64856.30        10643.600  59444.40     5402.24                0
  7         17336.50         3030.400  15090.31     1945.23                0
  8         38380.90         6435.870  34249.71     3827.36                0
  9         83535.20        13080.500  75523.95     7684.52                0
  10       169659.00        26745.400 154314.76    15085.08                0
     minor_page_fault cpu_utilization        uss         rss page_table_size
  1            960072            0.99  155148288  2055086080        10604000
  2           1054062            0.99  369696768  4383203328        21240000
  3           1282294            0.99 1012477952  9367576576        42912000
  4           1852119            0.99 3103875072 15318568960        87740000
  5           2768705            0.99 3103895552 16934834176        87748000
  6           4704339            0.99 3102445568 19464646656        87748000
  7           7663911            0.98  151576576  2056916992        10604000
  8           7725625            0.99  369872896  4120702976        21212000
  9           7917525            0.99 1012191232  9221050368        42880000
  10          8550745            0.99 3113381888 20408209408        87808000
     memory_size
  1    282558464
  2    429948928
  3    962826240
  4   2814042112
  5   3425406976
  6   5910134784
  7  13079060480
  8  13275557888
  9  13825183744
  10 15763668992
#+end_example
- +Memory measurement failed for the experiments with 4096 nodes (=smpimain= took too much time to start, so its PID was+
  +not found by =run_measure.py= at the beginning, so it assumed it was already terminated... really need to find something+
  +more robust).+
- For the record, ran this command on the nodes (same command used in the script to estimate the memory consumption):
  #+begin_src sh
  python3 -c "import psutil; print(psutil.virtual_memory().available)"
  #+end_src
- Result:
  + For size=2000000 and nbproc=4096: 60468817920
  + For size=4000000 and nbproc=1024: 53105373184
  + For size=4000000 and nbproc=2048: 52539293696
  + For size=4000000 and nbproc=4096: 50614239232
- On a freshly deployed node, the same command returns 66365100032
*** 2017-06-04 Sunday
**** Investigate capacity planning: small test program          :SMPI:HPL:
- As mentionned in [2017-06-02 Fri], the duration of HPL does not seem to be impacted by the topology, which is strange.
- Implemented a small test program, called =network_test=. It takes as argument a size and a number of iterations. Every
  process sends the given number of messages, each having the given size, to the next process (and thus receives from
  the previous one).
- Tested with the following topology (only changing the fat-tree description):
  #+begin_example
  <?xml version='1.0' encoding='ASCII'?>
  <!DOCTYPE platform SYSTEM "http://simgrid.gforge.inria.fr/simgrid/simgrid.dtd">
  <!--2-level fat-tree with 16 nodes-->
  <platform version="4">
    <AS id="AS0" routing="Full">
      <cluster id="cluster0" prefix="host-" suffix=".hawaii.edu" radical="0-15" speed="1Gf" bw="10Gbps" lat="2.4E-5s" loopback_bw="5120MiBps" loopback_lat="1.5E-9s" core="1" topology="FAT_TREE" topo_parameters="2;4,4;1,4;1,1"/>
    </AS>
  </platform>
  #+end_example
- Results for one iteration:
  + With a size of =200000000= and the fat-tree =2;4,4;1,4;1,1=, takes a time of 1.28   seconds.
  + With a size of =200000000= and the fat-tree =2;4,4;1,1;1,1=, takes a time of 2.69   seconds.
  + With a size of =200000=    and the fat-tree =2;4,4;1,4;1,1=, takes a time of 0.0025 seconds.
  + With a size of =200000=    and the fat-tree =2;4,4;1,1;1,1=, takes a time of 0.0040 seconds.
  + With a size of =2000=      and the fat-tree =2;4,4;1,4;1,1=, takes a time of 0.0004 seconds.
  + With a size of =2000=      and the fat-tree =2;4,4;1,1;1,1=, takes a time of 0.0004 seconds.
- Thus, for large enough size, the difference is very clear, the topology *does* have a high impact. For small messages
  however, this is not the case.
- It does not seem to change for several iterations.
**** TODO Check whas are the sizes of the messagess in HPL.     :SMPI:HPL:
:LOGBOOK:
- State "TODO"       from              [2017-06-04 Sun 19:44]
:END:
**** Investigate capacity planning: odd networks                :SMPI:HPL:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =4ff3ccbcbb77e126e454a16dea0535493ff1ff0b=
- Try several topologies for HPL with absurdly good or bad networks (e.g. high/null bandwidth and/or high/null latency).
- The idea is that if doing so has a little impact on performances, then it is hopeless to observe any impact from
  adding/removing switches.
- Quick and dirty experiments: do not add any option to the script, just modify the values in =topology.py= (lines
  161-164).
- Note that in the previous experiments, where nearly no impact was observed, the different values were:
  #+begin_src python
  bw = '10Gbps'
  lat = '2.4E-5s'
  loopback_bw = '5120MiBps'
  loopback_lat = '1.5E-9s'
  #+end_src
- Run this command, which outputs the Gflops:
  #+begin_src sh
  ./run_measures.py --global_csv /tmp/bla.csv --nb_runs 1 --size 10000 --nb_proc 16 --topo "2;4,4;1,4;1,1" --experiment
  HPL --running_power 6217956542.969 && tail -n 1 /tmp/bla.csv | cut -f10 -d','
  #+end_src
- Result for the same network characteristics: 21.96
- Results with other characteristics:
  + Very high bandwidth: 22.15
    #+begin_src python
    bw = '1000000Gbps'
    lat = '2.4E-5s'
    loopback_bw = '1000000GBps'
    loopback_lat = '1.5E-9s'
    #+end_src
  + Very low bandwidth: 1.505
    #+begin_src python
    bw = '10Mbps'
    lat = '2.4E-5s'
    loopback_bw = '10Mbps'
    loopback_lat = '1.5E-9s'
    #+end_src
  + Low bandwidth: 19.95
    #+begin_src python
    bw = '1Gbps'
    lat = '2.4E-5s'
    loopback_bw = '512MiBps'
    loopback_lat = '1.5E-9s'
    #+end_src
  + Very low latency: 25.95
    #+begin_src python
    bw = '10Gbps'
    lat = '0s'
    loopback_bw = '5120MiBps'
    loopback_lat = '0s'
    #+end_src
  + Very high latency: 0.1534
    #+begin_src python
    bw = '10Gbps'
    lat = '2.4E-2s'
    loopback_bw = '5120MiBps'
    loopback_lat = '1.5E-5s
    #+end_src
  + High latency: 9.477
    #+begin_src python
    bw = '10Gbps'
    lat = '2.4E-4s'
    loopback_bw = '5120MiBps'
    loopback_lat = '1.5E-8s'
    #+end_src
- Improving the network performances has a limited impact. Using a nearly infinite bandwidth increases the Gflops by
  less than 1%. Using a null latency has more impact, but still limited, it increases the Gflops by 18%.
- Degrading the network performances has more impact. Using a bandwidth 1000 times lower divides by 15 the Gflops, but
  using a bandwidth 10 times lower decreases the Gflops by only 9%. Both the very high latency and the high latency have
  a great impact.
- To sum up, the latency seems to have an higher impact on HPL performances than the bandwidth.
- It is not clear if the contention created by using less switches will only decrease the bandwidth, or also increase
  the latency. It depends if there is on the switches one queue per port, or one queue for all the ports (in the former
  case, contention will have a much lower impact on the latency than in the later case).
- Hypothesis: in the case of a one queue per port model, removing switches will not increase too much the latency and
  therefore have a very limited impact on HPL performances.
**** TODO Ask what model is used in Simgrid’s switches           :SIMGRID:
:LOGBOOK:
- State "TODO"       from              [2017-06-04 Sun 19:40]
:END:
- Is it one queue per port, or one single queue for all the ports?
**** More thoughts on capacity planning                         :SMPI:HPL:
- The plot of the Gflops as a function of the bandwidth (resp. inverse of latency) seems to look like the plot of the
  Gflops as a function of the number of processes or the size. It is a concave function converging to some finite limit.
- In the settings currently used for HPL, the bandwidth of 10Gbps seems to be already very close th the limit (since
  using a bandwidth thousands of time larger has little to no impact). This is why decreasing the bandwidth a bit has a
  very little impact. If we want to observe something when we remove switches, we should use lower bandwidths.
- Quick test, using the same command than previous section and with these values:
  #+begin_src python
  bw = '10Mbps'
  lat = '2.4E-5s'
  loopback_bw = '5120MiBps'
  loopback_lat = '1.5E-9s'
  #+end_src
  + With =2;4,4;1,4;1,1=, 1.505 Gflops.
  + With =2;4,4;1,1;1,1=, 1.025 Gflops.
  + With =2;4,4;1,4;1,1= and a random mapping, 1.268 Gflops.
  + With =2;4,4;1,1;1,1= and a random mapping, 0.6464 Gflops.
- The hypothesis seems to be confirmed. With a lower bandwidth, a difference of bandwidth has much more impact. Thus,
  removing a switch and/or using a random mapping has also much more impact.
*** 2017-06-05 Monday
**** Comparison with real Taurus experiment  :SMPI:EXPERIMENTS:HPL:REPORT:
- File [[file:hpl_analysis/taurus/real.csv]] holds real experiment data. It has been created manually, thanks to the energy
  paper [[https://gitlab.inria.fr/fheinric/paper-simgrid-energy/tree/master/experiments/mpi_hpl_dvfs/taurus_2017-01-17/original-data][repository]].

  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)

  get_results <- function(nb_proc) {
      result <- read.csv(paste('hpl_analysis/taurus/hpl_paper_', nb_proc, '.csv', sep=''))
      result$full_time = max(result$time)
      result$total_energy = sum(result$power_consumption)

      used_energy = 0
      result = result[with(result, order(-power_consumption)),] # sort by power consumption
      result$used_energy = sum(head(result, nb_proc/12)$power_consumption)
      result$nb_proc = nb_proc
      return(unique(result[c('nb_proc', 'full_time', 'total_energy', 'used_energy')]))
  }
  simulation_vanilla_results = data.frame()
  #  for(i in (c(1,4,8,12,48,96,144))) {
  for(i in (c(12,48,96,144))) {
    simulation_vanilla_results = rbind(simulation_vanilla_results, get_results(i))
  }
  simulation_vanilla_results$type = 'Vanilla simulation'
  simulation_vanilla_results$time = -1 # do not have it
  simulation_vanilla_results$Gflops = -1 # do not have it

  real_results = read.csv('hpl_analysis/taurus/real.csv')
  real_results$type = 'Real execution'
  real_results$used_energy = real_results$used_energy * 1e3 # kJ -> J
  sim_results <- read.csv('hpl_analysis/taurus/hpl2.csv')
  sim_results$type = 'Optimized simulation'
  results = rbind(real_results[c('nb_proc', 'full_time', 'time', 'Gflops', 'used_energy', 'type')],
                  sim_results[c('nb_proc', 'full_time', 'time', 'Gflops', 'used_energy', 'type')],
                  simulation_vanilla_results[c('nb_proc', 'full_time', 'time', 'Gflops', 'used_energy', 'type')])
  results$type <- factor(results$type, levels = c('Optimized simulation', 'Vanilla simulation', 'Real execution'))
  #+end_src

  #+RESULTS:

  #+begin_src R :file hpl_analysis/taurus/validation.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  p1 = generic_do_plot(ggplot(results, aes(x=nb_proc, y=full_time, color=type, shape=type)), fixed_shape=FALSE) +
           xlab("Number of processes")+
           ylab("Duration (seconds)")+
           scale_shape_manual(values = c(0, 1, 2))+
           labs(colour="Experiment type")+
           labs(shape="Experiment type")+
           ggtitle("HPL duration for different numbers of processes\nMatrix size: 20,000")
  p2 = generic_do_plot(ggplot(results, aes(x=nb_proc, y=used_energy, color=type, shape=type)), fixed_shape=FALSE) +
           xlab("Number of processes")+
           ylab("Energy consumption (joules)")+
           scale_shape_manual(values = c(0, 1, 2))+
           labs(colour="Experiment type")+
           labs(shape="Experiment type")+
           ggtitle("HPL energy consumption for different numbers of processes\nMatrix size: 20,000")
  grid_arrange_shared_legend(p1, p2, nrow=1, ncol=2)
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/validation.pdf]]

  #+begin_src R :file hpl_analysis/taurus/validation2.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  tmp_results = results[results$type != "Vanilla simulation",]
  grid_arrange_shared_legend(
      generic_do_plot(ggplot(tmp_results, aes(x=nb_proc, y=time, color=type))) +
        xlab("Number of processes")+
        ylab("Duration (seconds)")+
        labs(colour="Simulated")+
        ggtitle("HPL “short” duration for different numbers of processes\nMatrix size: 20,000"),
      generic_do_plot(ggplot(tmp_results, aes(x=nb_proc, y=Gflops, color=type))) +
        xlab("Number of processes")+
        ylab("Energy consumption (joules)")+
        labs(colour="Simulated")+
        ggtitle("HPL performances for different numbers of processes\nMatrix size: 20,000"),
      nrow=1, ncol=2
  )
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/validation2.pdf]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = x[x$nb_proc %in% c(12, 48, 96, 144)]
      x = as.data.frame(x[, list(time=mean(full_time), energy=mean(used_energy)), by=c("nb_proc")])
      return(x[with(x, order(nb_proc)),])
  }
  aggr_real = aggregate_results(real_results)
  aggr_sim = aggregate_results(sim_results)
  aggr_vanilla = aggregate_results(simulation_vanilla_results)
  aggr_sim$time_error = (aggr_sim$time - aggr_real$time)/aggr_real$time * 100
  aggr_sim$energy_error = (aggr_sim$energy - aggr_real$energy)/aggr_real$energy * 100
  aggr_sim$optimized = TRUE
  aggr_vanilla$time_error = (aggr_vanilla$time - aggr_real$time)/aggr_real$time * 100
  aggr_vanilla$energy_error = (aggr_vanilla$energy - aggr_real$energy)/aggr_real$energy * 100
  aggr_vanilla$optimized = FALSE
  aggr_results = rbind(aggr_vanilla, aggr_sim)
  aggr_results$optimized <- factor(aggr_results$optimized, levels = c(TRUE, FALSE))
  #+end_src

  #+RESULTS:

- Get the three colors used for the previous plots to use the ones corresponding to vanilla and optimized.
  #+begin_src R :results output :session *R* :exports both
  x = unique(ggplot_build(p1)$data[[1]]$colour)
  x
  colors = x[c(1, 2)]
  colors
  #+end_src

  #+RESULTS:
  : [1] "#F8766D" "#00BA38" "#619CFF"
  : [1] "#F8766D" "#00BA38"

  #+begin_src R :file hpl_analysis/taurus/errors.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
  grid_arrange_shared_legend(
      generic_do_plot(ggplot(aggr_results, aes(x=nb_proc, y=time_error, color=optimized))) +
        geom_hline(yintercept=0) +
        scale_color_manual(values=colors) +
        xlab("Number of processes")+
        ylab("Relative error (percent)")+
        labs(colour="Optimized simulation")+
        ggtitle("Error on the duration prediction")+
        expand_limits(y=15)+
        expand_limits(y=-15),
      generic_do_plot(ggplot(aggr_results, aes(x=nb_proc, y=energy_error, color=optimized))) +
        geom_hline(yintercept=0) +
        scale_color_manual(values=colors) +
        xlab("Number of processes")+
        ylab("Relative error (percent)")+
        labs(colour="Optimized simulation")+
        ggtitle("Error on the energy consumption prediction")+
        expand_limits(y=15)+
        expand_limits(y=-15),
      nrow=1, ncol=2
  )
  #+end_src

  #+RESULTS:
  [[file:hpl_analysis/taurus/errors.pdf]]

- The plots are funny. The shapes of the error plots for optimized and vanilla look similar, but shifted. They both
  reach some high errors (~ 10%), but not for the same number of processes. Also, the optimized version is always above
  0 while the vanill is below 0 for some points.
- There are some mismatches between time prediction and energy prediction. For instance, optimized has a large error for
  the time prediction of 144 processes, but nearly no error for the energy prediction. Similarly, vanilla
  over-estimates the duration for 48 processes but under-estimates the energy consumption, which seems odd.
**** Plots for scalability test              :SMPI:EXPERIMENTS:HPL:REPORT:
#+begin_src R :results output :session *R* :exports both
library(ggplot2)
library(ggrepel)
library(reshape2)
library(gridExtra)
results = rbind(
    read.csv('scalability/result_500000_512.csv'),
    read.csv('scalability/result_500000_1024.csv'),
    read.csv('scalability/result_500000_2048.csv'),
    read.csv('scalability/result_500000_4096.csv'),
    read.csv('scalability/result_1000000_512.csv'),
    read.csv('scalability/result_1000000_1024.csv'),
    read.csv('scalability/result_1000000_2048.csv'),
    read.csv('scalability/result_1000000_4096.csv'),
    read.csv('scalability/result_2000000_512.csv'),
    read.csv('scalability/result_2000000_1024.csv'),
    read.csv('scalability/result_2000000_2048.csv'),
    read.csv('scalability/result_2000000_4096.csv'),
    read.csv('scalability/result_4000000_512.csv'),
    read.csv('scalability/result_4000000_1024.csv'),
    read.csv('scalability/result_4000000_2048.csv'),
    read.csv('scalability/result_4000000_4096.csv')
)
results$simulation_time = results$simulation_time/3600
results$memory_size = results$memory_size * 1e-9
number_verb <- function(n) {
    return(format(n,big.mark=",",scientific=FALSE))
}
results$size_verb = factor(unlist(lapply(results$size, number_verb)), levels = c('500,000','1,000,000','2,000,000','4,000,000'))
results$nb_proc_verb = factor(unlist(lapply(results$nb_proc, number_verb)), levels = c('512', '1,024', '2,048', '4,096'))
results
#+end_src

#+RESULTS:
#+begin_example
             topology nb_roots nb_proc    size  full_time        time Gflops
1  2;16,32;1,16;1,1;8       16     512  500000    91246.1    91246.02  913.3
2  2;16,32;1,16;1,1;8       16    1024  500000    46990.1    46990.02 1773.0
3  2;16,32;1,16;1,1;8       16    2048  500000    24795.5    24795.50 3361.0
4  2;16,32;1,16;1,1;8       16    4096  500000    13561.0    13561.01 6145.0
5    2;16,32;1,16;1,1       16     512 1000000   716521.0   716521.00  930.4
6    2;16,32;1,16;1,1       16    1024 1000000   363201.0   363201.04 1836.0
7    2;16,32;1,16;1,1       16    2048 1000000   186496.0   186495.70 3575.0
8  2;16,32;1,16;1,1;8       16    4096 1000000    97836.6    97836.54 6814.0
9    2;16,32;1,16;1,1       16     512 2000000  5685080.0  5685077.72  938.1
10   2;16,32;1,16;1,1       16    1024 2000000  2861010.0  2861012.55 1864.0
11   2;16,32;1,16;1,1       16    2048 2000000  1448900.0  1448899.09 3681.0
12 2;16,32;1,16;1,1;8       16    4096 2000000   742691.0   742690.59 7181.0
13 2;16,32;1,16;1,1;8       16     512 4000000 45305100.0 45305083.56  941.8
14 2;16,32;1,16;1,1;8       16    1024 4000000 22723800.0 22723820.45 1878.0
15 2;16,32;1,16;1,1;8       16    2048 4000000 11432900.0 11432938.62 3732.0
16 2;16,32;1,16;1,1;8       16    4096 4000000  5787160.0  5787164.09 7373.0
   simulation_time application_time user_time system_time major_page_fault
1        0.3311083          204.992   1098.25       93.12                0
2        0.6895222          441.897   2296.51      184.70                0
3        1.4144361          872.425   4741.26      349.79                0
4        3.1448889         1947.320  10640.63      679.53                0
5        0.7319722          500.970   2367.19      259.91                0
6        1.6771917         1036.960   5515.36      515.05                0
7        3.4421944         2092.950  11389.36      995.39                0
8        7.2368056         4362.660  24082.38     1966.10                0
9        1.9263500         1169.660   6193.80      683.73                0
10       4.2217500         2551.100  13714.01     1430.93                0
11       8.9621111         5236.560  29357.92     2844.89                0
12      18.0156389        10643.600  59444.40     5402.24                0
13       4.8156944         3030.400  15090.31     1945.23                0
14      10.6613611         6435.870  34249.71     3827.36                0
15      23.2042222        13080.500  75523.95     7684.52                0
16      47.1275000        26745.400 154314.76    15085.08                0
   minor_page_fault cpu_utilization        uss         rss page_table_size
1            960072            0.99  155148288  2055086080        10604000
2           1054062            0.99  369696768  4383203328        21240000
3           1282294            0.99 1012477952  9367576576        42912000
4           1852119            0.99 3103875072 15318568960        87740000
5           1916208            0.99  153665536  2317279232        10600000
6           2002989            0.99  369676288  4837175296        21252000
7           2154982            0.99 1010696192  7774138368        42908000
8           2768705            0.99 3103895552 16934834176        87748000
9           3801905            0.99  150765568  2758770688        10604000
10          3872820            0.99  365555712  5273034752        21220000
11          4038099            0.99 1009606656  7415914496        42884000
12          4704339            0.99 3102445568 19464646656        87748000
13          7663911            0.98  151576576  2056916992        10604000
14          7725625            0.99  369872896  4120702976        21212000
15          7917525            0.99 1012191232  9221050368        42880000
16          8550745            0.99 3113381888 20408209408        87808000
   memory_size size_verb nb_proc_verb
1    0.2825585   500,000          512
2    0.4299489   500,000        1,024
3    0.9628262   500,000        2,048
4    2.8140421   500,000        4,096
5    0.8944435 1,000,000          512
6    1.0553098 1,000,000        1,024
7    1.5811707 1,000,000        2,048
8    3.4254070 1,000,000        4,096
9    3.3384202 2,000,000          512
10   3.4971116 2,000,000        1,024
11   4.0274084 2,000,000        2,048
12   5.9101348 2,000,000        4,096
13  13.0790605 4,000,000          512
14  13.2755579 4,000,000        1,024
15  13.8251837 4,000,000        2,048
16  15.7636690 4,000,000        4,096
#+end_example

#+begin_src R :file scalability/1.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
size_time = generic_do_plot(ggplot(results, aes(x=size, y=simulation_time, color=nb_proc_verb))) +
    xlab("Matrix size") +
    ylab("Simulation time (hours)") +
    labs(colour="Number of processes")+
    ggtitle("Simulation time for different matrix sizes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, size == max(size)),
        aes(label = nb_proc_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
size_time
#+end_src

#+RESULTS:
[[file:scalability/1.pdf]]

#+begin_src R :file scalability/2.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
nbproc_time = generic_do_plot(ggplot(results, aes(x=nb_proc, y=simulation_time, color=size_verb))) +
    xlab("Number of processes") +
    ylab("Simulation time (hours)") +
    labs(colour="Matrix size")+
    ggtitle("Simulation time for different number of processes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, nb_proc == max(nb_proc)),
        aes(label = size_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
nbproc_time
#+end_src

#+RESULTS:
[[file:scalability/2.pdf]]

#+begin_src R :file scalability/3.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
size_mem = generic_do_plot(ggplot(results, aes(x=size, y=memory_size, color=nb_proc_verb))) +
    xlab("Matrix size") +
    ylab("Memory consumption (gigabytes)") +
    labs(colour="Number of processes")+
    ggtitle("Memory consumption for different matrix sizes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, size == max(size)),
        aes(label = nb_proc_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
      )
size_mem
#+end_src

#+RESULTS:
[[file:scalability/3.pdf]]

#+begin_src R :file scalability/4.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
nbproc_mem = generic_do_plot(ggplot(results, aes(x=nb_proc, y=memory_size, color=size_verb))) +
    xlab("Number of processes") +
    ylab("Memory consumption (gigabytes)") +
    labs(colour="Matrix size")+
    ggtitle("Memory consumption for different number of processes")+
    theme(legend.position = "none")+
    geom_text_repel(
        data = subset(results, nb_proc == max(nb_proc)),
        aes(label = size_verb),
        nudge_x = 45,
        segment.color = NA,
        show.legend = FALSE
    )
nbproc_mem
#+end_src

#+RESULTS:
[[file:scalability/4.pdf]]

#+begin_src R :file scalability/plot_size.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
grid_arrange_shared_legend(size_time, size_mem, nrow=1, ncol=2)
#+end_src

#+RESULTS:
[[file:scalability/plot_size.pdf]]

#+begin_src R :file scalability/plot_nbproc.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
grid_arrange_shared_legend(nbproc_time, nbproc_mem, nrow=1, ncol=2)
#+end_src

#+RESULTS:
[[file:scalability/plot_nbproc.pdf]]

*** 2017-06-06 Tuesday
**** Discussion about the report                          :MEETING:REPORT:
***** State of the art
****** Important features
- offline vs. online, en particulier pour HPL (probes pour le pipeline des comms)
- si online: language scope, besoin de modifier le code pour que ça passe
- modèles: notion de topologies et prise en compte de la contention (super important a priori), prise en compte des
  spécificités des communications avec MPI (sémantique de synchronisation, différents ranges de performance,
  probablement pas trop grave dans le cas de HPL), collectives (dans le cas de HPL, on s’en fiche)
  - modèle classique dans ce contexte = LogP* mais prend mal en compte la contention (au niveau des noeuds mais pas du
    tout au niveau de la topologie)
  - deux approches principales: packet level et flow level
- passage à l’échelle: ça motive l’utilisation de Parallel DES et de techniques d’émulation d’applications MPI un peu
  “système”
****** Projects:
- Dimemas (Barcelona Supercomputing center), offline (extrae/paraver), “performance debugging” (sensibility analysis, what if,  +performance prediction+)
- LogoPsim (Torsten Hoefler), offline (dag, GOAL), collective algorithms @ scale
- SST macro, online/offline (DUMPI), MPI only, skeletonization/templating, more robust but more specialized (C++)
- +BigSIM (?)+, offline, PDES éventuellement, projet mort. source-to-source transformation for privatization for CHARM++/AMPI
- xSim, online, PDES aux modèles sous-jacents à la validité plus que discutable, mais scalable, privatization à coup de
  copie du segment data et pas de mmap
- CODES, offline, PDES, new kid on the block
***** Validation and capacity planning
- For the comparison with a real execution (Taurus), get the data for real experiment by executing the org-file. Long (~
  5 minutes).
- On capacity planning, it is expected that removing switches has little to no impact. Computation is in O(n^3) while
  communication is in O(n^2) (and most of the communications are asynchronous, so happen during computations).
**** Webinar                                                     :MEETING:
- [[https://github.com/alegrand/RR_webinars/blob/master/9_experimental_testbeds/index.org][Testbeds in computer science]]
- Lucas Nussbaum
*** 2017-06-07 Wednesday
**** DONE Some text is displayed in the pdf but not in the printed version :REPORT:
:LOGBOOK:
- State "DONE"       from "TODO"       [2017-06-07 Wed 09:43]
- State "TODO"       from              [2017-06-07 Wed 09:16]
:END:
- It seems that no text entered between === signs (translated to =\texttt= in Latex) appear in the printed version of the
  report. It is displayed correctly in the pdf. Fix this.
- Reprint the first page of the same file, it is fixed now. The difference is that I printed it by the network and not
  by plugging a USB stick in the printer.
**** Network printer setup                                         :TOOLS:
- Make sure the right package is installed:
  #+begin_src sh
  sudo aptitude install cups-browsed
  #+end_src
- Add these lines to the file =/etc/cups/cups-browsed.conf=:
  #+begin_example
  BrowseRemoteProtocols cups
  BrowsePoll print.imag.fr:631
  #+end_example
- Enable the service:
  #+begin_src sh
  sudo systemctl enable cups-browsed
  #+end_src
- Restart the service:
  #+begin_src sh
  sudo service cups-browsed restart
  #+end_src
*** 2017-06-08 Thursday
**** Capacity planning: components           :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =c2d1d734c80f084157ad70d702e8c669772fb2e4=
- Command (used on =nova-21=, configured as above experiments):
  #+begin_src sh
  bash run_capacity_planning.sh 100000 512

  bash run_capacity_planning.sh 50000 512
  #+end_src
- Results:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  library(reshape2)
  library(gridExtra)

  get_results <- function(directory, name) {
      result <- read.csv(paste('capacity_planning/', directory, '/', name, '.csv', sep=''))
      result$name = name
      return(result)
  }
  get_all_results <- function(directory) {
      results = data.frame()
      for(type in c('bandwidth', 'latency', 'speed')) {
          for(subtype in c('high', 'low')) {
              name = paste(type, subtype, sep='_')
              tmp = get_results(directory, name)
              tmp$type = type
              if(type == 'latency'){
                  if(subtype == 'high')
                      tmp$subtype = 'bad'
                  else
                      tmp$subtype = 'good'
              }
              else {
                  if(subtype == 'high')
                      tmp$subtype = 'good'
                  else
                      tmp$subtype = 'bad'
              }
              results = rbind(results, tmp)
          }
          default = get_results(directory, 'default')
          default$type = type
          default$subtype = 'default'
          results = rbind(results, default)
      }
      return(results[c('size', 'Gflops', 'type', 'subtype')])
  }
  results_1E5 = get_all_results('exp_100000_512')
  results_5E4 = get_all_results('exp_50000_512')
  results_1E5
  results_5E4
  #+end_src

  #+RESULTS:
  #+begin_example
      size  Gflops      type subtype
  1 100000  710.40 bandwidth    good
  2 100000  702.20 bandwidth     bad
  3 100000  722.70 bandwidth default
  4 100000  349.10   latency     bad
  5 100000  823.70   latency    good
  6 100000  722.70   latency default
  7 100000 3419.00     speed    good
  8 100000   83.94     speed     bad
  9 100000  722.70     speed default
     size  Gflops      type subtype
  1 50000  458.80 bandwidth    good
  2 50000  477.00 bandwidth     bad
  3 50000  475.20 bandwidth default
  4 50000  127.30   latency     bad
  5 50000  697.60   latency    good
  6 50000  475.20   latency default
  7 50000 1346.00     speed    good
  8 50000   71.95     speed     bad
  9 50000  475.20     speed default
#+end_example

#+begin_src R :results output :session *R* :exports both
do_plot <- function(results, type) {
    tmp = results[results$type == type,]
    title = paste('HPL performance estimation for different components\nMatrix size of',
        format(unique(results$size),big.mark=",",scientific=FALSE))
    plot = ggplot(results, aes(x=type, y=Gflops, color=subtype, shape=subtype)) +
        geom_point(size=4, stroke=1) +
        scale_shape_manual(values = c(0, 1, 2))+
        theme_bw()+
        expand_limits(x=0, y=0)+
        ggtitle(title)+
        xlab('Component')+
        ylab('Performance estimation (Gflops)')+
        labs(colour='Metric')+
        labs(shape='Metric')
    return(plot)
}
#+end_src

#+RESULTS:

#+begin_src R :file capacity_planning/components_perf.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 4
grid_arrange_shared_legend(
    do_plot(results_5E4, 'bandwidth') + expand_limits(x=0, y=max(results_1E5$Gflops)),
    do_plot(results_1E5, 'bandwidth') + expand_limits(x=0, y=max(results_1E5$Gflops)),
    nrow=1,
    ncol=2
)
#+end_src

#+RESULTS:
[[file:capacity_planning/components_perf.pdf]]
**** Capacity planning: topology             :SMPI:EXPERIMENTS:HPL:REPORT:
- Simgrid commit: =9a8e2f5bce8c6758d4367d21a66466a497d136fe=
- HPL commit: =41774905395aebcb73650defaa7e2aa462e6e1a3=
- Script commit: =c2d1d734c80f084157ad70d702e8c669772fb2e4=
- Four series of experiments:
  + Bandwidth of 10Gbps, sequential mapping of the processes
  + Bandwidth of 10Gbps, random mapping of the processes
  + Bandwidth of 10Mbps, sequential mapping of the processes
  + Bandwidth of 10Mbps, random mapping of the processes
- For the series with a bandwidth of 10MBps, the file =topology.py= has been locally modified to use a bandwidth 1000
  times lower:
  #+begin_example
  176 % git diff                                                        -- INSERT -- 15:42:08
  diff --git a/topology.py b/topology.py
  index 2d7d76c..1a3cd67 100644
  --- a/topology.py
  +++ b/topology.py
  @@ -158,7 +158,7 @@ class FatTree:
       prefix = 'host-'
       suffix = '.hawaii.edu'
       speed = '1Gf'
  -    bw = '10Gbps'
  +    bw = '10Mbps'
       lat = '2.4E-5s'
       loopback_bw = '5120MiBps'
       loopback_lat = '1.5E-9s'
  #+end_example
- Command (used on =nova-2=, =nova-8=, =nova-15= and =nova-16= configured as above experiments):
  + For sequential mapping:
    #+begin_src sh
    ./run_measures.py --global_csv result_capacity_50000.csv --nb_runs 3 --size 50000 --nb_proc 512 --topo
    "2;16,32;1,1:16;1,1" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge 
    #+end_src
  + For random mapping:
    #+begin_src sh
    ./run_measures.py --global_csv result_capacity_50000.csv --nb_runs 3 --size 50000 --nb_proc 512 --topo
    "2;16,32;1,1:16;1,1" --experiment HPL --running_power 5004882812.500 --hugepage /root/huge --shuffle_hosts
    #+end_src
- For the random mapping with 10Mbps bandwidth, more runs have been done (8 instead of 3) to get rid of any bias.
- Results:
  #+begin_src R :results output :session *R* :exports both
  library(ggplot2)
  results_highbw_sequential <- read.csv("capacity_planning/exp_topo_50000_512/result_capacity_50000.csv")
  results_highbw_random <- read.csv("capacity_planning/exp_topo_50000_512/result_capacity_50000_shuffled.csv")
  results_lowbw_sequential <- read.csv("capacity_planning/exp_topo_50000_512/result_capacity_50000_lowbw.csv")
  results_lowbw_random <- read.csv("capacity_planning/exp_topo_50000_512/result_capacity_50000_lowbw_shuffled.csv")
  results_highbw_sequential$mapping = "Sequential"
  results_highbw_random$mapping = "Random"
  results_lowbw_sequential$mapping = "Sequential"
  results_lowbw_random$mapping = "Random"
  results_highbw = rbind(results_highbw_sequential, results_highbw_random)
  results_highbw$bandwidth = '10Gbps'
  results_lowbw = rbind(results_lowbw_sequential, results_lowbw_random)
  results_lowbw$bandwidth = '10Mbps'
  #+end_src

  #+RESULTS:

  #+begin_src R :results output :session *R* :exports both
  do_plot <- function(results) {
      title = paste('HPL performance estimation for different topologies\nBandwidth of', unique(results$bandwidth))
      plot = generic_do_plot(ggplot(results, aes(x=nb_roots, y=Gflops, color=mapping, shape=mapping)), fixed_shape=FALSE) +
          ggtitle(title)+
          xlab('Number of L2 switches')+
          ylab('Performance estimation (Gflops)')+
          scale_shape_manual(values = c(1, 2))+
          labs(colour='Mapping')+
          labs(shape='Mapping')
      return(plot)
  }
  #+end_src

  #+RESULTS:

  #+begin_src R :file capacity_planning/topology.pdf :results value graphics :results output :session *R* :exports both :width 12 :height 6
  grid_arrange_shared_legend(
      do_plot(results_lowbw),
      do_plot(results_highbw),
      nrow=1, ncol=2
  )
  #+end_src

  #+RESULTS:
  [[file:capacity_planning/topology.pdf]]
  
- The results for 10MBps are somehow expected. Removing switches deteriorates the performances, using a random mapping
  of the processes makes things even worse. Also, we can observe some performance peaks for 4, 8 and 16 root
  switches. Maybe this is due to the D mod K algorithm (TODO: check that this is indeed this algorithm). For instance,
  16 divides 512 but 15 does not. So the load of all messages should be spread more uniformly with 16 root switches than
  with 15.
- For 10GBps however, this is more strange. The number of switches has no impact, but this has already been observed on
  previous experiments (see [2017-06-02 Fri]). What is more surprising however is that the random mapping yields to
  betterperformances than the sequential mapping. Is it a bug?

  #+begin_src R :file capacity_planning/topology_sim_time.pdf :results value graphics :results output :session *R* :exports both :width 6 :height 4
  tmp = rbind(results_lowbw, results_highbw)
  tmp$bandwidth <- factor(tmp$bandwidth, levels = c('10Mbps', '10Gbps'))
  generic_do_plot(ggplot(tmp, aes(x=nb_roots, y=simulation_time, color=mapping, shape=mapping, linetype=bandwidth)), fixed_shape=FALSE)+
          ggtitle('Simulation time for different networks')+
          xlab('Number of L2 switches')+
          ylab('Simulation time (seconds)')+
          scale_shape_manual(values = c(1, 2))+
          labs(colour='Mapping')+
          labs(shape='Mapping')+
          labs(linetype='Bandwidth')
  #+end_src

  #+RESULTS:
  [[file:capacity_planning/topology_sim_time.pdf]]

  #+begin_src R :file capacity_planning/2.png :results value graphics :results output :session *R* :exports both
  results_lowbw$simgrid_time = results_lowbw$simulation_time - results_lowbw$application_time
  generic_do_plot(ggplot(results_lowbw, aes(x=nb_roots, y=simgrid_time, color=mapping)))
  #+end_src

  #+RESULTS:
  [[file:capacity_planning/2.png]]

  #+begin_src R :results output :session *R* :exports both
  library(data.table)
  aggregate_results <- function(results) {
      x = data.table(results)
      x = as.data.frame(x[, list(Gflops=mean(Gflops)), by=c("nb_roots")])
      return(x[with(x, order(nb_roots)),])
  }
  aggr_seq = aggregate_results(results_lowbw_sequential)
  aggr_rand = aggregate_results(results_lowbw_random)
  aggr_rand$gflops_ratio = aggr_seq$Gflops / aggr_rand$Gflops
  #+end_src

  #+begin_src R :file capacity_planning/3.png :results value graphics :results output :session *R* :exports both
  generic_do_plot(ggplot(aggr_rand, aes(x=nb_roots, y=gflops_ratio)))
  #+end_src

  #+RESULTS:
  [[file:capacity_planning/3.png]]

- There are *huge* differences (factor 10) in the simulation time depending on the mapping and the number of root
  switches. This time is spent in Simgrid. It certainly comes from more complex communication behaviors (congestion)
  that gives much more work to the network part of Simgrid.

*** 2017-06-09 Friday
**** TODO Work on =run_measure.py= script [0/5]                     :PYTHON:
:LOGBOOK:
- State "TODO"       from              [2017-06-09 Fri 15:47]
:END:
- [ ] Clean the code. In particular, remove the stuff related to the small matrix product test.
- [ ] Write some unit tests.
- [ ] Add options, e.g. to set the bandwidth or the latency without modifying the code.
- [ ] Add flexibility in the way the series of experiments are described. Maybe describe them with Python code in a
  separate file? Or a JSON file?
- [ ] Parallelism: allow to launch experiments on remote machines by ssh.
*** 2017-06-12 Monday
**** Add [[file:hplpaper.pdf][The LINPACK Benchmark: Past, Present and Future]]           :PAPER:
Bibtex: Dongarra03thelinpack
*** 2017-06-14 Wednesday
**** Add [[file:simgrid.pdf][Versatile, Scalable and Accurate Simulation of Distributed Applications and Platforms]] :PAPER:
Bibtex: casanova:hal-01017319
**** Add [[file:CKP93.pdf][LogP: Towards a Realistic Model of Parallel Computation]]   :PAPER:
Bibtex: Culler_1993
**** Finally found a grammar checker \o/                           :TOOLS:
- Check https://www.languagetool.org/
- It can be used in several ways, one of which is a command line tool.
